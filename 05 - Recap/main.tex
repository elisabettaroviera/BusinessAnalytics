\documentclass[a4paper,12pt]{article}

% Preambolo
\usepackage[utf8]{inputenc}  % Supporto per caratteri UTF-8
\usepackage[margin=1cm,includefoot]{geometry}  % Margini ridotti (1 cm su tutti i lati)
\usepackage{titlesec}  % Personalizzazione dei titoli
\usepackage{setspace}  % Controllo della spaziatura
\usepackage{parskip}   % Evita indentazioni, aggiunge spazio tra i paragrafi
\usepackage{enumitem}  % Per personalizzare gli elenchi
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{amsmath,amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}


% --- LISTINGS (MATLAB look) ---
\usepackage[T1]{fontenc}
\usepackage[dvipsnames]{xcolor}
\usepackage{listings}

% Colori simili a MATLAB editor
\definecolor{MatlabBlue}{RGB}{0,0,255}
\definecolor{MatlabGreen}{RGB}{0,153,0}
\definecolor{MatlabPurple}{RGB}{153,0,153}
\definecolor{MatlabGray}{RGB}{120,120,120}

\lstdefinestyle{matlabStyle}{
  language=Matlab,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{MatlabBlue},
  commentstyle=\color{MatlabGreen},
  stringstyle=\color{MatlabPurple},
  numbers=none,              % come MATLAB: niente numeri
  showstringspaces=false,
  breaklines=true,
  frame=single,              % riquadro
  rulecolor=\color{MatlabGray},
  tabsize=4,
  columns=fullflexible,
  keepspaces=true,
  % --- accenti robusti dentro lstlisting ---
  literate=
    {à}{{\`a}}1 {è}{{\`e}}1 {é}{{\'e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
    {À}{{\`A}}1 {È}{{\`E}}1 {É}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
}




% Operatore "opt" (max o min a seconda del contesto)
\DeclareMathOperator*{\opt}{opt}

\usepackage{graphicx}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[dvipsnames,svgnames]{xcolor}

% ------------------------------
% INDICE: solo section, senza titolo
% ------------------------------
\setcounter{tocdepth}{1}      % mostra solo \section
\renewcommand{\contentsname}{} % rimuove "Contents"

% Personalizzazione del titolo in alto
\makeatletter
\renewcommand{\maketitle}{
    \begin{center}
        \vspace{-2cm}
        {\LARGE \textbf{\@title}} \\[-0.2cm]
    \end{center}
}


% Impostazioni per i numeri di pagina
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[R]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

% Dettagli del documento
\title{Business Analytics}
\date{}

\begin{document}

\maketitle

\vspace{-1cm}
\textcolor{NavyBlue}{\section{CAP 1 -- Introduzione alle decisioni in condizioni di incertezza}}

\paragraph{Recap}
Le decisioni di business si collocano spesso in contesti caratterizzati da \textcolor{NavyBlue}{\textbf{incertezza}}, rendendo insufficiente una semplice previsione puntuale delle variabili rilevanti. Le tre componenti della \textcolor{NavyBlue}{\textbf{Business Analytics}} chiariscono ruoli diversi ma complementari: la descrittiva analizza ciò che è accaduto, la predittiva costruisce modelli per descrivere il futuro, mentre la prescrittiva individua \textbf{azioni ottimali} tenendo conto delle conseguenze economiche.\\
\hspace*{1.5em} La qualità di una previsione dipende dalla \textcolor{NavyBlue}{\textbf{funzione di perdita}} associata all’errore. Penalità diverse conducono a previsioni ottimali diverse: il valore atteso è appropriato solo in presenza di costi simmetrici, mentre penalità asimmetriche portano naturalmente a soluzioni basate su \textbf{quantili}. Non esiste quindi una previsione universalmente ottima, ma solo scelte coerenti con il criterio decisionale adottato.\\
\hspace*{1.5em}Nei problemi decisionali l’obiettivo diventa la \textcolor{NavyBlue}{\textbf{scelta di un vettore di decisioni}} sotto incertezza. Quando i fattori di rischio non sono probabilisticamente modellabili si adotta un approccio \textcolor{NavyBlue}{\textbf{robusto}}, basato su scenari peggiori; quando invece è disponibile una distribuzione di probabilità si ottiene un \textcolor{NavyBlue}{\textbf{problema di ottimizzazione stocastica}}. In generale, sostituire l’incertezza con valori medi non preserva l’ottimalità della decisione.\\
\hspace*{1.5em}Il modello del \textcolor{NavyBlue}{\textbf{newsvendor}} evidenzia come la decisione ottimale derivi dal bilanciamento tra \textbf{costi di underage e overage}, mostrando che la domanda media non è un criterio corretto. Questo esempio chiarisce il legame diretto tra struttura dei costi e decisioni sotto incertezza.\\
\hspace*{1.5em}I \textcolor{NavyBlue}{\textbf{vincoli probabilistici}} consentono di imporre il rispetto dei vincoli solo con una certa probabilità, ma possono generare insiemi ammissibili non convessi e problemi difficili da trattare. Gli \textcolor{NavyBlue}{\textbf{alberi decisionali}} permettono invece di rappresentare decisioni \textbf{adattive}, distinguendo tra nodi decisionali e nodi casuali e risolvendo il problema tramite \textcolor{NavyBlue}{\textbf{backward induction}} sulla base del valore atteso.\\
\hspace*{1.5em}Il valore dell’informazione viene quantificato attraverso \textcolor{NavyBlue}{\textbf{EVPI}} e \textcolor{NavyBlue}{\textbf{VSS}}, che misurano rispettivamente il beneficio teorico dell’informazione perfetta e il guadagno ottenuto risolvendo il problema stocastico rispetto a una soluzione deterministica a valore atteso.\\
\hspace*{1.5em}L’introduzione ai \textcolor{NavyBlue}{\textbf{modelli a due stadi}} chiarisce la distinzione tra decisioni \textcolor{NavyBlue}{\textbf{here-and-now}} e decisioni \textcolor{NavyBlue}{\textbf{wait-and-see}}, mostrando il ruolo delle \textbf{decisioni di ricorso}. Questa struttura si estende ai modelli multistadio, nei quali le decisioni dipendono dall’intera storia delle realizzazioni aleatorie e richiedono il rispetto della \textbf{non-anticipatività}.\\
\hspace*{1.5em}Nei modelli multistadio emerge il problema della \textcolor{NavyBlue}{\textbf{generazione degli scenari}} e della crescita dell’albero, che rende necessarie tecniche di campionamento, riduzione e progettazione controllata degli scenari. La qualità di una soluzione va quindi valutata anche in termini di \textcolor{NavyBlue}{\textbf{stabilità in-sample e out-of-sample}}, verificando che le prestazioni non dipendano in modo critico dall’albero utilizzato.\\
\hspace*{1.5em}Si analizzano le \textcolor{NavyBlue}{\textbf{proprietà di convessità}} dei modelli stocastici: i vincoli probabilistici non preservano la convessità in generale, mentre nei modelli a due stadi con ricorso lineare la funzione di ricorso risulta convessa, proprietà fondamentale per lo sviluppo di metodi di soluzione basati su decomposizione.

\paragraph{Introduzione} Si distinguono tre principali tipologie di \emph{Business Analytics}
\begin{itemize}[label=-]
    \item \textcolor{NavyBlue}{\textbf{Business Analytics descrittiva}}: ha l’obiettivo di \textbf{analizzare} e sintetizzare i \textbf{dati storici} al fine di comprendere che cosa è accaduto in passato (e.g. overbooking nel trasporto aereo).
    \item \textcolor{NavyBlue}{\textbf{Business Analytics predittiva}}: mira a \textbf{stimare eventi o variabili future} sulla base di modelli (e.g. distinzione tra \emph{forecasting}--previsione non sotto il mio controllo-- e \emph{prediction}).
    \item \textcolor{NavyBlue}{\textbf{Business Analytics prescrittiva}}: si concentra sul \textbf{supporto alle decisioni}, indicando quale azione dovrebbe essere intrapresa per ottimizzare una misura (e.g. \emph{operations management}).
\end{itemize}

\subsection{Motivazione: perché considerare l’incertezza}

\paragraph{Previsioni puntuali e decisioni}
Un punto di partenza naturale nello studio delle decisioni in condizioni di incertezza è la \textbf{costruzione di una previsione puntuale} di una variabile aleatoria. Sia $X$ una v.a. reale, con distribuzione di probabilità nota. Una \textbf{previsione puntuale} consiste nella scelta di un valore $x \in \mathbb{R}$, che rappresenta una \textbf{stima ex ante} della realizzazione futura di $X$.
La \textbf{qualità di una previsione} deve essere misurata in funzione del \textbf{costo associato all’errore} di previsione, che dipende dalla discrepanza tra il valore scelto $x$ e la realizzazione effettiva di $X$.

\paragraph{Errore quadratico medio (MSE)}
Una scelta naturale per misurare il \textbf{costo dell’errore di previsione è la perdita quadratica}. In questo caso, il costo associato alla previsione puntuale $x$ è
\(
(X - x)^2.
\)
Il \emph{problema decisionale} consiste quindi nel \textbf{minimizzare \textcolor{NavyBlue}{l’errore quadratico medio}}:
\(
\mathbb{E}\big[(X - x)^2\big].
\)
Sviluppando il valore atteso, si ottiene
\[
MSE(x) \dot{=} \mathbb{E}\big[(X - x)^2\big]
= \mathbb{E}[X^2] - 2x\,\mathbb{E}[X] + x^2.
\]
La condizione di ottimalità del primo ordine implica che la \emph{previsione ottima coincide con il valore atteso della variabile aleatoria},
\(
x^\ast = \mathbb{E}[X].
\)
Questo risultato mostra che \textcolor{NavyBlue}{l’uso del valore atteso come previsione puntuale è ottimale solo sotto l’ipotesi di una perdita quadratica} (simmetrica).

\paragraph{Perdita assoluta e penalità asimmetriche}
Un criterio alternativo consiste nel misurare l’errore di previsione tramite la \textcolor{NavyBlue}{\textbf{deviazione assoluta}},
\(
\mathbb{E}\big[|X - x|\big].
\)
In questo caso, la soluzione ottima è data dalla \textbf{mediana della distribuzione} di $X$, anziché dal valore atteso.\\
Nelle applicazioni economiche, errori di previsione positivi e negativi possono avere impatti diversi. È quindi naturale introdurre una \textcolor{NavyBlue}{\textbf{funzione di perdita asimmetrica}} del tipo
\[
\mathbb{E}\big[c_u (X - x)^+ + c_o (X-x)^-\big],
\]
dove $(\cdot)^+ = \max\{0,\cdot\}$ e $(\cdot)^- = \max\{0,-\cdot\}$, mentre $c_u$ e $c_o$ rappresentano rispettivamente il \textbf{costo di sottostima e di sovrastima}.
La soluzione ottima è un \emph{quantile della distribuzione di $X$}, il cui livello dipende dal rapporto tra $c_u$ e $c_o$.\\
\hspace*{1.5em} Il \textcolor{NavyBlue}{\textbf{messaggio essenziale}} è che una \textcolor{NavyBlue}{previsione puntuale, tipicamente basata sul valore atteso, non è sufficiente per prendere decisioni ottimali in condizioni di incertezza}. 
La previsione ottimale dipende dalla funzione di perdita adottata e quindi dall’impatto economico dell’errore di previsione. 
In generale, \emph{non esiste una previsione “migliore” in senso assoluto, ma solo previsioni coerenti con uno specifico criterio decisionale.}

\paragraph{Modelli decisionali in ambito business}
L’obiettivo non è la previsione di una variabile aleatoria, ma la \textbf{selezione di un vettore di decisioni} $x \in X$ che ottimizzi una funzione economica in presenza di \emph{fattori di rischio}. 
Se $\xi$ denota un vettore di variabili aleatorie che influenzano il risultato economico, il problema può essere formulato come un \textcolor{NavyBlue}{\textbf{problema di ottimizzazione sotto incertezza}}
\[
\min_{x \in S} f(x,\xi).
\]
\hspace*{1.5em} Se non si dispone di informazioni probabilistiche sui fattori di rischio e si conosce solo un insieme di incertezza $U$, 
e il set di ammissibilità $S$, il problema assume la forma di un \textcolor{NavyBlue}{\textbf{worst-case robust optimization problem}}
\[
\min_{x \in S} \; \max_{\xi \in U} f(x,\xi).
\]
\hspace*{1.5em} Se invece i fattori di rischio sono modellati come variabili casuali con distribuzione nota, si ottiene un \textcolor{NavyBlue}{\textbf{problema di ottimizzazione stocastica}}
\[
\min_{x \in S} \; \mathbb{E}_\mathbb{P}\big[f(x,\tilde{\xi})\big].
\]
Questa formulazione mette in evidenza che, in generale,
\(
\mathbb{E}_\mathbb{P}\big[f(x,\tilde{\xi})\big] \neq f\big(x,\mathbb{E}_\mathbb{P}[\tilde{\xi}]\big),
\)
e giustifica la necessità di \emph{modelli decisionali che tengano esplicitamente conto dell’incertezza}.
La formulazione di questo  modello  suggerisce un problema decisionale statico, in base al quale prendiamo una
decisione $x$ prima della realizzazione della variabile casuale $\tilde{\xi}$, ma non c'è modo di
adattare la decisione dopo il verificarsi dell'evento casuale. 

\paragraph{Differenza tra rischio e incertezza} Il \textcolor{NavyBlue}{\textbf{rischio}} è definito come la presenza di variabili aleatorie con distribuzione di probabilità nota, mentre l'\textcolor{NavyBlue}{\textbf{incertezza}} si riferisce a situazioni in cui tali distribuzioni non sono note o non possono essere stimate con precisione. 

\subsection{Modelli decisionali statistici}
\paragraph{Il modello classico del newsvendor}
Il \textcolor{NavyBlue}{\textbf{modello classico del \emph{newsvendor}}} rappresenta un problema di decisione sotto condizione di incertezza in cui \emph{una decisione deve essere presa prima dell’osservazione di una variabile aleatoria}. 
Sia $D$ una variabile casuale non negativa che rappresenta la \textbf{domanda}, con distribuzione di probabilità nota. 
Il decisore sceglie una \textbf{quantità $q \ge 0$}, a \textbf{prezzo $c$} prima di osservare la realizzazione di $D$.
Ogni pezzo viene venduto a un \textbf{prezzo $p>c$} durante la finestra di vendita, e ad un \textbf{prezzo ridotto} successivamente $p_u<c$.\\
L'intuizione potrebbe suggerire di porre semplicemente $q=\mathbb{E}\big[D\big]$ per massimizzare il valore atteso del profitto. Ma questo risulta essere sbagliato. In realtà, il risultato non
sorprende se introduciamo due tipi di costo:
\begin{itemize}[label=-]
    \item Se $q<D$, avremo un \textcolor{NavyBlue}{\textbf{costo opportunità}} $m=p-c$, cioè il margine di profitto per la parte di domanda non soddisfatta (\textbf{shortfall}).
    \item Se $q>D$, avremo un \textcolor{NavyBlue}{\textbf{costo dell'invenduto}} $c_u=c-p_u$, legata alla svendita della giacenza residue (\textbf{surplus}).
\end{itemize}

\paragraph{Espressione generale del profitto e riscrittura con costi di underage/overage}
La \textcolor{NavyBlue}{\textbf{formulazione tramite funzione di perdita}} può essere affiancata da una scrittura esplicita del profitto, utile per evidenziare la decomposizione in termini di \emph{underage} e \emph{overage}. L’espressione generale del profitto è
\[
\pi(q,d) = -cq + p\min(q,d) + p_u\max(q-d,0) = -cq +p\min(q,d)+p_u(q-d)^+.
\]
Usando l’identità
\(
q = \min(q,d) + (q-d)^+,
\)
il profitto può essere riscritto come
\(
\pi(q,d) = c_u \min(q,d) - c_o (q-d)^+,
\)
dove definiamo il costo di underage $c_u = p-c$ e il costo di overage $c_o = c-p_u$.\\ 
Se l'\textbf{incertezza} sulla \textbf{domanda} è modellata da una v.a. continua con densità
$f_D(x)$, il \textbf{profitto atteso} è
\[
\mathbb{E}[\pi(q,D)]
= c_u\left(\int_0^q x f_D(x)\,dx + \int_q^{+\infty} q f_D(x)\,dx\right)
- c_o \int_0^q (q-x) f_D(x)\,dx.
\]

\paragraph{Teorema -- Regola di Leibniz}
Consideriamo una funzione di due variabili $g(q,x)$ e definiamo una funzione della sola $q$ come
\[
G(q)=\int_{h_1(q)}^{h_2(q)} g(q,x)\,dx.
\]
Notiamo che anche gli estremi di integrazione sono funzioni di $q$. Sotto opportune
ipotesi di continuità, la \textcolor{NavyBlue}{\textbf{regola di Leibniz}} permette di scrivere
\[
\frac{dG}{dq}(q)=\int_{h_1(q)}^{h_2(q)} \frac{\partial g}{\partial q}(q,x)\,dx
+ g\big(q,h_2(q)\big)\,h_2'(q) - g\big(q,h_1(q)\big)\,h_1'(q).
\]
Nel caso del \textbf{profitto atteso del newsvendor}, tramite la regola di Leibniz otteniamo
\[
\frac{d\,\mathbb{E}[\pi(q,D)]}{dq}
= c_u\left(q f_D(q) + \int_{q}^{+\infty} f_D(x)\,dx - q f_D(q)\right)
- c_o \int_{0}^{q} f_D(x)\,dx
\]
\[
= c_u\int_{q}^{+\infty} f_D(x)\,dx - c_o\int_{0}^{q} f_D(x)\,dx
= c_u\big(1-F_D(q)\big) - c_o F_D(q)=0,
\]
dove
\(
F_D(x)\dot{=}\mathbb{P}\{D\le x\}
\)
è la \emph{funzione di distribuzione cumulativa} della domanda.
Ponendo a zero la derivata prima, ricaviamo immediatamente
\[
F_D(q^\ast)=\frac{c_u}{c_u+c_o}.
\]
Come verifica, risulta utile controllare la \textbf{derivata seconda}
\[
\frac{d^2 \mathbb{E}[\pi(q,D)]}{dq^2}
= -c_u f_D(q) - c_o f_D(q) < 0 \qquad \forall q,
\]
in quanto la funzione di densità non può assumere valori negativi.
Ciò dimostra la \textcolor{NavyBlue}{\textbf{concavità del profitto atteso rispetto a $q$}}.

\paragraph{Modelli con vincoli probabilistici (chance-constrained models)}
L’idea alla base dei \emph{chance-constrained models} è che un \textcolor{NavyBlue}{\textbf{vincolo stocastico non debba necessariamente essere soddisfatto in ogni realizzazione dell’incertezza}}, ma solo con una probabilità sufficientemente elevata. In particolare, un vincolo del tipo
\(
g(x,\tilde{\xi}) \le 0
\)
è considerato \emph{accettabile} se risulta soddisfatto con probabilità almeno pari a un livello prefissato. 
Sono considerati \textbf{vincoli probabilistici individuali} o \textbf{congiunto}
\[
\mathbb{P}\{g_j(x,\tilde{\xi}) \le 0\} \ge 1-\alpha_j, \; j \in [m], \qquad
\mathbb{P}\{\mathbf{g}(\mathbf{x},\tilde{\boldsymbol{\xi}}) \le \mathbf{0}_m\} \ge 1-\alpha,
\]
dove $\mathbf{g}$ è una \textbf{funzione vettoriale}.
L’intuizione potrebbe suggerire che, \emph{se le funzioni $g_j$ sono convesse rispetto a $x$ per ogni realizzazione di
$\tilde{\xi}$, allora anche il vincolo probabilistico dovrebbe preservare la convessità}. Tuttavia, \textcolor{NavyBlue}{\textbf{questo non è vero in
generale}}: \emph{l’insieme ammissibile di un problema con vincoli probabilistici può risultare non convesso},
in quanto definito come unione di insiemi ammissibili associati a diversi scenari. 
Per questo motivo, i modelli chance-constrained possono risultare difficili da trattare dal punto di vista computazionale 
e spesso vengono approssimati tramite formulazioni di tipo robusto.
 Nonostante ciò, essi rappresentano uno strumento naturale per modellare problemi statici in cui non è possibile 
 adattare le decisioni dopo la realizzazione dell’incertezza.

\subsection{Alberi decisionali}
\paragraph{Dalle decisioni statiche a quelle adattive}
Un modo naturale per introdurre \textbf{decisioni adattive in condizioni di incertezza} è l’utilizzo degli \textbf{alberi di decisione}.
Gli alberi di decisione \emph{consentono di rappresentare in modo esplicito la sequenza temporale delle decisioni
e delle realizzazioni aleatorie}, evidenziando come le decisioni possano essere adattate sulla base delle informazioni
che si rendono disponibili nel tempo. Un albero di decisione è costituito da due tipi fondamentali di nodi
\begin{itemize}[label=-]
    \item \textcolor{NavyBlue}{\textbf{Nodi decisionali}}, rappresentati da quadrati, che corrispondono a \textbf{scelte discrete tra alternative mutuamente esclusive}. In questi nodi il decisore deve selezionare una sola azione tra quelle disponibili.
    \item \textcolor{NavyBlue}{\textbf{Nodi casuali}}, rappresentati da cerchi, che descrivono la \textbf{realizzazione di esiti aleatori}. A ciascun esito $i$ è associata una probabilità $\pi_i$, e la somma delle probabilità degli esiti che partono da uno stesso nodo casuale è pari a 1.
\end{itemize}
\hspace*{1.5em} Un albero di decisione è composto da una \emph{sequenza di nodi decisionali e nodi casuali}, 
che in genere si \textbf{alternano}. L’albero termina con \textbf{nodi terminali}, rappresentati da punti, ai quali sono
associati i \textbf{risultati finali}, tipicamente espressi in termini monetari.

\paragraph{Strategia e criterio di ottimalità}
Risolvere un problema rappresentato mediante un albero di decisione significa individuare una \textbf{strategia}, ovvero una \emph{regola che specifica quale decisione assumere in ciascun nodo decisionale che può essere visitato}.  
Assumendo che i risultati abbiano natura monetaria, il criterio più naturale è la \textcolor{NavyBlue}{\textbf{massimizzazione del valore monetario atteso EMV}} (\emph{Expected Monetary Value}).\\
\hspace*{1.5em} Quando un nodo decisionale è seguito da nodi casuali, ciascun nodo casuale viene etichettato con il proprio \textbf{valore monetario atteso}. Questo consente di confrontare le alternative disponibili nel nodo decisionale.  
La procedura di soluzione procede \textbf{a ritroso nel tempo} (\textcolor{NavyBlue}{\emph{backward induction}}), iniziando dai nodi terminali e risalendo progressivamente verso la radice dell’albero. 
\emph{Un nodo può essere valutato solo quando tutti i suoi successori sono già stati valutati.}

\paragraph{Esempio: introduzione di un nuovo prodotto}
Un tipico esempio di applicazione degli alberi di decisione riguarda la scelta se \textbf{lanciare un nuovo prodotto}, vendere una licenza o acquisire \textbf{informazioni aggiuntive} tramite un’indagine di mercato.  
L’indagine fornisce informazioni che modificano le stato di conoscenza, ma non la probabilità di successo del prodotto. La soluzione ottimale si ottiene confrontando il valore monetario atteso delle diverse strategie, tenendo conto del costo dell’informazione e delle decisioni ottimali successive ai diversi esiti osservati.

\subsection{EVPI and VSS}
\paragraph{Problema statico di ottimizzazione stocastica}
Si consideri il valore ottimo di un problema di ottimizzazione stocastica statica:
\[
f^{*} = \min_{x \in S} \mathbb{E}_{P}\!\left[ f(x,\tilde{\xi}) \right].
\]
In questo contesto la decisione viene presa \textcolor{NavyBlue}{\textbf{here-and-now}}, prima della realizzazione dell’incertezza.\\
\hspace*{1.5em} Ci si può chiedere come migliorerebbe il valore ottimo se fosse possibile \textbf{posticipare la decisione} e osservare prima lo scenario realizzato. Formalmente, ciò equivale a scambiare l’operatore di minimo con il valore atteso:
\[
f^{*}_{PI} = \mathbb{E}_{P}\!\left[ \min_{x \in S} f(x,\tilde{\xi}) \right].
\]
Il pedice $PI$ indica che questo valore corrisponde all’ottimo ottenibile in presenza di \textbf{informazione perfetta}.

\paragraph{Valore atteso dell’informazione perfetta (EVPI)} 
Per un problema di minimizzazione vale la disuguaglianza
\(
f^{*}_{PI} \le f^{*},
\)
poiché l’accesso all’informazione perfetta non può peggiorare la soluzione.
La differenza tra i due valori ottimi definisce il \textcolor{NavyBlue}{\textbf{valore atteso dell’informazione perfetta}}:
\[
EVPI = f^{*} - f^{*}_{PI}.
\]
L’EVPI fornisce una misura quantitativa dell’\textbf{impatto dell’incertezza} sul problema decisionale. 
L’EVPI misura il beneficio teorico della \textbf{chiaroveggenza}. In pratica, l’accesso a informazione perfetta è estremamente raro; pertanto, l’EVPI va interpretato come un \textbf{limite superiore} al valore che l’informazione può avere.

\paragraph{Esempio: scelta di una strategia di investimento}
Un esempio illustrativo riguarda la scelta tra due strategie di investimento alternative, una \emph{aggressiva} e una \emph{conservativa}, in presenza di diversi stati possibili dell’economia.  
Nel caso \textcolor{NavyBlue}{\textbf{here-and-now}}, la decisione viene presa prima di osservare lo stato dell’economia e la strategia viene scelta massimizzando il rendimento atteso.  
Nel caso \textcolor{NavyBlue}{\textbf{wait-and-see}}, la strategia ottimale può essere selezionata dopo aver osservato lo scenario realizzato, ottenendo un valore atteso maggiore. La differenza tra i due valori rappresenta l’EVPI.

\paragraph{Soluzione a valore atteso}
Un approccio più semplice consiste nel \textbf{trascurare l’incertezza} e sostituire le variabili aleatorie con i loro valori attesi. In questo modo si ottiene il problema deterministico a valore atteso:
\[
f^{*}_{EV} = \min_{x \in S} f\!\left(x, \mathbb{E}_{P}[\tilde{\xi}]\right),
\]
la cui soluzione è detta \textcolor{NavyBlue}{\textbf{expected-value solution}} e viene indicata con $\bar{x}$.\\
\hspace*{1.5em} La soluzione $\bar{x}$ deve essere valutata nel contesto stocastico originale. Il costo risultante è la variabile aleatoria $f(\bar{x},\tilde{\xi})$, di cui si considera il valore atteso:
\[
f_{EEV} = \mathbb{E}_{P}\!\left[ f(\bar{x},\tilde{\xi}) \right].
\]
Il confronto corretto non è tra $f^{*}_{EV}$ e $f^{*}$, ma tra $f_{EEV}$ e $f^{*}$.
Infatti, il \textcolor{NavyBlue}{\textbf{valore della soluzione stocastica}} è definito come
\[
VSS = f_{EEV} - f^{*},
\]
nel caso di minimizzazione (con scambio dei termini nel caso di massimizzazione).  
È possibile dimostrare che $VSS \ge 0$. Quando il VSS è elevato, \emph{lo sforzo computazionale richiesto per risolvere il problema stocastico completo è giustificato dal miglioramento ottenuto rispetto alla soluzione deterministica a valore atteso.}

\paragraph{Collegamento con modelli multi-stadio}
Il concetto di VSS viene ulteriormente chiarito attraverso esempi numerici e rappresenta un ponte naturale verso i 
\textcolor{NavyBlue}{\textbf{modelli di ottimizzazione a due stadi e multi-stadio}}, nei quali le decisioni possono 
essere adattate dopo l’osservazione dell’incertezza.

\subsection{Un'introduzione a un modello a due stadi: Assemble-to-order (ATO)}
\paragraph{ATO} Si consideri un esempio didattico di \textbf{pianificazione della produzione} in cui i prodotti finiti sono ottenuti assemblando un insieme di componenti. Ogni prodotto finale è caratterizzato da un insieme di caratteristiche, per ciascuna delle quali sono disponibili opzioni alternative che richiedono componenti diversi. Dalla combinazione di un numero limitato di componenti può derivare un numero molto elevato di prodotti finali.\\
\hspace*{1.5em} Per questo motivo, non è possibile proteggersi dall’incertezza della domanda mantenendo scorte di prodotti
finiti. D’altra parte, una strategia puramente \textcolor{NavyBlue}{\textbf{make-to-order}} può risultare poco efficace a
causa dei lunghi tempi di approvvigionamento dei componenti. In un ambiente \textcolor{NavyBlue}{\textbf{assemble-To-Order}}, in cui
l’assemblaggio è rapido, è invece possibile mantenere a magazzino i componenti e assemblare i prodotti finali solo dopo 
aver osservato la domanda.\\
\hspace*{1.5em} Questo contesto suggerisce naturalmente una \textbf{strategia a due stadi}:
\begin{itemize}[label=-]
    \item Pianificazione della produzione dei componenti \textcolor{NavyBlue}{\textbf{here-and-now}}, sotto incertezza sulla domanda.
    \item Assemblaggio dei prodotti finali in modalità \textcolor{NavyBlue}{\textbf{wait-and-see}}, dopo l’osservazione degli ordini.
\end{itemize}

\paragraph{Modello deterministico a valore atteso}
\emph{Ignorando l’incertezza della domanda} e \emph{sostituendola con il valore medio}, si introducono le seguenti variabili decisionali
\begin{itemize}[label=-]
    \item $x_i \in \mathbb{Z}_+$, $i=1,\dots,n_i$, numero di componenti prodotti.
    \item $y_j \in \mathbb{Z}_+$, $j=1,\dots,n_j$, numero di prodotti finali assemblati.
\end{itemize}
Il \textcolor{NavyBlue}{\textbf{problema di massimizzazione del profitto}} può essere formulato così
\begin{align*}
\max \quad & - \sum_{i \in [n_i]} C_i x_i + \sum_{j \in [n_j]} P_j y_j \\[0.3em]
\text{s.t.} \quad
& \sum_{i \in [n_i]} T_{im} x_i \le L_m, 
&& m \in [n_m]  \\[0.3em]
& y_j \le \bar{d}_j,
&& j \in [n_j]  \\[0.3em]
& \sum_{j \in [n_j]} G_{ij} y_j \le x_i,
&& i \in [n_i]  \\[0.3em]
& x_i,\, y_j \in \mathbb{Z}_+,
&& i \in [n_i],\ j \in [n_j]. 
\end{align*}

\paragraph{Limiti della soluzione deterministica}
La soluzione ottenuta è facilmente interpretabile ma estremamente \textbf{sbilanciata}. Essa rappresenta una scommessa sulla domanda del prodotto più redditizio e può risultare molto rischiosa se la domanda effettiva si discosta dal valore medio.

\paragraph{Modello stocastico a due stadi}
Per tenere conto dell’incertezza, si introducono:
\begin{itemize}[label=-]
    \item Una domanda scenario-dipendente $d^s_j$ con probabilità $\pi_s$;
    \item Variabili di secondo stadio $y^s_j \in \mathbb{Z}_+$ per l’assemblaggio adattivo.
\end{itemize}
Il \textcolor{NavyBlue}{\textbf{modello stocastico a due stadi}} è
\begin{align*}
\max \quad 
& - \sum_{i \in [n_i]} C_i x_i 
  + \sum_{s \in [n_s]} \pi^s 
    \left( \sum_{j \in [n_j]} P_j y_j^s \right) \\[0.3em]
\text{s.t.} \quad
& \sum_{i \in [n_i]} T_{im} x_i \le L_m,
&& m \in [n_m] \\[0.3em]
& y_j^s \le \bar{d}_j^{\,s},
&& j \in [n_j],\ s \in [n_s] \\[0.3em]
& \sum_{j \in [n_j]} G_{ij} y_j^s \le x_i,
&& i \in [n_i],\ s \in [n_s] \\[0.3em]
& x_i, y_j^s \in \mathbb{Z}_+,
&& i \in [n_i] j \in [n_j],\ s \in [n_s].\\[0.3em]
\end{align*}
La soluzione stocastica produce quantità di componenti meno estreme e decisioni di assemblaggio che variano a seconda dello scenario. 
Le variabili di secondo stadio rappresentano piani di contingenza e non costituiscono output operativi immediati.

\paragraph{Confronto corretto tra le soluzioni}
Il confronto diretto tra $f^{*}$ e $f_{EV}$ non è significativo. Occorre fissare le decisioni al primo stadio e risolvere, 
per ciascuno scenario $S$, il \textcolor{NavyBlue}{\textbf{problema al secondo stadio}}
\begin{align*}
R^s(\mathbf{x}^\circ) = \max \quad 
& \sum_{j \in [n_j]} P_j y_j^s \\[0.3em]
\text{s.t.} \quad
& y_j^s \le d_j^{\,s},
&& j \in [n_j] \\[0.3em]
& \sum_{j \in [n_j]} G_{ij} y_j^s \le x_i^\circ,
&& i \in [n_i] \\[0.3em]
& y_j^s \in \mathbb{Z}_+,
&& j \in [n_j].
\end{align*}
Dove $R^s(\mathbf{x}^\circ)$ è il \textbf{ricavo ottimo} che otteniamo nello scenario $s$,
dato il vettore di decisione di \textbf{primo stadio} $\mathbf{x}^\circ$, sfruttando in modo
ottimale i componenti disponibili per soddisfare la domanda. Si noti che, in questo modello, la disponibilità dei componenti $\mathbf{x}^\circ$ è data,
a seconda dei casi, dal modello \textbf{stocastico} oppure dal modello
\textbf{deterministico}. In entrambi i casi, il \textbf{ricavo atteso} risultante è dato da
\[
\sum_{s \in S} \pi^s R^s(\mathbf{x}^\circ).
\]

\subsection{Modelli a due stadi}
\paragraph{Dall’ATO ai modelli a due stadi con ricorso}
Il modello \emph{assemble-to-order} fornisce una buona introduzione ai \textbf{modelli di programmazione lineare stocastica a due stadi con ricorso} (\emph{recourse}), che rappresentano l’esempio più semplice di modelli di ottimizzazione adattivi.
In un modello a due stadi si distinguono due tipi di decisioni:
\begin{itemize}[label=-]
    \item Decisioni \textbf{here-and-now} (primo stadio) $x$, che devono essere prese sotto incertezza (ad esempio al tempo $t=0$);
    \item Decisioni \textbf{wait-and-see} (secondo stadio) $y(\tilde{\xi})$, che possono essere prese dopo aver osservato la realizzazione dei fattori di rischio $\tilde{\xi}$ (ad esempio al tempo $t=1$).
\end{itemize}
Le decisioni di secondo stadio sono anche dette \textcolor{NavyBlue}{\textbf{decisioni di ricorso}}. Si osserva immediatamente che tali decisioni sono in realtà \textbf{politiche decisionali}, cioè funzioni che mappano la realizzazione dei fattori di rischio in una decisione ammissibile; pertanto appartengono a uno spazio \textbf{infinito-dimensionale}.

\paragraph{Formulazione annidata del modello}
Il modello può essere formulato come \textcolor{NavyBlue}{\textbf{due problemi di ottimizzazione annidati}}
\[
\begin{aligned}
\min_{x}\;& c^{T}x + Q(x) \\
\text{s.t. }\;& Ax=b \\
& x \ge 0,
\end{aligned}
\]
dove la \textbf{funzione di ricorso} è definita come
\(
Q(x) = \mathbb{E}_{\mathbb{P}}\!\left[Q(x,\tilde{\xi})\right].
\)
Il \textcolor{NavyBlue}{\textbf{problema al secondo stadio}} è
\[
\begin{aligned}
Q(x,\xi) = \min_{y}\;& q(\xi)^{T}y \\
\text{s.t. }\;& Wy = h(\xi) - T(\xi)x \\
& y \ge 0.
\end{aligned}
\]

\paragraph{Ricorso fisso e non-linearità indotta dal ricorso}
Nel problema di secondo stadio sia la decisione di primo stadio $x$ sia la realizzazione $\xi$ dei fattori di rischio
sono dati. Risolvendo il secondo stadio per ogni realizzazione $\xi$, si definisce implicitamente una funzione
$y(\tilde{\xi})$, mostrando che le variabili di secondo stadio sono effettivamente funzioni. In questa formulazione
la \textbf{matrice di ricorso} $W$ non dipende da variabili aleatorie e si parla di \textbf{ricorso fisso} 
(\emph{fixed recourse}). La formulazione evidenzia che \textcolor{NavyBlue}{\textbf{la programmazione lineare stocastica con ricorso è, in generale, 
un problema di programmazione non lineare}} (poiché $Q(x)$ dipende da $x$ tramite un problema di ottimizzazione 
interno).\\
\hspace*{1.5em}
La funzione $Q(x)$ è un valore atteso rispetto alla distribuzione congiunta di $\tilde{\xi}$; se le variabili aleatorie sono continue, essa è un integrale multidimensionale. Inoltre, tale integrale riguarda una funzione che non è nota in forma chiusa, poiché è definita implicitamente dalla soluzione di un problema di ottimizzazione. In molti casi di interesse pratico si possono tuttavia dimostrare proprietà utili della funzione di ricorso, in particolare la \textbf{convessità}.\\
\hspace*{1.5em}
Poiché le variabili di secondo stadio sono funzioni in uno spazio infinito-dimensionale, una strategia comune consiste 
nella discretizzazione tramite campionamento. Si genera un \textbf{albero di scenario} (\emph{fan}) in cui l’evento
 $\omega_{s}$ corrisponde alla realizzazione dello scenario $s\in S$, con $S$ insieme degli scenari.
  Se gli scenari sono \textbf{campionati via Monte Carlo semplice}, le probabilità sono uniformi: $\pi_{s}=1/|S|$ 
  (sono possibili anche metodi più sofisticati di generazione scenari).

\paragraph{Modello deterministico equivalente (LP a grande scala)}
La discretizzazione conduce a
\[
\begin{aligned}
\min\;& c^{T}x + \sum_{s\in S} \pi_{s}\, (q^{s})^{T}y^{s}\\
\text{s.t. }\;& Ax=b\\
& Wy_s + T_sx = h_s, \quad s\in S \\
& x,\; y_s\ge 0.
\end{aligned}
\]

\paragraph{Fattibilità del secondo stadio e concetti di ricorso completo}
Una questione centrale è stabilire se \textcolor{NavyBlue}{\textbf{il secondo stadio sia fattibile per ogni scelta delle variabili di primo stadio}} e 
per ogni realizzazione delle variabili aleatorie. Occorre restringere l’insieme delle decisioni \emph{here-and-now} al
 dominio in cui il secondo stadio è fattibile (equivalentemente, la funzione di ricorso è limitata; come usuale, a un 
 problema infeattibile si associa costo infinito). Riscrivendo i vincoli di collegamento come
\[
Wy_s=h_s-T_sx, \quad s\in S,
\]
il \textbf{secondo stadio è fattibile quando il termine noto può essere espresso come \textcolor{NavyBlue}{combinazione conica} delle colonne di $W$}. 
In tal caso si parla di \textcolor{NavyBlue}{\textbf{complete recourse}}. Se il ricorso completo non vale per decisioni
 arbitrarie di primo stadio, ma vale per le decisioni che soddisfano i vincoli del primo stadio, si parla di
  \textcolor{NavyBlue}{\textbf{relatively complete recourse}}.\\
\hspace*{1.5em} Nei problemi di business è spesso possibile introdurre flessibilità tramite penalità, in modo da evitare infeattibilità al secondo stadio. 

\subsubsection{The plant location model}

\paragraph{Descrizione del problema e rete bipartita}
Il classico \textcolor{NavyBlue}{\textbf{plant location model}} è il più semplice problema di \textbf{network design}. 
Si considera una rete bipartita con un insieme $P$ di nodi sorgente potenziali e un insieme $\cal{D}$ di nodi domanda. I nodi 
sorgente rappresentano impianti produttivi che possono essere aperti oppure no; i nodi domanda possono rappresentare 
magazzini regionali o centri retail. In ciascun nodo domanda si realizza una domanda aleatoria $D_{j}(\omega)$,
 $j\in \cal{D}$, che va soddisfatta al costo minimo.
Sono necessari i seguenti dati
\begin{itemize}[label=-]
    \item Per ogni $i\in P$, un \textbf{costo fisso di apertura} $f_i$ e una \textbf{capacità} $u_i$.
    \item Per ogni $j\in \cal{D}$ e \textbf{scenario} $s\in S$, una domanda $d^{s}_{j}$ (discretizzazione della domanda aleatoria).
    \item Per ogni arco $(i,j)$, un \textbf{costo unitario di trasporto} $c_{ij}$ (costi variabili lineari).
\end{itemize}
Il \textbf{problema è naturalmente a due stadi}: le \emph{decisioni di apertura impianti} devono essere prese sotto incertezza,
mentre le \emph{decisioni di trasporto} possono essere rimandate a dopo l’osservazione della domanda. Si introducono le variabili
\[
y_i =
\begin{cases}
1 & \text{se il nodo sorgente } i\in P \text{ è aperto}\\
0 & \text{altrimenti}
\end{cases}
\qquad
\text{e}
\qquad
x^{s}_{ij}\ge 0,
\]
dove $x^{s}_{ij}$ è il flusso da $i\in P$ a $j\in \cal{D}$ nello scenario $s\in S$.
Le variabili di localizzazione $y_i$ sono \textbf{di primo stadio} (design), mentre i flussi $x^{s}_{ij}$ sono \textbf{di secondo stadio} (controllo).
\\ \hspace*{1.5em} Si ottiene il seguente \textcolor{NavyBlue}{\textbf{modello MILP stocastico a due stadi con ricorso}}
\begin{align*}
\min \quad 
& \sum_{i \in \mathcal{P}} f_i y_i 
+ \sum_{s \in S} \pi^s 
\left(
\sum_{i \in \mathcal{P}} \sum_{j \in \cal{D}} c_{ij} x_{ij}^s
\right) \\[0.4em]
\text{s.t.} \quad
& \sum_{i \in \mathcal{P}} x_{ij}^s = d_j^s,
&& \forall s \in S,\ \forall j \in \mathcal{D} \\[0.3em]
& \sum_{j \in \mathcal{D}} x_{ij}^s \le R_i y_i,
&& \forall s \in S,\ \forall i \in \mathcal{P} \\[0.3em]
& x_{ij}^s \ge 0, y_i \in \{0,1\}.
\end{align*}


\paragraph{Scenari estremi e necessità di formulazioni elastiche}
È opportuno chiedersi \emph{cosa accade se viene incluso uno scenario estremo con domanda molto elevata}.
Potrebbe non essere nemmeno possibile soddisfare tale domanda; inoltre, la pianificazione della capacità verrebbe guidata da
 scenari estremi ma molto improbabili, producendo soluzioni eccessivamente costose. 
 In questi casi conviene ricorrere a formulazioni elastiche, permettendo \textbf{violazioni penalizzate dei vincoli}.

\paragraph{Modello elastico con domanda non soddisfatta penalizzata}
Sia $z^{s}_{j}\ge 0$ la \textbf{quantità di domanda non soddisfatta} nel nodo $j$ nello scenario $S$. 
Tali variabili entrano nell’obiettivo moltiplicate per un coefficiente di penalità $\beta_j$. Il modello diventa
\begin{align*}
\min \quad 
& \sum_{i \in \mathcal{P}} f_i y_i
+ \sum_{s \in S} \pi^s
\left(
\sum_{i \in \mathcal{P}} \sum_{j \in \mathcal{D}} c_{ij} x_{ij}^s
\right)
+ \sum_{s \in S} \pi^s
\left(
\sum_{j \in \mathcal{D}} \beta_j z_j^s
\right) \\[0.4em]
\text{s.t.} \quad
& \sum_{i \in \mathcal{P}} x_{ij}^s + z_j^s = d_j^s,
&& \forall s \in S,\ \forall j \in \mathcal{D} \\[0.3em]
& \sum_{j \in \mathcal{D}} x_{ij}^s \le R_i y_i,
&& \forall s \in S,\ \forall i \in \mathcal{P} \\[0.3em]
& x_{ij}^s \ge 0,\ z_j^s \ge 0, y_i \in \{0,1\}.
\end{align*}
La \textcolor{NavyBlue}{\textbf{quantificazione delle penalità}} $\beta_j$ dipende dal significato di $z^{s}_{j}$: 
può rappresentare \emph{domanda effettivamente non servita} (con penalità differenziate per priorità di mercato)
 oppure \emph{domanda soddisfatta tramite fornitori terzi} (con penalità pari al costo effettivo di tale opzione).

\paragraph{Osservazioni di modellazione}
Il modello considerato è solo un primo passo verso formulazioni più realistiche (costi di trasporto non lineari, più item). Inoltre si trascura la dimensione temporale e l’uso di scorte per gestire variabilità parzialmente prevedibile della domanda, che porterebbero a un modello multistadio più impegnativo. L’output rilevante del modello rimane la \textbf{scelta del network design}.

\subsubsection{Modello del newsvendor a due stadi}

\paragraph{Motivazione e struttura informativa}
Si consideri una \textcolor{NavyBlue}{\textbf{versione a due stadi e multi-item del newsvendor problem}}, soggetta a vincoli di capacità.
La struttura informativa può essere rappresentata da un albero a tre livelli: al nodo radice si decide il
 primo lotto; nei nodi intermedi si decide un secondo lotto sotto incertezza ridotta; nei nodi foglia si osservano
  le vendite effettive e si contabilizzano costi di overage/underage.
Ogni nodo foglia $n\in N_2$ ha esattamente un predecessore (antecedente) $\alpha(n)\in N_1$. Il nodo radice $0$ è l’antecedente diretto di ogni nodo in $N_1$. Le decisioni dell’ultimo livello sono in realtà variabili fittizie di contabilizzazione (surplus/shortfall penalizzati), non variabili di controllo o design.
Sono necessari i seguenti insiemi e parametri
\begin{itemize}[label=-]
    \item $I$: insieme degli item (SKU).
    \item $K_1$ e $K_2$: capacità massime (numero massimo di item producibili) nel primo e nel secondo run produttivo.
    \item $L_i$, $i\in I$: lotti minimi. Le variabili di lotto sono semicontinue: si può non produrre un item, ma se lo si produce esiste un minimo economicamente giustificato.
    \item Domanda $d^{n}_{i}$ per ciascun $i\in I$ e foglia $n \in N_2$, con probabilità associata $\pi^{n}$.
    \item Coefficienti di penalità di overage $c^{o}_{i}$ e underage $c^{u}_{i}$ per ciascun $i\in I$.
\end{itemize}
Si introducono le \textbf{variabili decisionali}
\begin{itemize}[label=-]
    \item $x^{n}_{i}$: quantità prodotta dell’item $i$ nel nodo $n\in\{0\}\cup N_1$, intera non negativa.
    \item Variabili binarie $z^{n}_{i}\in\{0,1\}$ per rappresentare la semicontinuità.
    \item Variabili di contabilizzazione $u^{n}_{i}$ (underage) e $v^{n}_{i}$ (overage) ai nodi foglia $n\in N_2$.
\end{itemize}
La formulazione \textcolor{NavyBlue}{\textbf{nel MILP stocastico}} è
\begin{align*}
\min \quad 
& \sum_{n \in \mathcal{N}_2} \pi^n
\left[
\sum_{i \in \mathcal{I}}
\left(
c_i^{o}\, o_i^n + c_i^{u}\, u_i^n
\right)
\right] \\[0.4em]
\text{s.t.} \quad
& \sum_{i \in \mathcal{I}} x_i^0 \le K_1 \\[0.3em]
& x_i^0 \ge m_i \delta_i^0, \qquad
  x_i^0 \le K_1 \delta_i^0
&& i \in \mathcal{I} \\[0.3em]
& \sum_{i \in \mathcal{I}} x_i^n \le K_2
&& n \in \mathcal{N}_1 \\[0.3em]
& x_i^n \ge m_i \delta_i^n, \qquad
  x_i^n \le K_2 \delta_i^n
&& i \in \mathcal{I},\ n \in \mathcal{N}_1 \\[0.3em]
& x_i^0 + x_i^{a(n)} = d_i^n + o_i^n - u_i^n
&& i \in \mathcal{I},\ n \in \mathcal{N}_2 \\[0.3em]
& x_i^n \in \mathbb{Z}_+, \qquad
  \delta_i^n \in \{0,1\}
&& i \in \mathcal{I},\ n \in \{0\} \cup \mathcal{N}_1 \\[0.3em]
& u_i^n,\ o_i^n \ge 0
&& i \in \mathcal{I},\ n \in \mathcal{N}_2.
\end{align*}

\subsubsection{Programmazione lineare stocastica multistadio con ricorso}
\paragraph{Generalizzazione ai modelli multistadio}
Le \emph{formulazioni multistadio sorgono come generalizzazione dei modelli a due stadi}.
Concettualmente, \textbf{si annidano funzioni di ricorso corrispondenti ai diversi stadi decisionali}.
In questo caso, le decisioni future possono dipendere dall’intera storia del processo stocastico dei fattori di rischio.
Si introduce la notazione
\[
\tilde{\xi}_{[t]} = (\tilde{\xi}_1,\tilde{\xi}_2,\dots,\tilde{\xi}_t),
\qquad
\tilde{\xi}_{[1]}=\tilde{\xi}_1.
\]
La decisione \emph{here-and-now} è $x_0\in X_0$ e si risolve
\[
\min_{x_0\in X_0}\left\{ f_0(x_0) + \mathbb{E}\big[Q(x_0,\tilde{\xi}_1)\big]\right\}.
\]
Il termine $Q(x_0,\xi_1)$ è definito ricorsivamente come
\[
Q(x_0,\xi_1)=\min_{x_1\in X_1(x_0,\xi_1)}
\left\{ f_1(x_1,\xi_1) + \mathbb{E}\!\left[Q\!\big(x_1,\tilde{\xi}_{[2]}\big)\mid \tilde{\xi}_1=\xi_1\right]\right\}.
\]
Allo stesso modo, per $t = 2, \ldots, T$, si ha
\[
Q_t\!\left(x_{t-1}, \tilde{\xi}_{[t]}\right)
=
\min_{x_t \in X_t(x_{t-1}, \tilde{\xi}_{[t]})}
\left\{
f_t\!\left(x_t, {\xi}_{[t]}\right)
+
\mathbb{E}
\left[
Q_{t+1}\!\left(x_t, \tilde{\xi}_{[t+1]}\right)
\;\middle|\;
\tilde{\xi}_{[t]} = \xi_{[t]}
\right]
\right\}.
\]
Possiamo definire la funzione di ricorso
\[
Q_{t+1}\!\left(x_t, {\xi}_{[t]}\right)
\;\doteq\;
\mathbb{E}
\left[
Q_{t+1}\!\left(x_t, \tilde{\xi}_{[t+1]}\right)
\;\middle|\;
\tilde{\xi}_{[t]} = \xi_{[t]}
\right],
\]
e riscrivere l’equazione precedente come
\[
Q_t\!\left(x_{t-1}, {\xi}_{[t]}\right)
=
\min_{x_t \in X_t(x_{t-1}, {\xi}_{[t]})}
\left\{
f_t\!\left(x_t, {\xi}_{[t]}\right)
+
Q_{t+1}\!\left(x_t, {\xi}_{[t]}\right)
\right\}.
\]
Nel problema dell’ultimo stadio, per $t = T$, si può semplicemente porre
\(
Q_T\!\left(x_T, {\xi}_{[T]}\right) \equiv 0.
\)\\
\hspace*{1.5em} Una formulazione alternativa,si ottiene enfatizzando
che le decisioni di ricorso sono in realtà funzioni (\emph{non anticipative}) del cammino
campionario osservato fino a quel momento, $x_t(\tilde{\xi}_{[t]})$.
Questo conduce alla formulazione funzionale
\[
\begin{aligned}
\min_{x_0,\,x_1(\cdot),\,\ldots,\,x_T(\cdot)}
\;&
\mathbb{E}
\Big[
f_0(x_0)
+
f_1\!\left(x_1(\tilde{\xi}_{[1]}), \tilde{\xi}_1\right)
+
\cdots
+
f_T\!\left(x_T(\tilde{\xi}_{[T]}), \tilde{\xi}_T\right)
\Big] \\[0.6em]
\text{s.t.}\quad
& x_0 \in X_0, \\[0.3em]
& x_t({\xi}_{[t]}),\, {\xi}_t \in
X_t\!\left(x_{t-1}({\xi}_{[t-1]}), {\xi}_{t-1}\right),
\qquad
t = 1, \ldots, T.
\end{aligned}
\]

\subsubsection{Un modello di pianificazione finanziaria multistadio con costi di transazione proporzionali} 
\paragraph{Asset--liability management su albero di scenario}
Si formula un modello di \textcolor{NavyBlue}{\textbf{asset--liability management}} in cui si scambia un insieme di
 strumenti finanziari (asset) per far fronte a un flusso di passività (liabilities). L’incertezza è rappresentata 
 mediante un albero di scenario.
\begin{itemize}[label=-]
    \item $L_n$ è la liability da soddisfare nel nodo $n\in N$.
    \item Non si considerano nuovi apporti di cassa lungo il percorso; l’unico modo per generare cassa è vendere asset.
    \item Gli asset sono indicizzati da $i\in I$ e $P^n_i$ è il prezzo dell’asset $i$ nel nodo $n$.
    \item Si considerano costi di transazione proporzionali (lineari): per acquistare o vendere si paga una percentuale $c$ del valore scambiato, sia in acquisto sia in vendita.
\end{itemize}
La radice è il nodo $n_0$, in cui si effettua la prima allocazione considerando le dotazioni iniziali $\bar{h}^{n_0}_i$.
 L’insieme delle foglie è $S$; i nodi di trading intermedi sono
\(
T = N \setminus \big(\{n_0\}\cup S\big).
\)
L’obiettivo è massimizzare l’utilità attesa della ricchezza terminale.
Si introducono  le variabili decisionali:
\begin{itemize}[label=-]
    \item $z^{n}_i$: quantità di asset $i$ acquistata nel nodo $n$.
    \item $y^{n}_i$: quantità di asset $i$ venduta nel nodo $n$.
    \item $x^{n}_i$: quantità detenuta di asset $i$ nel nodo $n$ dopo il ribilanciamento.
    \item $W^{\ell}$: ricchezza terminale nella foglia $s\in S$.
\end{itemize}
Le variabili $z^{n}_i$, $y^{n}_i$, $x^{n}_i$ sono definite per $n\in N\setminus S$ (nessun ribilanciamento alle foglie). Sia $U(\cdot)$ l’utilità non lineare della ricchezza.

\begin{align*}
\max \quad 
& \sum_{s \in S} \pi^s \, u(W^s) \\[0.4em]
\text{s.t.} \quad
& x_i^{n_0} = \bar{h}_i^{n_0} + z_i^{n_0} - y_i^{n_0},
&& i \in \mathcal{I} \\[0.3em]
& x_i^{n} = x_i^{a(n)} + z_i^{n} - y_i^{n},
&& i \in \mathcal{I},\ n \in \mathcal{T} \\[0.3em]
& (1-c)\sum_{i \in \mathcal{I}} P_i^{n} y_i^{n}
-
(1+c)\sum_{i \in \mathcal{I}} P_i^{n} z_i^{n}
= L^{n},
&& n \in \mathcal{N} \setminus S \\[0.3em]
& W^{s} = (1-c)\sum_{i \in \mathcal{I}} P_i^{s} x_i^{a(s)} - L^{s},
&& s \in S \\[0.3em]
& x_i^{n},\, z_i^{n},\, y_i^{n},\, W^{s} \ge 0.
\end{align*}

\paragraph{Osservazioni operative}
In pratica il modello verrebbe risolto ripetutamente con una logica \textcolor{NavyBlue}{\emph{rolling horizon}};
 il ruolo dell’utilità terminale è evitare effetti distorsivi di fine orizzonte, imponendo che 
 il portafoglio sia in una buona posizione alla fine dell’orizzonte di pianificazione. In un modello più realistico
  si dovrebbero includere eventuali contributi in ingresso e shortfall penalizzati rispetto alle liabilities.

\subsection{Modelli multiperiodali e multistadio}
\paragraph{Esplosione dell’albero di scenario nei modelli multistadio}
Un problema chiaro nei modelli di programmazione stocastica \textbf{multistadio} è la possibile 
\textbf{esplosione dell’albero di scenario}. Il numero di nodi necessario per rappresentare l’incertezza può diventare
 un fattore fortemente limitante.
Questo problema è meno rilevante per una struttura ad albero \textbf{lineare}.
 È però importante osservare che una struttura ad albero lineare 
  \textbf{non corrisponde a un modello multistadio}, bensì a un \textbf{modello a due stadi multiperiodale}.

\paragraph{Multiperiodo non significa multistage} È possibile avere un modello multiperiodale ma \textbf{statico} se tutte le decisioni sono prese \textbf{here-and-now} e poi implementate nei periodi successivi senza adattamento al flusso informativo.  
Al contrario, \textbf{multistage} implica \textbf{adattamento} delle decisioni nel tempo, soggetto a vincoli di \textbf{non-anticipatività} delle politiche decisionali.

\paragraph{Caso particolare del two-stage multiperiod}
La struttura è peculiare: è \textbf{a due stadi}, ma coinvolge \textbf{più periodi}. Apparentemente viola la non-anticipatività, perché dopo la prima diramazione successiva alla radice sarebbe possibile prevedere perfettamente il cammino futuro. Tuttavia, una tale struttura può avere senso quando:
\begin{itemize}[label=-]
    \item Si devono prendere \textbf{decisioni strutturali o di design} \textbf{here-and-now} che non possono essere adattate.
    \item Segue un flusso di decisioni di controllo nel tempo, per le quali non si può trarre vantaggio dalla struttura lineare dell’albero.
\end{itemize}
Il modello di pianificazione energetica della sezione successiva illustra un caso di questo tipo.

\subsubsection{Un modello multiperiodo a due stadi: Unit commitment}
\paragraph{Domanda aleatoria come processo discreto nel tempo}
Si consideri un modello stocastico di \textcolor{NavyBlue}{\textbf{unit commitment}}, problema di pianificazione energetica
 in cui l’incertezza della domanda è modellata come processo stocastico a tempo discreto
\[
\xi(\omega)=\big(d_1(\omega),\dots,d_T(\omega)\big),
\qquad
t\in\{1,\dots,T\},\;\omega\in\Omega,
\]
dove $T$ è la lunghezza dell’orizzonte di pianificazione e $\Omega$ è l’insieme (finito) dei cammini campionari. 
Si denoti con $\pi^\omega$ la probabilità dello scenario $\omega$.

\paragraph{Classi di generatori e struttura dei costi}
Si dispone di un insieme di $I$ classi di unità generatrici. Una unità della classe $i$ (con $i=1,\dots,I$) 
può produrre in un intervallo $[m_i,M_i]$ quando è attiva. Per ciascuna classe $i$ sono disponibili $a_i$ unità.
\\ \hspace*{1.5em} Si consideri un insieme di $I$ classi di unità di generazione, che possono fornire
un livello di output nell’intervallo $[m_i, M_i]$, $i \in [I]$, quando sono attive.
Per ciascuna classe $i$ sono disponibili $a_i$ unità.
Anche in questo caso, la potenza erogata è una variabile decisionale
\emph{semicontinua}, che può assumere valore nullo, ma che presenta
un livello minimo $m_i$ per un’unità attiva.
Se un’unità di generazione della classe $i$ è attiva al tempo $t$, si sostiene
un costo $E_i$ per mantenerla al livello minimo $m_i$, e un costo variabile
lineare $C_i$ per ogni unità addizionale di potenza erogata.
Si sostiene inoltre un costo di avviamento $s_i$ per l’accensione di un’unità
della classe $i$.
\\ \hspace*{1.5em} Viene introdotta una variabile decisionale intera non negativa $u_{it}$ che denota
il numero totale di unità della classe $i$ attive durante l’intervallo temporale $t$.
Data la struttura dei costi, non risulta necessario rappresentare ciascuna singola unità.
Si considera inoltre una variabile intera non negativa $s_{it}$ che rappresenta
il numero di unità della classe $i$ che vengono accese all’inizio
dell’intervallo temporale $t$.
Tali variabili costituiscono decisioni di \emph{primo stadio}.
Al contrario, la variabile non negativa $q_{it}^{\omega}$, che rappresenta
l’output totale della classe $i$ durante l’intervallo temporale $t$,
è una variabile decisionale di \emph{secondo stadio}, indicizzata per scenario.
Il \textcolor{NavyBlue}{\textbf{modello}} può essere formulato come segue
\begin{align*}
\min \quad 
& \sum_{i \in [I],\, t \in [T]}
\left( E_i u_{it} + F_i s_{it} \right)
+ \sum_{\omega \in \Omega} \pi^{\omega}
\left[
\sum_{i \in [I],\, t \in [T]}
C_i \left( q_{it}^{\omega} - m_i u_{it} \right)
\right] \\[0.4em]
\text{s.t.} \quad
& \sum_{i \in [I]} q_{it}^{\omega} \ge d_t(\omega),
&& t \in [T],\ \omega \in \Omega \\[0.3em]
& m_i u_{it} \le q_{it}^{\omega} \le M_i u_{it},
&& i \in [I],\ t \in [T],\ \omega \in \Omega \\[0.3em]
& s_{it} \ge u_{it} - u_{i,t-1},
&& i \in [I],\ t \in [T] \\[0.3em]
& u_{it} \le a_i,
&& i \in [I],\ t \in [T] \\[0.3em]
& u_{it} \in \mathbb{Z}_+,\ 
s_{it} \in \mathbb{Z}_+,\ 
q_{it}^{\omega} \ge 0,
&& i \in [I],\ t \in [T],\ \omega \in \Omega.
\end{align*}

\paragraph{Osservazioni critiche sul modello}
Si notano due aspetti rilevanti
\begin{itemize}[label=-]
    \item \textbf{Fattibilità e scenari estremi.} Se il vincolo di domanda è imposto come vincolo rigido in ogni scenario, la soluzione può diventare molto costosa e guidata da pochi scenari improbabili con picchi di domanda elevati. Come nel plant location model, può essere opportuno introdurre una formulazione elastica, ad esempio includendo una unità “di ultima istanza” con capacità illimitata (fornitore esterno) o una penalità per domanda non soddisfatta.
    \item \textbf{Flusso informativo.} Il modello è multiperiodale ma a due stadi: dopo aver osservato la prima domanda al tempo $t=1$, si assume di conoscere tutte le domande per $t\in\{2,\dots,T\}$. Questo non è realistico e si potrebbe pensare a un modello multistadio con vincoli di non-anticipatività. Tuttavia, qui non c’è modo di sfruttare tale informazione, perché le variabili di design sono \emph{here-and-now}. Inoltre, nel modello non esiste uno stato energetico (l’energia non è immagazzinabile), e quindi i periodi sono indipendenti tra loro in termini di potenza erogata.
\end{itemize}

\subsection{Generazione di scenari e stabilità nella programmazione stocastica}
\paragraph{Ruolo centrale dell’albero di scenario}
Nei modelli di programmazione stocastica \textbf{multistadio}, l’elemento chiave è l’\textcolor{NavyBlue}{\textbf{albero di scenario}}:
 la qualità della soluzione dipende in modo critico da quanto bene l’albero rappresenta l’incertezza. In linea di principio si potrebbe campionare un modello dinamico dei fattori di rischio tramite \textbf{simulazione Monte Carlo}, ma sorgono ulteriori difficoltà.

\paragraph{Complicazioni principali}
\begin{itemize}[label=-]
    \item La \textcolor{NavyBlue}{\textbf{dimensione del campione}} deve restare contenuta, perché si associano variabili decisionali agli stati e risolvere un problema di ottimizzazione stocastica (non solo calcolare una media campionaria) può essere computazionalmente oneroso.
    \item Le decisioni devono essere \textcolor{NavyBlue}{\textbf{non-anticipative}}: ciò richiede una struttura ad albero soggetta a crescita \textbf{esponenziale}. Ad esempio, con 100 rami per nodo si ottengono un milione di scenari dopo tre passi.
    \item Non si sta solo stimando un valore atteso, ma si stanno \textcolor{NavyBlue}{\textbf{prendendo decisioni}} risolvendo un problema di ottimizzazione: qualunque \textbf{bias} o incoerenza nell’albero verrà sfruttata dal solver, producendo una soluzione magari buona \emph{in-sample} ma debole \emph{out-of-sample}.
\end{itemize}
Qualunque strategia di generazione scenari si scelga, è quindi importante controllare la \textbf{stabilità} della soluzione risultante.

\paragraph{Due classi di approcci alla generazione di scenari}
Si distinguono due approcci fondamentali
\begin{itemize}[label=-]
    \item \textcolor{NavyBlue}{\textbf{Generazione stocastica di scenari.}} Basata su campionamento casuale \textbf{Monte Carlo} e quindi sulla “forza bruta” di una grande numerosità campionaria. È più gestibile nei modelli \textbf{a due stadi} che nei multistadio; le prestazioni possono essere migliorate con tecniche di \textbf{riduzione della varianza}, ad esempio \emph{importance sampling}.
    \item \textcolor{NavyBlue}{\textbf{Generazione deterministica di scenari.}} Invece della forza bruta, si selezionano scenari in modo “intelligente” tramite metodi quali \textbf{quadratura gaussiana}, \textbf{sequenze a bassa discrepanza} (ad es.\ \textbf{Sobol}) o procedure di generazione \textbf{ottimizzata}.
\end{itemize}

\paragraph{Interpretazione come integrazione numerica (quadratura)}
La generazione scenari può essere vista come un problema di \textbf{integrazione numerica}. Se $f(x,\tilde{\xi})$ dipende dalle decisioni $x$ e da variabili aleatorie $\tilde{\xi}$ con densità congiunta $h(\xi)$, allora
\[
F(x)=
\int f(x,\xi)\,h(\xi)\,d\xi \approx \frac{1}{S}\sum_{s=1}^{S} f(x,\xi_s).
\]
In alternativa, si può usare un insieme (anche deterministico) di punti con pesi $\{(\pi_s,\xi_s)\}$:
\[
F(x)\approx \sum_{s=1}^{S'} \pi_s\, f(x,\xi_s),
\qquad S'<S,
\]
interpretando le coppie $(\pi_s,\xi_s)$ come una \textbf{distribuzione discreta} che approssima quella continua.

\paragraph{Moment/property matching e alternative}
Un’idea comune è scegliere scenari e probabilità in modo da riprodurre (approssimativamente) alcune \textbf{proprietà} della distribuzione, ad esempio i \textbf{momenti} (\emph{moment matching}). Questo approccio è stato criticato perché è possibile costruire distribuzioni diverse che condividono i primi momenti. Un’alternativa è utilizzare \textbf{metriche di distanza} tra distribuzioni: fissata una topologia dell’albero, si cercano valori e probabilità che minimizzano una distanza rispetto alla distribuzione “vera”, oppure si parte da un grande insieme di scenari e si applica \textbf{scenario reduction} per ottenere un albero più gestibile. D’altra parte, spesso ciò che conta non è la distanza tra distribuzioni, ma la \textbf{qualità della soluzione} (ad es.\ in un modello mean--variance può essere sufficiente approssimare bene i primi due momenti).

\paragraph{Linee guida pratiche per progettare l’albero}
\begin{itemize}[label=-]
    \item Se l’obiettivo è una decisione robusta di primo stadio, conviene spesso usare un branching \textbf{ricco al primo stadio} e più \textbf{limitato negli stadi successivi} (in finanza, imponendo che l’albero sia \textbf{arbitrage-free}).
    \item Si può ridurre la complessità usando \textbf{passi temporali non uniformi} (più fitti all’inizio, più radi in seguito).
    \item Si può limitare il numero di stadi \textbf{aumentando l’obiettivo} con un termine che valuta la qualità dello \textbf{terminale}, riducendo decisioni miopi e permettendo alberi più piccoli.
\end{itemize}

\subsubsection{In-- and out--of--sample stability}

\paragraph{Perché serve controllare la stabilità}
Qualunque strategia di generazione scenari venga impiegata, è importante verificare la \textbf{stabilità} della soluzione. In pratica, l’ottimizzazione usa un albero approssimato e quindi occorre capire quanto l’output dipenda da come l’albero è stato generato.

\paragraph{Problema “esatto” e problema approssimato}
Il problema stocastico “esatto” può essere scritto come
\[
\min_{x\in X}\; \mathbb{E}_{\mathbb{P}}\!\left[f(x,\tilde{\xi})\right],
\]
dove $P$ è la \textbf{misura vera} dei fattori di rischio. In pratica si risolve un problema basato su un albero di scenario approssimato $T$
\[
\min_{x\in S}\; \widehat{f}(x;T),
\]
dove $\widehat{f}$ rappresenta una stima del valore atteso indotta dall’albero.

\paragraph{Stabilità del valore obiettivo}
Campionando (o costruendo) alberi diversi si ottengono soluzioni ottime diverse e valori obiettivo diversi. Spesso è più informativo controllare la stabilità del \textbf{valore dell’obiettivo} (piuttosto che della soluzione), perché quando l’obiettivo è piatto soluzioni anche molto differenti possono avere prestazioni simili.

\paragraph{In-sample stability}
La \textcolor{NavyBlue}{\textbf{stabilità \emph{in-sample}}} richiede che, generando due alberi $T_1$ e $T_2$, i valori obiettivo ottenuti non differiscano troppo:
\(
\widehat{f}(x^{*}_{1};T_{1}) \approx \widehat{f}(x^{*}_{2};T_{2}).
\)
Se l’albero è generato in modo deterministico, si può comunque confrontare la stabilità variando leggermente la struttura di branching per verificare che sia sufficientemente ricca.

\paragraph{Out-of-sample stability}
La \textcolor{NavyBlue}{\textbf{stabilità \emph{out-of-sample}}} confronta la performance reale attesa delle soluzioni sotto la distribuzione vera:
\(
\mathbb{E}_{\mathbb{P}}\!\left[f(x^{*}_{1},\tilde{\xi})\right]
\approx
\mathbb{E}_{\mathbb{P}}\!\left[f(x^{*}_{2},\tilde{\xi})\right].
\)
Se gli alberi sono affidabili, le prestazioni reali non dovrebbero cambiare in modo significativo.

\paragraph{Valutazione out-of-sample: due stadi vs multistadio}
Nei modelli a \textbf{due stadi} la verifica out-of-sample è relativamente semplice: si fissa la soluzione di primo stadio e si risolvono molti problemi di secondo stadio corrispondenti a diverse realizzazioni di $\tilde{\xi}$. Nei modelli \textbf{multistadio}, invece, una valutazione realistica richiede tipicamente una simulazione \textbf{rolling horizon}, che è computazionalmente costosa.

\paragraph{Rolling horizon e shrinking horizon}
\begin{itemize}[label=-]
    \item \textcolor{NavyBlue}{\textbf{Sliding/rolling horizon}}: si risolve un problema con $H$ stadi, si applica la prima decisione, si osserva una realizzazione out-of-sample e si risolve di nuovo un problema con $H$ stadi a partire dal periodo successivo.
    \item \textcolor{NavyBlue}{\textbf{Shrinking horizon}}: si risolve con $H$ stadi, si applica la prima decisione, si osserva una realizzazione e poi si risolve con $H-1$ stadi, riducendo progressivamente l’orizzonte. Lo shrinking horizon è più economico quando l’orizzonte di valutazione è ben definito.
\end{itemize}

\paragraph{Test economico alternativo: scambio degli alberi}
Un controllo più economico consiste nel generare soluzioni da due alberi $T_1$ e $T_2$ e \textbf{valutarle scambiando gli alberi}, verificando se
\(
\widehat{f}(x^{*}_{1};T_{2}) \approx \widehat{f}(x^{*}_{2};T_{1}).
\)

\paragraph{Osservazione finale: limite operativo dello stochastic programming}
La necessità di valutazione out-of-sample evidenzia che la programmazione stocastica produce decisioni esplicite \textbf{associate ai nodi dell’albero}; non fornisce direttamente una politica generale facilmente applicabile quando l’incertezza si realizza fuori dagli scenari previsti. Approcci come \textbf{programmazione dinamica} e \textbf{decision rules} restituiscono invece decisioni in \textbf{feedback}, più adatte alla simulazione e alla valutazione delle politiche.

\subsection{Convessità nei modelli di programmazione stocastica}
\paragraph{Vincoli probabilistici congiunti}
Si consideri un vincolo probabilistico congiunto della forma
\(
\mathbb{P}\bigl(\{\omega \mid \mathbf{g}(\mathbf{x};\boldsymbol{\xi}(\omega)) \le \mathbf{0}\}\bigr) \ge 1 - \alpha,
\)
dove $\mathbf{g}$ è una funzione vettoriale.
Un punto $\hat{\mathbf{x}}$ è ammissibile se l’insieme
\(
\mathbb{S}(\hat{\mathbf{x}}) = \{\omega \mid \mathbf{g}(\hat{\mathbf{x}};\boldsymbol{\xi}(\omega)) \le \mathbf{0}\}
\)
ha misura di probabilità almeno pari a $1-\alpha$.\\
\hspace*{1.5em} Sia $\mathcal{F}$ il campo di tutti gli eventi e sia $\mathcal{G} \subseteq \mathcal{F}$ la collezione degli eventi con probabilità almeno $1-\alpha$.  
Allora $\hat{x}$ è \emph{ammissibile} se esiste almeno un evento $G \in \mathcal{G}$ tale che
\(
\mathbf{g}(\hat{\mathbf{x}};\boldsymbol{\xi}(\omega)) \le \mathbf{0}
\quad \forall \,\omega \in G,
\)
ossia
\[
\hat{\mathbf{x}} \in \bigcap_{\omega \in G}
\left\{\mathbf{x} \mid \mathbf{g}(\mathbf{x};\boldsymbol{\xi}(\omega)) \le \mathbf{0}\right\}.
\]

\paragraph{Struttura dell’insieme ammissibile}
L'\textcolor{NavyBlue}{\textbf{insieme ammissibile}} complessivo può quindi essere scritto come
\[
X =
\bigcup_{G \in \mathcal{G}}
\bigcap_{\omega \in G}
\left\{\mathbf{x} \mid \mathbf{g}(\mathbf{x};\boldsymbol{\xi}(\omega)) \le \mathbf{0}\right\}.
\]
Anche se per ogni $\omega$ l’insieme
\(
\{\mathbf{x} \mid \mathbf{g}(\mathbf{x};\boldsymbol{\xi}(\omega)) \le \mathbf{0}\}
\)
è convesso, non ci si può aspettare che l’insieme $X$ sia convesso in generale, poiché l’unione di insiemi convessi non è necessariamente convessa.
La convessità del problema complessivo dipende dal tipo di distribuzione (continua o discreta), dalla sua funzione di distribuzione cumulativa, dalla natura dei vincoli (congiunti o disgiunti) e dalla struttura delle funzioni in $\mathbf{g}$ (lineari, convesse/concave o arbitrarie).

\paragraph{Vincolo probabilistico lineare con distribuzione normale}
Si consideri un vincolo lineare casuale
\(
\mathbf{a}^T \mathbf{x} \le b,
\)
dove $\mathbf{a} \sim \mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$, e si richieda
\(
\mathbb{P}\{\mathbf{a}^T \mathbf{x} \le b\} \ge \eta.
\)
Per un dato vettore $\mathbf{x}$, la variabile casuale $\mathbf{a}^T \mathbf{x}$ è distribuita come
\(
\mathbf{a}^T \mathbf{x} \sim \mathcal{N}(\nu,\sigma^2),
\)
dove $\nu = \boldsymbol{\mu}^T \mathbf{x}$ e $\sigma^2 = \mathbf{x}^T \boldsymbol{\Sigma}\mathbf{x}$.\\
\hspace*{1.5em} Usando la funzione di distribuzione cumulativa della normale standard $\Phi(\cdot)$, il vincolo può essere riscritto come
\[
\mathbb{P}\!\left\{
\frac{\mathbf{a}^{T}\mathbf{x} - \nu}{\sigma}
\le
\frac{b - \nu}{\sigma}
\right\}
=
\Phi\!\left(\frac{b - \nu}{\sigma}\right)
\ge \eta
\;\Longleftrightarrow\;
\frac{b - \nu}{\sigma}
\ge
\Phi^{-1}(\eta).
\]
Assumendo $\eta > 0.5$, quindi $\Phi^{-1}(\eta) > 0$, il vincolo diventa
\[
\boldsymbol{\mu}^T \mathbf{x} + \Phi^{-1}(\eta)\,\|\boldsymbol{\Sigma}^{1/2}\mathbf{x}\|_2 \le b,
\]
che è un vincolo conico del secondo ordine.  
Ne segue che un problema di programmazione lineare con vincoli probabilistici disgiunti di questo tipo è un \textcolor{NavyBlue}{\textbf{problema convesso di tipo SOCP}}.

\paragraph{Convessità della funzione di ricorso}
Si consideri ora la funzione di ricorso
\(
Q(\mathbf{x}) = \mathbb{E}[Q(\mathbf{x};\boldsymbol{\tilde{\xi}})]
\)
nel caso di costo di ricorso deterministico, con
\[
Q(\mathbf{x};\boldsymbol{\xi}) = \min_{\mathbf{y}}
\left\{
\mathbf{q}^T \mathbf{y} \mid \mathbf{W}\mathbf{y} = \mathbf{h}(\boldsymbol{\xi}) - \mathbf{T}(\boldsymbol{\xi})\mathbf{x},\; \mathbf{y} \ge \mathbf{0}
\right\}.
\]
Il dominio effettivo di $Q(\mathbf{x})$ è costituito dai vettori $\mathbf{x}$ ammissibili per i vincoli di primo stadio tali che $Q(\mathbf{x}) < +\infty$, ossia per cui il problema di secondo stadio è (quasi sicuramente) ammissibile.
\\ \hspace*{1.5em} Per \textcolor{NavyBlue}{\textbf{dualità della programmazione lineare}} si ottiene
\[
Q(\mathbf{x};\boldsymbol{\xi}) =
\max_{\boldsymbol{\pi}}
\left\{
[\mathbf{h}(\boldsymbol{\xi})-\mathbf{T}(\boldsymbol{\xi})\mathbf{x}]^T \boldsymbol{\pi}
\mid \mathbf{W}^T \boldsymbol{\pi} \le \mathbf{q}
\right\}.
\]
Si denoti con
\(
\Pi = \{\boldsymbol{\pi} \mid \mathbf{W}^T \boldsymbol{\pi} \le \mathbf{q}\}
\)
l’\textbf{insieme ammissibile del duale}, che è non casuale poiché $\mathbf{q}$ è deterministico.

\paragraph{Dimostrazione della convessità}
Siano $\mathbf{x}_1,\mathbf{x}_2 \in D$, $\lambda \in [0,1]$, e $\mathbf{x}_\lambda = \lambda \mathbf{x}_1 + (1-\lambda)\mathbf{x}_2$. Allora
Si considerino $\mathbf{x}^1,\mathbf{x}^2 \in \mathcal{D}$, $\lambda \in [0,1]$, e
$\mathbf{x}_\lambda = \lambda \mathbf{x}^1 + (1-\lambda)\mathbf{x}^2$.

\[
\begin{aligned}
Q(\mathbf{x}_\lambda)
&\overset{\mathrm{def}}{=}
\int Q(\mathbf{x}_\lambda,\boldsymbol{\xi})\, dP \\[1mm]
&=
\int \max_{\boldsymbol{\pi} \in \Pi}
\left\{
\big[\mathbf{h}(\boldsymbol{\xi}) - \mathbf{T}(\boldsymbol{\xi})\mathbf{x}_\lambda\big]^T
\boldsymbol{\pi}
\right\} dP \\[1mm]
&=
\int \max_{\boldsymbol{\pi} \in \Pi}
\left\{
\lambda
\big[\mathbf{h}(\boldsymbol{\xi}) - \mathbf{T}(\boldsymbol{\xi})\mathbf{x}^1\big]^T
\boldsymbol{\pi}
+
(1-\lambda)
\big[\mathbf{h}(\boldsymbol{\xi}) - \mathbf{T}(\boldsymbol{\xi})\mathbf{x}^2\big]^T
\boldsymbol{\pi}
\right\} dP \\[1mm]
&\le
\lambda
\int
\max_{\boldsymbol{\pi} \in \Pi}
\left\{
\big[\mathbf{h}(\boldsymbol{\xi}) - \mathbf{T}(\boldsymbol{\xi})\mathbf{x}^1\big]^T
\boldsymbol{\pi}
\right\} dP
+
(1-\lambda)
\int
\max_{\boldsymbol{\pi} \in \Pi}
\left\{
\big[\mathbf{h}(\boldsymbol{\xi}) - \mathbf{T}(\boldsymbol{\xi})\mathbf{x}^2\big]^T
\boldsymbol{\pi}
\right\} dP \\[1mm]
&=
\lambda Q(\mathbf{x}^1) + (1-\lambda) Q(\mathbf{x}^2).
\end{aligned}
\]
Pertanto la funzione di ricorso è convessa.

\paragraph{Osservazioni finali}
La dimostrazione precedente risulta particolarmente semplice poiché l’\textbf{insieme ammissibile del problema duale è non casuale}. In un contesto più generale, tuttavia, la \textbf{convessità della funzione di ricorso} può essere dimostrata anche senza questa assunzione.
In particolare, la funzione di ricorso è \textbf{tipicamente continua} quando le variabili aleatorie seguono \textbf{distribuzioni continue}, mentre nel caso di \textbf{distribuzioni discrete} essa assume una struttura \textbf{poliedrica}. In entrambi i casi è possibile applicare il \textbf{metodo dei piani di taglio di Kelley}. Inoltre, grazie alla \textbf{struttura del problema}, tali piani di taglio possono essere generati in modo efficiente tramite \textbf{strategie di decomposizione}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF 2 COMPLESSITA' COMPUTAZIONALE %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1cm}
\textcolor{RoyalBlue}{\section{CAP 2 -- Elementi di complessità computazionale}}
\paragraph{Recap}
La \textcolor{RoyalBlue}{\textbf{complessità computazionale}} di un problema dipende dalla
\textbf{dimensione dell’istanza}, intesa come \emph{lunghezza della sua codifica}, e può crescere in
modo \textbf{fattoriale}, \textbf{esponenziale} o \textbf{polinomiale}. Piccole modifiche nella struttura di un
problema possono determinare variazioni drastiche della complessità, come accade nei
problemi di scheduling $1//L_{\max}$ e $1/r_i/L_{\max}$.\\
\hspace*{1.5em}È fondamentale distinguere tra \textcolor{RoyalBlue}{\textbf{complessità di un algoritmo}} e
\textcolor{RoyalBlue}{\textbf{complessità intrinseca di un problema}}: la prima riguarda il numero di
operazioni elementari richieste da una procedura di soluzione, mentre la seconda riguarda
la difficoltà strutturale del problema stesso, indipendentemente dall’algoritmo utilizzato.\\
\hspace*{1.5em}La \textcolor{RoyalBlue}{\textbf{teoria della NP-completezza}} fornisce un quadro concettuale per
classificare i \textbf{problemi di decisione} in base alla loro difficoltà computazionale.
In particolare
\begin{itemize}[label=-]
\item La classe \textcolor{RoyalBlue}{\textbf{$P$}} contiene i problemi di decisione risolvibili in
\textbf{tempo polinomiale}.
\item La classe \textcolor{RoyalBlue}{\textbf{$NP$}} contiene i problemi di decisione per cui le istanze
con risposta positiva sono \textbf{verificabili in tempo polinomiale} su calcolatori non deterministici.
\item La classe \textcolor{RoyalBlue}{\textbf{$NPH$}} (NP-hard) raccoglie i problemi almeno difficili
quanto tutti i problemi in $NP$.
\item La classe \textcolor{RoyalBlue}{\textbf{$NPC$}} (NP-complete) contiene i problemi più difficili in
$NP$, cioè quelli che sono \emph{sia} in $NP$ \emph{sia} NP-hard.
\end{itemize}
\hspace*{1.5em}Il concetto chiave che permette di confrontare la difficoltà dei problemi è quello di
\textcolor{RoyalBlue}{\textbf{riduzione in tempo polinomiale}}: se un problema $P$ può essere
trasformato in un problema $Q$ in tempo polinomiale, allora $Q$ non può essere più facile
di $P$. Tutti i problemi della classe $NPC$ risultano \textbf{equivalenti in termini di complessità}.\\
\hspace*{1.5em}Il \textcolor{RoyalBlue}{\textbf{teorema di Cook}} individua l’\textbf{innesco della catena delle
riduzioni}, dimostrando che il \textbf{problema della soddisfacibilità Booleana} è NP-completo.
A partire da esso, mediante riduzioni polinomiali successive, è possibile costruire un’ampia
famiglia di problemi NP-completi, tra cui il problema \textbf{subset-sum}.\\
\hspace*{1.5em}La NP-completezza della versione decisionale di $1/r_i/L_{\max}$ implica che il corrispondente
\textbf{problema di ottimizzazione} è \textcolor{RoyalBlue}{\textbf{NP-difficile}}. In generale, dimostrare
che una versione decisionale è NP-completa è sufficiente per concludere che il problema di
ottimizzazione associato non ammette algoritmi polinomiali, salvo l’ipotesi $P = NP$.\\
\hspace*{1.5em}Un aspetto cruciale è infine la \textcolor{RoyalBlue}{\textbf{codifica dei dati}}: la complessità va
valutata rispetto alla rappresentazione binaria delle istanze. L’algoritmo di
programmazione dinamica per il problema \textbf{knapsack} ha complessità $O(nB)$ ed è quindi
\textcolor{RoyalBlue}{\textbf{pseudo-polinomiale}}, poiché dipende dal valore numerico dei dati e non
dalla lunghezza della loro codifica. Questo chiarisce perché il knapsack rimane un problema
NP-difficile nonostante l’esistenza di algoritmi efficienti in casi particolari.

\paragraph{Complessità di problemi e algoritmi}
Si vuole caratterizzare la \emph{complessità} in funzione della \emph{dimensione di un problema}. 
\textcolor{RoyalBlue}{Occorre distinguere la complessità degli algoritmi da quella dei problemi.} 
Nel caso di un algoritmo caratterizzato da un \textbf{numero finito di passi}, possiamo valutare
 (eventualmente nel caso peggiore) il \textbf{numero di operazioni elementari} in funzione della dimensione 
 $n$ del problema.
\begin{itemize}[label=-]
\item \textcolor{RoyalBlue}{\textbf{Algoritmi enumerativi}}: algoritmi che considerano tutte le \textbf{permutazioni} di $n$ oggetti. La loro complessità è $O(n!)$ ed è certamente non praticabile per valori di $n$ anche moderatamente grandi.
\item \textcolor{RoyalBlue}{\textbf{Algoritmi di assegnamento esaustivo}}: algoritmi che valutano tutti gli \textbf{assegnamenti possibili} di $n$ variabili binarie. In questo caso la complessità cresce in modo esponenziale ed è pari a $O(2^n)$.
\item \textcolor{RoyalBlue}{\textbf{Algoritmi polinomiali}}: un tipico esempio è rappresentato dagli \textbf{algoritmi di ordinamento} di $n$ oggetti. Gli algoritmi più semplici hanno complessità $O(n^2)$, mentre algoritmi più efficienti raggiungono complessità $O(n\log_2 n)$.
\end{itemize}
\hspace*{1.5em}Nel caso di \textbf{algoritmi iterativi} che generano una sequenza di soluzioni,
si può cercare di caratterizzare la \textcolor{RoyalBlue}{\textbf{velocità di convergenza}} (es., lineare o quadratica).

\paragraph{Complessità intrinseca di un problema}
Una questione più sottile si pone quando si vuole caratterizzare la \textcolor{RoyalBlue}{\textbf{complessità intrinseca di un problema}}.\\
\hspace*{1.5em} Per comprendere la natura della questione, consideriamo un semplice \textcolor{RoyalBlue}{\textbf{problema di scheduling di $n$ job su macchina singola}}. 
Indichiamo con $p_j$ il tempo necessario per il job $j = 1, \dots, n$, e con $d_j$, $j = 1, \dots, n$ la sua data di consegna (\textbf{due date}).
La soluzione è una \emph{sequenza di job}, ovvero una permutazione $\sigma$ in cui $\sigma(k)$ è 
l’indice del job in posizione $k$. I \textcolor{RoyalBlue}{\textbf{tempi di completamento}} sono
\[
\begin{aligned}
C_{\sigma(1)} &= p_{\sigma(1)},\\
C_{\sigma(k)} &= C_{\sigma(k-1)} + p_{\sigma(k)}, \quad k = 2, \dots, n.
\end{aligned}
\]
\hspace*{1.5em}Si vuole minimizzare la massima lateness,
\(
L_{\max} \dot{=} \max_{j \in [n]} L_j,
\)
dove
\(
L_j \dot{=} C_j - d_j.
\)
Tale problema viene indicato con la stringa \textcolor{RoyalBlue}{\textbf{$1//L_{\max}$}}.

\paragraph{Teorema (regola EDD -- Earliest Due Date)}
Per il problema $1//L_{\max}$ esiste una \textbf{soluzione ottima} in cui 
\textcolor{RoyalBlue}{$d_{\sigma(k)} \le d_{\sigma(k+1)}$}.\\
\hspace*{1.5em}\textbf{Dimostrazione}
Si consideri una soluzione ottima in cui, per due job consecutivi in sequenza, prima
$i = \sigma(k)$ e poi $j = \sigma(k+1)$, risulti $d_i > d_j$. Si indichino con $C_i$ e
$C_j = C_i + p_j$ i due tempi di completamento nella soluzione corrente, per la quale
si hanno due valori di lateness $L_i = C_i - d_i$ e $L_j = C_j - d_j$.\\
Scambiando i due job, si ottengono $C'_j < C_j$ e $C'_i = C_j$. Per il
job $j$, che viene anticipato, vale $C'_j < C_j$ e quindi $L'_j < L_j$; di conseguenza,
la lateness del job $j$ migliora nella nuova soluzione. Per il job $i$, che viene
spostato in avanti nella sequenza, risulta
\(
L'_i = C'_i - d_i = C_j - d_i < C_j - d_j = L_j,
\)
per cui la lateness del job $i$ peggiora, ma non supera la precedente lateness del job
$j$. La nuova soluzione risulta quindi migliore della precedente, in contraddizione
con l’ipotesi di ottimalità.

\paragraph{Introduzione dei tempi di rilascio}
Si dispone dunque di un algoritmo di complessità polinomiale per il problema.
\textcolor{RoyalBlue}{\textbf{Cosa accade se il problema viene leggermente complicato,
introducendo tempi di rilascio (release time o ready time) $r_i$, $i \in [n]$, per i
job?}}
Per il problema $1/r_i/L_{\max}$ non sono noti algoritmi di complessità polinomiale ed è
semplice costruire controesempi alla regola EDD (da adattare comunque alla
disponibilità dei job).\\
\hspace*{1.5em}Per il problema $1/r_i/L_{\max}$ esistono algoritmi
\textcolor{RoyalBlue}{\textbf{branch-and-bound}} (quindi di complessità esponenziale).
Non è noto un algoritmo di complessità polinomiale per questo problema di
ottimizzazione combinatoria (e per molti altri), ma non è nemmeno stato dimostrato
che tale algoritmo non possa esistere.

\subsection{Caratterizzazione della complessità -- classi P, NP, NPC e NPH}
\paragraph{Teoria della NP-completezza} Esiste una classe molto ampia di problemi di ottimizzazione per
cui non sono disponibili \emph{algoritmi
di complessità polinomiale}. Tuttavia, la questione dell’esistenza o meno di un algoritmo di
complessità polinomiale per essi rimane aperta.
La \textcolor{RoyalBlue}{\textbf{teoria della NP-completezza}} permette di dare una risposta parziale alla questione,
mostrando come questi problemi siano \emph{equivalenti tra di loro}, nel senso che \textcolor{RoyalBlue}{un algoritmo di 
complessità polinomiale per anche uno solo di essi fornirebbe un algoritmo di complessità
polinomiale per tutti i problemi di una classe molto ampia}.\\
Il fatto che decenni di ricerca sulla soluzione di tutti questi problemi non abbiano prodotto 
un algoritmo polinomiale \textbf{suggerisce che esso in effetti non esiste}.

\paragraph{Problemi di decisione e di ottimizzazione}
Occorre distinguere \textcolor{RoyalBlue}{\textbf{problemi di decisione}} e \textcolor{RoyalBlue}{\textbf{problemi di ottimizzazione}}.
Esempi di problema di decisione sono i seguenti.
\begin{enumerate}
    \item \textcolor{RoyalBlue}{\textbf{Problema $K_0$ (subset sum)}}
Dati $n+1$ numeri interi positivi $a_1, a_2, \dots, a_n$ e $b$, esiste un sottoinsieme 
$J \subseteq [n]$ tale che
\(
\sum_{i \in J} a_i = b
\)?
    \item \textcolor{RoyalBlue}{\textbf{Problema $LS_0$}}
Dato un \textcolor{RoyalBlue}{\textbf{problema di lot sizing multiprodotto}}, con tempi e costi di setup, esiste una
soluzione ammissibile rispetto al soddisfacimento della domanda e ai vincoli di capacità
produttiva?
\end{enumerate}
\hspace*{1.5em}Data una specifica istanza di un \emph{problema di decisione}, la risposta è \emph{sì} oppure \emph{no}.
Dal punto di vista teorico, è più agevole trattare problemi di decisione, \textcolor{RoyalBlue}{\textbf{ma è facile vedere il
legame tra problemi di ottimizzazione e problemi di decisione}}. Dato un problema di ottimizzazione
\textcolor{RoyalBlue}{\(
\min_{x \in S} f(x),
\)}
\textbf{è possibile definirne una versione decisionale}, scegliendo un numero $k$ e chiedendo se esiste
$x \in S$ tale che \textcolor{RoyalBlue}{$f(x) \le k$}, dove $k$ è un numero intero.
Si indica con \textcolor{RoyalBlue}{$PO$} il \textbf{problema di ottimizzazione}, e con \textcolor{RoyalBlue}{$PD$}
il corrispondente \textbf{problema di decisione}.\\
\hspace*{1.5em}\textcolor{RoyalBlue}{Se si ha a disposizione un algoritmo efficiente per $PO$, allora è possibile risolvere in modo 
efficiente anche $PD$}: basta risolvere il problema di ottimizzazione e verificare se 
$f(x^*) \le k$. Questo permette di scrivere
\textbf{\(
PD \rightarrow PO,
\)}
nel senso che \textbf{il problema di decisione può essere ricondotto al problema di ottimizzazione}.\\
\hspace*{1.5em}\textbf{Non è detto che tale trasformazione sia conveniente}, ma è possibile escludere che $PO$ sia facile
se $PD$ è difficile, perché un ipotetico algoritmo efficiente per $PO$ risolverebbe anche
facilmente $PD$.
Quindi, \textcolor{RoyalBlue}{\emph{per dimostrare che un problema di ottimizzazione è difficile, può bastare dimostrare che
è difficile il corrispondente problema di decisione}}.\\
\hspace*{1.5em}D’altro canto, un algoritmo di decisione efficiente potrebbe, in certi casi, essere utilizzato
per risolvere il problema di ottimizzazione. Se la funzione di costo in $PO$ assume valori interi
non negativi, e si ha un upper bound $U$ sul costo ottimo, è possibile applicare una \textcolor{RoyalBlue}{\textbf{procedura di
bisezione}}.

\paragraph{Classe P} Si indichi con \textcolor{RoyalBlue}{$P$} la \textbf{classe dei problemi di decisione per cui esiste un \textcolor{RoyalBlue}{algoritmo di complessità
polinomiale}}, in grado di risolvere tutte le possibili istanze del problema.
Con questo si vuole dire che il \emph{numero di passi}, e quindi la complessità temporale dell’algoritmo,
\emph{è limitata superiormente da una funzione polinomiale dello spazio di memoria necessario per
descrivere ogni istanza del problema}.
Un tale algoritmo è in grado di fare due cose: \textbf{generare una soluzione} e \textbf{verificarne la correttezza}.
Esistono problemi, come $K_0$, per cui la prima parte del compito è difficile, ma la seconda no.
Se si enumerasse, mediante un \emph{albero di ricerca}, tutti i possibili sottoinsiemi $J$, si potrebbe
verificare se una specifica istanza ha risposta positiva o negativa, ma chiaramente tale
algoritmo ha \textbf{complessità esponenziale}.\\
\hspace*{1.5em}Tuttavia, se si disponesse di un ipotetico \textcolor{RoyalBlue}{\textbf{calcolatore non deterministico}}, in grado di eseguire
un numero infinito di processi di calcolo in \emph{parallelo}, si sarebbe in grado di risolvere il 
problema in tempo polinomiale.

\paragraph{Classe NP}
Si definisce \textbf{classe $NP$} \textcolor{RoyalBlue}{l’insieme dei \textbf{problemi di decisione} le cui istanze che hanno risposta 
positiva sono \emph{verificabili} in \textbf{tempo polinomiale}}.\\
\hspace*{1.5em}Per definizione, $P \subseteq NP$. Una questione meno ovvia è se valga $P \equiv NP$ o
$P \subset NP$ in senso stretto. È ragionevole, da questo punto di vista, cercare di caratterizzare la sottoclasse dei problemi 
più difficili in $NP$.

\paragraph{Riduzione in tempo polinomiale}
Siano $P$ e $Q$ due problemi di decisione, per cui ogni istanza $I_P$ di $P$ può essere
trasformata in tempo polinomiale in un’istanza $I_Q$ di $Q$ tale che \emph{$I_P$ ha risposta \textbf{positiva}
se e solo se $I_Q$ ha risposta positiva}.
Si dice che $P$ è \textcolor{RoyalBlue}{\textbf{riducibile in tempo polinomiale}} a $Q$ (\(
P \prec Q.
\))
La notazione $P \prec Q$ sottolinea che \emph{la complessità di $P$ non è maggiore della
complessità di trasformare $P$ in $Q$ e poi risolvere $Q$},
\[
\text{compl}(P) \le \text{compl}(Q) + \text{compl}(P \to Q).
\]
\hspace*{1.5em}Se la \textbf{trasformazione ha una complessità trascurabile}, la riduzione di $P$ a $Q$ mostra
che $Q$ non è più facile di $P$. Se $P$ è difficile e $P \prec Q$, $Q$ non può essere facile.
Altrimenti, si potrebbe trasformare un’istanza di $P$ in una di $Q$, e poi applicare
l’algoritmo efficiente per $Q$.

\paragraph{Problemi NP-difficili}
Un \textbf{problema di decisione} $P$ è detto \textbf{NP-difficile} (\emph{NP-hard}) \emph{se ogni
problema nella classe $NP$ è riducibile a $P$}.
Si indichi con \textbf{$NPH$} la \textcolor{RoyalBlue}{\textbf{classe dei problemi NP-difficili}}.
 (Questa classe è estendibile ai \emph{problemi di ottimizzazione}.)

\paragraph{Problemi NP-completi}
Un \textbf{problema di decisione} $P$ è detto \textbf{NP-completo} se è in NP ed è NP-hard. 
Si indichi con NPC la classe dei \textcolor{RoyalBlue}{\textbf{problemi NP-completi}}.
Le implicazioni di tali definizioni sono
\begin{enumerate}
    \item Un problema NP-difficile non è più facile di un problema qualsiasi in NP.
    \item La classe NPC è la classe dei problemi più difficili in NP.
\end{enumerate}
\hspace*{1.5em}Per dimostrare che un problema di decisione P è NP-completo, occorre dimostrare che \emph{$P$ è in NP} e \emph{un problema NP-completo $Q$ può essere ridotto in tempo polinomiale a P}.
Si osserva che, $Q$ è NP-completo, $P \prec Q$, e trascurando la
complessità della trasformazione, questo implica
\(
\text{compl}(P) \le \text{compl}(Q).
\)
Ma il secondo passo della dimostrazione di NP-completezza, ovvero dimostrare che
$Q \prec P$, implica anche che
\(
\text{compl}(Q) \le \text{compl}(P).
\)
Le due disuguaglianze, sempre a meno della complessità della trasformazione,
mostrano, quindi, che
\textcolor{RoyalBlue}{\(
\text{compl}(Q) = \text{compl}(P).
\)}\\
\hspace*{1.5em}In altre parole, \textcolor{RoyalBlue}{\textbf{i problemi della classe NPC sono equivalenti in termini di complessità
computazionale, e un algoritmo polinomiale per uno di essi permetterebbe di risolverli
tutti in tempo polinomiale}}. Si avrebbe quindi \textcolor{RoyalBlue}{$P = NP$}, ipotesi non troppo plausibile a
causa dell’equivalenza di una vasta classe di problemi per i quali non è noto un
algoritmo di complessità polinomiale, nonostante essi siano stati oggetto di ampio
studio nel corso degli anni.\\
\hspace*{1.5em}Se si accettasse l’ipotesi \textcolor{RoyalBlue}{$P \neq NP$}, è possibile rappresentare le relazioni tra le classi
$P$, $NP$ e $NPC$ come in figura. Essa ipotizza una gerarchia per cui la classe $P$
sarebbe la classe dei problemi più facili in $NP$, e $NPC$ quella dei problemi più
difficili.
Tutti i problemi in $NP$ si possono trasformare nel problema $Q \in NPC$. Se
$P \in NP$, per dimostrare che $P \in NPC$, occorre trasformare $Q$ in $P$.

\paragraph{L'innesco della catena} Se si dimostrasse che un problema $P$ è in $NPC$, questo può a sua volta essere
trasformato in altri problemi, permettendo di ampliare la classe dei problemi noti in
$NPC$. Il punto critico, chiaramente, è trovare l’innesco della catena, ovvero \emph{il primo
problema in $NPC$, al quale tutti i problemi in $NP$ possono essere ricondotti}.\\
\hspace*{1.5em}Il \textbf{teorema di Cook} dimostra che il \textcolor{RoyalBlue}{\textbf{problema della soddisfacibilità}} soddisfa i requisiti
necessari e fornisce la soluzione: \emph{data una \textbf{formula Booleana in forma canonica disgiuntiva}, decidere se esiste
un assegnamento di valori ai suoi elementi che la rende vera}.
\emph{Per esempio}, la formula
\(
(A \ \text{or} \ B) \ \text{and} \ (\text{not}(A) \ \text{or} \ C);
\)
definita rispetto alle variabili Booleane $A$, $B$ e $C$, è soddisfatta se $B$ e $C$
sono entrambe vere. \emph{Al contrario},
\(
(A \ \text{or} \ B) \ \text{and} \ (\text{not}(A) \ \text{or} \ B) \ \text{and} \ (\text{not}(B))
\)
non può essere soddisfatta da alcun assegnamento di verità alle variabili.\\
\hspace*{1.5em}A partire dal problema della soddisfacibilità, si può ricavare per riduzioni polinomiali
successive una famiglia crescente di problemi NP-completi, compreso il problema
$K_0$, che può essere considerato come un cugino in versione decisionale del problema
knapsack.

\paragraph{Teorema} Il fatto che tale problema faccia parte della classe $NPC$ permette di dimostrare il
teorema seguente, che risolve la questione iniziale.
Si consideri una \textcolor{RoyalBlue}{versione decisionale del problema $1/r_i/L_{\max}$}: dati i tempi di
rilascio $r_i$, le date di consegna $d_i$ e i tempi di lavorazione $p_i$, tutti a valori
interi positivi, per $n$ job $J_i$, $i \in [n]$, \emph{\textcolor{RoyalBlue}{esiste una soluzione in cui nessun job è
completato in ritardo?}} Tale problema di decisione è \textbf{NP-completo}.\\
\hspace*{1.5em}\textbf{Dimostrazione}
Il problema è chiaramente in $NP$, poiché per una data sequenza è facile verificare
se i job vengono completati in tempo rispetto alle due date.
Dati gli interi positivi $a_j$, $j \in [n]$, si creano $n$ job $J_j$ con parametri
\[
r_j = 0, \quad p_j = a_j, \quad d_j = 1 + \sum_{k \in [n]} a_k, \qquad j \in [n].
\]
Si crei un ulteriore job $J_0$ con parametri
\(
r_0 = b, \quad p_0 = 1, \quad d_0 = b + 1.
\)
Perché tutti i job rispettino la data di consegna, è necessario che il job $J_0$ inizi la
lavorazione al tempo $t = b$. Inoltre, dato che la data di consegna degli altri job è
pari alla somma di tutti i tempi di lavorazione, la soluzione non può presentare periodi
di tempo in cui la macchina è ferma, prima di avere completato l’intero insieme di job.
Questo richiede che sia possibile individuare un sottoinsieme $J$ di job da schedulare
prima di $J_0$, in modo tale che
\[
\sum_{j \in J} p_j = r_0.
\]
Tale insieme risolve il problema \emph{subset-sum}.

\subsection{Dai problemi di decisione ai problemi di ottimizzazione}
\paragraph{Problemi di ottimizzazione} Il teorema dimostra che un problema di decisione legato al problema di ottimizzazione
$1/r_i/L_{\max}$ è NP-completo, \textbf{ma cosa è possibile dire del problema di ottimizzazione
stesso?} 
Per definizione, la classe $NPC$ contiene solo problemi di decisione. É però possibile
estendere le classi $P$ e $NPH$, includendo in esse anche problemi di ottimizzazione.\\
\hspace*{1.5em}I \textcolor{RoyalBlue}{\textbf{problemi di ottimizzazione}} per cui è noto un algoritmo di complessità polinomiale
stanno in $P$. Nella classe $NPH$ è possibile includere problemi di ottimizzazione ai quali si può
ricondurre un corrispondente problema di decisione.
\textbf{La versione decisionale di $1/r_i/L_{\max}$ si riduce chiaramente al
problema di ottimizzazione.}\\
\hspace*{1.5em}Inoltre, nella dimostrazione si è assunto che i dati fossero numeri interi. Ma il
problema a numeri interi può evidentemente essere ridotto al problema generale.
Quindi \textbf{il problema di scheduling $1/r_i/L_{\max}$ è
NP-difficile}.

\paragraph{L’impatto della codifica di un problema}
Non è possibile fare a meno di considerare l’\textcolor{RoyalBlue}{\textbf{impatto del
modo in cui si codifica un problema}}.
Sarebbe infatti errato, per esempio, associare a un problema \emph{knapsack} una dimensione
pari al numero di oggetti. La dimensione si riferisce a una codifica binaria che
comprende tutti i dati del problema.
Per mostrare la rilevanza di ciò, \textbf{si consideri un classico algoritmo di
programmazione dinamica} per la soluzione del problema knapsack
\[
\begin{aligned}
\max \quad & \sum_{k=1}^{n} v_k x_k \\
\text{s.t.} \quad & \sum_{k=1}^{n} w_k x_k \le B \\
& x_k \in \{0,1\}, \quad k = 1, \dots, n.
\end{aligned}
\]
\hspace*{1.5em}Si definisce la \textcolor{RoyalBlue}{\textbf{value function}}
\[
V_k(s) := \text{valore del sottoinsieme ottimale tra gli oggetti } \{k, k+1, \dots, n\},
\]
quando la capacità residua è $s$.
In sostanza, la value function assume che siano già state fatte scelte di inserimento
o meno degli oggetti da $1$ a $k-1$; a valle di tale selezione, si ha una capacità
residua $s$, e ci si chiede come utilizzarla al meglio per le scelte rimanenti. Se i dati
$w_k$ e $B$ del problema sono interi, lo sarà anche la capacità residua $s$.
Per \emph{risolvere il problema}, ovvero trovare il valore di $V_1(B)$, si applica una relazione
ricorsiva
\[
V_k(s) =
\begin{cases}
V_{k+1}(s), & 0 \le s < w_k, \\
\max \{ V_{k+1}(s), \, V_{k+1}(s - w_k) + v_k \}, & w_k \le s \le B.
\end{cases}
\]
Si tratta di una equazione funzionale con condizione terminale
\[
V_n(s) =
\begin{cases}
0, & 0 \le s < w_n, \\
v_n, & w_n \le s \le B.
\end{cases}
\]
\hspace*{1.5em}Occorre tabulare tutte le funzioni $V_k(s)$, $k = 1, \dots, n$, per valori interi di $s$,
che assume valori nel range da $0$ a $B$. Pertanto, \textbf{tale algoritmo ha complessità
$O(nB)$}.
Questo \emph{non implica che il problema knapsack abbia complessità polinomiale}: per
rappresentare il valore $B$ in aritmetica binaria bastano $\log_2 B$ bit.
Quindi \textbf{l’algoritmo, rispetto a tale codifica binaria, ha complessità esponenziale}. Se si
utilizzasse un \textbf{computer con una codifica unaria}, l’algoritmo considerato
avrebbe \textbf{complessità polinomiale}.
Si dice infatti che un algoritmo di questo tipo è \textcolor{RoyalBlue}{\textbf{pseudo-polinomiale}}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1cm}
\textcolor{CadetBlue}{\section{CAP 3 -- Metodi di decomposizione in ottimizazione}}
\paragraph{Recap}
I \textbf{metodi di decomposizione} servono a risolvere problemi \textbf{di grande scala} sfruttando \textcolor{CadetBlue}{\textbf{strutture favorevoli}}: permettono di \textcolor{CadetBlue}{\textbf{parallelizzare}} i calcoli, di gestire modelli di \textcolor{CadetBlue}{\textbf{ottimizzazione stocastica}} basati su scenari e di affrontare problemi combinatori difficili tramite una \textcolor{CadetBlue}{\textbf{sequenza di sottoproblemi più semplici}}.  
\\\hspace*{1.5em}La decomposizione nasce quando un problema non è pienamente separabile: in presenza di \textcolor{CadetBlue}{\textbf{fattori complicanti}} (vincoli di interazione o variabili di interazione) si ottiene una struttura \textcolor{CadetBlue}{\textbf{block-angular}}. Nel caso di \textcolor{CadetBlue}{\textbf{vincoli di interazione}} si può decomporre \textbf{rilassando} tali vincoli (ad esempio con \textbf{decomposizione lagrangiana/duale}); nel caso di \textcolor{CadetBlue}{\textbf{variabili di interazione}} si può decomporre \textbf{fissando} tali variabili (logica alla base di \textbf{Benders} e della \textbf{L-shaped}).  
\\\hspace*{1.5em}\textcolor{CadetBlue}{\textbf{Decomposizione duale e progressive hedging.}}
La \textbf{decomposizione duale} usa la \textcolor{CadetBlue}{\textbf{dualità}} per rendere separabile un problema: si \textbf{dualizza} un vincolo complicante introducendo un \textcolor{CadetBlue}{\textbf{moltiplicatore di Lagrange}} interpretabile come \textcolor{CadetBlue}{\textbf{prezzo ombra}} della risorsa; così il problema si spezza in sottoproblemi indipendenti e il moltiplicatore viene aggiornato in base allo squilibrio tra consumo e disponibilità (meccanismo tipo \textbf{domanda--offerta}). La convergenza può essere lenta e spesso si mira anche a \textcolor{CadetBlue}{\textbf{dual heuristics}}.  
Nella \textbf{programmazione stocastica multistadio} con formulazione a variabili per scenario, è cruciale imporre i \textcolor{CadetBlue}{\textbf{vincoli di non anticipatività}}: decisioni di scenari indistinguibili fino a un certo tempo devono coincidere. La \textbf{progressive hedging} sfrutta una decomposizione \textcolor{CadetBlue}{\textbf{per scenario}} e migliora la convergenza combinando \textcolor{CadetBlue}{\textbf{moltiplicatori}} e un \textcolor{CadetBlue}{\textbf{termine di penalità quadratica}} (interpretato come \textbf{regolarizzazione}); la scelta del parametro di penalità e dello schema di aggiornamento dei moltiplicatori è \textcolor{CadetBlue}{\textbf{critica}}. Il metodo è particolarmente utile per costruire \textbf{euristiche} in problemi \textbf{misti-interi} multistadio difficili.\\
\hspace*{1.5em}\textcolor{CadetBlue}{\textbf{Benders/L-shaped}}
La \textbf{decomposizione L-shaped} (equivalente alla \textbf{decomposizione di Benders} in programmazione stocastica a due stadi) separa il \textbf{primo stadio} dai \textbf{sottoproblemi di scenario} del secondo stadio: si risolve un \textcolor{CadetBlue}{\textbf{master}} che contiene un’approssimazione della funzione di ricorso e la si raffina con \textcolor{CadetBlue}{\textbf{cutting planes}} (tagli di \textbf{optimality} e di \textbf{feasibility}). In parallelo, l’idea dei \textbf{Kelley cutting planes} è costruire progressivamente un \textbf{bound inferiore} tramite iperpiani di supporto e iterare finché il gap tra \textbf{upper bound} e \textbf{lower bound} è sufficientemente piccolo. Varianti come la \textbf{regularized L-shaped decomposition} e generalizzazioni multistadio (\textbf{Nested Benders}, \textbf{Abridged nested Benders}, \textbf{Stochastic Dual Dynamic Programming}) mirano a migliorare \textcolor{CadetBlue}{\textbf{convergenza}} e scalabilità; nei metodi di \textbf{decomposizione stocastica} (\textbf{Higle e Sen}) si combina \textbf{campionamento} e \textbf{ottimizzazione} costruendo \textcolor{CadetBlue}{\textbf{tagli asintoticamente validi}}.  
\\\hspace*{1.5em}\textcolor{CadetBlue}{\textbf{Dantzig--Wolfe/column generation.}}
La \textbf{decomposizione di Dantzig--Wolfe} e la \textbf{generazione di colonne} affrontano modelli con moltissime variabili “utili” ma non enumerabili: si lavora con un \textcolor{CadetBlue}{\textbf{restricted master problem}} e si aggiungono nuove colonne risolvendo un \textcolor{CadetBlue}{\textbf{pricing problem}} (scelto per individuare colonne con \textbf{costo ridotto} favorevole). Nel \textbf{cutting stock} la formulazione a \textbf{pattern} elimina la \textcolor{CadetBlue}{\textbf{simmetria}} del modello diretto, ma richiede generazione dinamica dei pattern; lo schema è alla base di approcci \textbf{branch-and-price} e, più in generale, consente di “nascondere” vincoli complessi nel pricing e di risolvere problemi \textcolor{CadetBlue}{\textbf{realmente di grande scala}} quando i metodi standard (simplesso o interior point) non sono più competitivi.


\paragraph{Ruolo dei metodi di decomposizione}
I metodi di \textbf{decomposizione} svolgono un ruolo centrale nell’ottimizzazione, poiché consentono di
\begin{itemize}[label=-]
\item Sfruttare \textcolor{CadetBlue}{\textbf{strutture favorevoli}} del problema, come: sottoproblemi di rete e trasformazioni da problemi \textbf{NP-hard} a problemi risolvibili in \textbf{tempo polinomiale}.
\item \textcolor{CadetBlue}{\textbf{Parallelizzare}} la soluzione di problemi di grande scala.
\item Affrontare modelli di \textcolor{CadetBlue}{\textbf{ottimizzazione stocastica}} di grandi dimensioni basati su scenari.
\item Trattare problemi combinatori difficili mediante una \textcolor{CadetBlue}{\textbf{sequenza di sottoproblemi più semplici}}, eventualmente combinando diverse strategie di soluzione.
\item Evitare difficoltà di modellazione dovute a \textcolor{CadetBlue}{\textbf{vincoli molto complessi}}.
\end{itemize}

\paragraph{Problema separabile}
Si consideri il problema di ottimizzazione
\[
\begin{aligned}
\operatorname{opt} \quad & \sum_{j \in [n]} f_j(x_j) \\
\text{s.t.} \quad & x_j \in S_j .
\end{aligned}
\]
In questo caso il problema può essere \textbf{decomposto} in sottoproblemi indipendenti
\(
\operatorname{opt}_{x_j \in S_j} f_j(x_j),
\)
poiché i sottovettori $x_j$ sono soggetti a \textbf{vincoli indipendenti}.\\
\hspace*{1.5em}Nel caso di un \textcolor{CadetBlue}{\textbf{modello di programmazione lineare}}
\[
\begin{aligned}
\min \quad & c^T x \\
\text{s.t.} \quad & Ax = b, \\
                  & x \ge 0,
\end{aligned}
\]
la separabilità corrisponde a una struttura \textbf{a blocchi diagonali} della matrice $A$. 
Nella maggior parte dei casi reali, non si dispone di una struttura così favorevole e sono presenti \textbf{fattori complicanti}.

\paragraph{Struttura block-angular}
Un caso tipico è quello di un \textbf{problema LP di grande scala} con struttura \textcolor{CadetBlue}{\textbf{block-angular}} della matrice $A$:
\[
A =
\begin{bmatrix}
C_1 & C_2 & C_3 & \cdots & C_n \\
D_1 &     &     &        &     \\
    & D_2 &     &        &     \\
    &     & D_3 &        &     \\
    &     &     & \ddots &     \\
    &     &     &        & D_n
\end{bmatrix}
\qquad \text{o} \qquad
A =
\begin{bmatrix}
C_1 & D_1 &        &        &      &   \\
C_2 &     & D_2    &        &      &   \\
C_3 &     &        &  D_3   &      &   \\
\vdots &  &        &        &  \ddots &      \\
C_n &     &        &        &         &  D_n
\end{bmatrix}.
\]
\hspace*{1.5em}\textcolor{CadetBlue}{\textbf{Nel primo caso}} il fattore complicante è rappresentato da un insieme di \textbf{vincoli di interazione} che accoppiano i sottoproblemi e impediscono la decomposizione basata sulla struttura a blocchi diagonali.
La decomposizione diventa possibile se i vincoli di interazione vengono \textcolor{CadetBlue}{\textbf{rilassati}}
in qualche modo, ad esempio tramite la \textbf{decomposizione lagrangiana duale}.\\
\hspace*{1.5em}\textcolor{CadetBlue}{\textbf{Nel secondo caso}} il fattore complicante è costituito da
\textbf{variabili di interazione}, che impediscono la decomposizione. La decomposizione è possibile se si
\textcolor{CadetBlue}{\textbf{fissano}} le variabili di interazione, come avviene nella \textbf{decomposizione L-shaped}, 
che corrisponde alla \textbf{decomposizione di Benders} nella programmazione stocastica.

\paragraph{Classificazione dei metodi}
Esiste un ampio insieme di metodi di decomposizione tra loro correlati. Alcuni sono \textbf{esatti}, almeno in linea di principio, mentre altri conducono ad \textbf{algoritmi approssimati}: 
\textbf{rilassamento lagrangiano e decomposizione lagrangiana}, 
\textbf{euristiche duali}, 
\textbf{decomposizione di Dantzig--Wolfe}, 
\textbf{generazione di colonne}, 
\textbf{decomposizione gerarchica}, 
\textbf{mateuristiche} (da non confondere con le metaeuristiche), 
\textbf{decomposizione di Benders} per MILP con struttura speciale, 
\textbf{decomposizione L-shaped} nella programmazione stocastica a due stadi con ricorso, 
\textbf{progressive hedging} per la programmazione stocastica multistadio con ricorso e
\textbf{programmazione dinamica} (decomposizione basata sul tempo).


\subsection{Decomposizione duale e progressive hedging per la programmazione stocastica}
\subsubsection{Decomposizione duale}

\paragraph{Problema di partenza}
Per comprendere come la dualità possa essere utilizzata in un contesto semplice, si consideri il problema
\[
\begin{aligned}
\max \quad & \sum_{i=1}^n f_i(x_i) \\
\text{s.t.} \quad & \sum_{i=1}^n g_i(x_i) \le b, \\
                  & x_i \in S_i, \quad i = 1, \ldots, n .
\end{aligned}
\]
Le variabili decisionali $x_i$, $i = 1, \ldots, n$, possono essere interpretate come \textbf{attività} che generano un profitto $f_i(x_i)$ e consumano una quantità di risorsa $g_i(x_i)$.  
La \textcolor{CadetBlue}{\textbf{funzione obiettivo}} rappresenta il \textbf{profitto totale}, 
mentre il \textcolor{CadetBlue}{\textbf{vincolo}} è di \textcolor{CadetBlue}{\textbf{budget}} sulla risorsa.
La funzione obiettivo è misurata in termini monetari, mentre il parametro $b$ è espresso in unità di risorsa.  
Se fosse possibile eliminare il vincolo di budget, il problema risulterebbe immediatamente \textbf{decomponibile}.

\paragraph{Dualizzazione del vincolo di budget}
Il \textcolor{CadetBlue}{\textbf{vincolo di budget}} viene \emph{dualizzato} introducendo un
\textcolor{CadetBlue}{\textbf{moltiplicatore di Lagrange $\mu \ge 0$}} e scrivendo la \textbf{funzione lagrangiana}:
\[
L(x; \mu) =
\sum_{i=1}^n f_i(x_i)
+ \mu
\left(
b - \sum_{i=1}^n g_i(x_i)
\right)
=
\sum_{i=1}^n \bigl[f_i(x_i) - \mu g_i(x_i)\bigr]
+ \mu b.
\]
La funzione lagrangiana deve essere \textbf{massimizzata} rispetto alle variabili primali, ottenendo un insieme di
sottoproblemi indipendenti
\[
\max_{x_i \in S_i}
\bigl[
f_i(x_i) - \mu g_i(x_i)
\bigr]
\equiv \pi_i(x_i).
\]
Ogni sottoproblema consiste nel \emph{massimizzare il contributo di profitto netto}, dato dal profitto meno il costo della risorsa.
Il \textcolor{CadetBlue}{moltiplicatore} $\mu$ rappresenta un \textcolor{CadetBlue}{\textbf{prezzo ombra}} della risorsa, misurato in unità monetarie per unità di risorsa.  
In questo caso, il problema duale consiste nel \textbf{minimizzare} la funzione duale rispetto a $\mu$.
Date le soluzioni rilassate $x_i^\ast$, un sottogradiente della funzione duale è
\[
\sum_{i=1}^n g_i(x_i^\ast) - b.
\]
Questo valore è \textcolor{CadetBlue}{positivo} quando il \emph{budget viene superato}; in tal caso il prezzo della risorsa deve essere aumentato.  
Invece, il prezzo deve invece essere ridotto quando il budget non viene completamente utilizzato.
Il meccanismo risultante può essere interpretato come uno schema di \textbf{domanda--offerta}, in cui il prezzo della risorsa viene aggiornato in funzione dello squilibrio tra consumo e disponibilità.

\paragraph{Osservazioni sulla dual decomposition}
La \textcolor{CadetBlue}{\textbf{decomposizione duale può convergere lentamente}} nella pratica, ma può risultare efficace per alcuni problemi di grande scala con struttura particolare.  
Talvolta è sufficiente ottenere una soluzione di buona qualità: se da una decomposizione duale è possibile ricostruire una soluzione primale ammissibile, si ottiene una \textbf{dual heuristic}.\\
\hspace*{1.5em}I metodi lagrangiani possono essere integrati con \textcolor{CadetBlue}{\textbf{metodi a funzione di penalità}},
ottenendo schemi di \textbf{Lagrangiano aumentato}, ad esempio minimizzando
\[
f(x)
+ \sum_{i \in I} \lambda_i h_i(x)
+ \sigma \sum_{i \in I} h_i^2(x),
\]
nel caso di problemi con vincoli di uguaglianza.

\subsubsection{Copertura progressiva per la programmazione stocastica multistadio}
\paragraph{Rappresentazione dell’incertezza}
Nella \textbf{programmazione stocastica multistadio}, l’incertezza è rappresentata tramite \textbf{alberi di scenario}.  
In una \textcolor{CadetBlue}{\textbf{formulazione compatta del modello}}, si definiscono variabili decisionali $x_n$ 
associate ai nodi $n$ dell’albero.
In una \textcolor{CadetBlue}{\textbf{formulazione a variabili separate}}, si introducono variabili $x_t^s$, associate 
allo scenario $s$ al tempo $t$.
Nel secondo caso è necessario imporre esplicitamente i \textcolor{CadetBlue}{\textbf{vincoli di non anticipatività}}: 
variabili decisionali corrispondenti a scenari diversi allo stesso tempo $t$ devono assumere lo stesso valore se gli scenari sono indistinguibili a quel tempo.

\paragraph{Insiemi di scenari indistinguibili}
Si denoti con $\mathcal{F}_{s}^t$ l’insieme degli \textbf{scenari indistinguibili} da $s$ fino al tempo $t$.  
I vincoli di non anticipatività possono essere scritti come:
\(
x_{it}^s = x_{it}^{s'}, \quad \forall i,t,\; s,s' \in \mathcal{F}_s^t.
\)

\paragraph{Formulazione del problema multistadio} La \textbf{copertura progressiva} è una strategia che può essere
applicata a problemi di programmazione stocastica multistadio, possibilmente non lineari.
Si consideri un sistema dinamico con stato $z_t$ e equazione di stato:
\(
z_{t+1} = G_t(z_t, x_t, \xi_{t+1}), \quad t = 0,1,\ldots,T,
\)
dove $x_t$ è la \textbf{variabile di controllo} e $\xi_{t+1}$ è una variabile aleatoria.\\
\hspace*{1.5em}Sia $s = (\xi_1^s, \xi_2^s, \ldots, \xi_{T+1}^s)$ uno \textbf{scenario} con probabilità $\pi_s$.  
Il \textcolor{CadetBlue}{\textbf{problema di scenario individuale}} è
\[
\begin{aligned}
\min \quad 
& \sum_{t=0}^T \gamma^t f_t\!\left(z_t, x_t, \xi_{t+1}^s\right)
  + \gamma^{T+1} Q\!\left(z_{T+1}\right) \\
\text{s.t.} \quad
& z_{t+1} = G_t\!\left(z_t, x_t, \xi_{t+1}^s\right),
  \quad t = 0,1,\ldots,T, \\
& L_t\!\left(z_t\right) \le x_t \le U_t\!\left(z_t\right),
  \quad t = 0,1,\ldots,T .
\end{aligned}
\]
\hspace*{1.5em}Sia $\left(x_0^s, x_1^s, \ldots, x_T^s\right)$ la soluzione ottima del problema associato allo scenario $s$.
Ci si potrebbe chiedere se sia possibile aggregare le soluzioni individuali e scegliere
\(
\underline{x}_t = \sum_{s \in S} \pi_s x_t^s .
\)
Purtroppo, non vi è alcuna ragione per ritenere che tale scelta conduca a una soluzione ottima, o anche solo ammissibile.
Una \textbf{buona politica decisionale} dovrebbe sfruttare l’informazione disponibile, ma non può essere anticipativa.
Il problema multistadio può essere formulato all’interno di un framework di modellazione a variabili replicate per scenario (\emph{split-variable}),
introducendo i vincoli di non anticipatività in una forma opportuna.\\
\hspace*{1.5em}Sia $\{s\}_t$ l’insieme degli scenari che, fino all’istante di tempo $t$, non sono distinguibili dallo scenario $s$,
e sia $\mathbb{P}(\{s\}_t)$ la somma delle loro probabilità.
Il \textcolor{CadetBlue}{\textbf{problema di ottimizzazione stocastica}}complessivo può essere scritto come
\[
\begin{aligned}
\min \quad
& \sum_{s \in S} \pi_s
\left(
\sum_{t=0}^T \gamma^t f_t\!\left(z_t^s, x_t^s, \xi_{t+1}^s\right)
+ \gamma^{T+1} Q\!\left(z_{T+1}^s\right)
\right) \\
\text{s.t.} \quad
& z_{t+1}^s = G_t\!\left(z_t^s, x_t^s, \xi_{t+1}^s\right),
  \quad \forall s,\; t = 0,1,\ldots,T, \\
& L_t\!\left(z_t^s\right) \le x_t^s \le U_t\!\left(z_t^s\right),
  \quad \forall s,\; t = 0,1,\ldots,T, \\
& x_t^s =
\sum_{s' \in \{s\}_t}
\frac{\pi_{s'}x_t^{s'}}{\mathbb{P}(\{s\}_t)},
\qquad \forall s \in S,\; t = 0,1,\ldots,T .
\end{aligned}
\]
\hspace*{1.5em}In questo caso si utilizza una \emph{singola aspettativa condizionata} per ciascuno scenario e istante temporale,
anziché imporre uguaglianze a coppie tra le decisioni associate agli insiemi di scenari indistinguibili.
Introduciamo quindi dei \textbf{moltiplicatori di Lagrange} $w_t^s$ associati ai vincoli di non anticipatività.
La dualizzazione del problema conduce alla seguente formulazione
\[
\begin{aligned}
\min \quad
& \sum_{s \in S} \pi_s
\Bigg(
\sum_{t=0}^T \gamma^t
\Big[
f_t\!\left(z_t^s, x_t^s, \xi_{t+1}^s\right)
+ w_t^s \big( x_t^s - \underline{x}(\{s\}_t) \big)
\Big]
+ \gamma^{T+1} Q\!\left(z_{T+1}^s\right)
\Bigg),
\end{aligned}
\]
dove l’aspettativa condizionata
\(
\underline{x}(\{s\}_t) \;\equiv\;
\sum_{s' \in \{s\}_t}
\frac{\pi_{s'}x_t^{s'}}{\mathbb{P}(\{s\}_t)}, 
\)
può essere interpretata come una proiezione sull’insieme delle politiche non anticipative.\\
\hspace*{1.5em}Il problema può essere \textcolor{CadetBlue}{riordinato e decomposto per scenario}.
Al fine di migliorare la convergenza, è possibile aggiungere un \textcolor{CadetBlue}{\textbf{termine di penalità quadratica}},
ottenendo un metodo di tipo \emph{Lagrangiano aumentato}.
Purtroppo, l’introduzione diretta del termine di penalità distruggerebbe la struttura di decomposizione.
Tuttavia, è possibile utilizzare la soluzione aggregata per scenario $\underline{x}(\{s\}_t)$ ottenuta
all’iterazione precedente, il che conduce (trascurando i termini costanti) al seguente problema
per il singolo scenario
\[
\begin{aligned}
\min \quad
& \sum_{t=0}^T \gamma^t
\Bigg\{
f_t\!\left(z_t^s, x_t^s, \xi_{t+1}^s\right)
+ w_t^s x_t^s
+ \frac{\rho}{2}
\big[ x_t^s - \underline{x}(\{s\}_t) \big]^2
\Bigg\}
+ \gamma^{T+1} Q\!\left(z_{T+1}^s\right).
\end{aligned}
\]
\hspace*{1.5em}Il \textcolor{CadetBlue}{\emph{termine quadratico}} può essere interpretato come un
\textcolor{CadetBlue}{\textbf{termine di regolarizzazione}}.
La scelta del \textbf{parametro di penalità $\rho$} e dello schema di aggiornamento dei moltiplicatori
è critica ai fini della convergenza.
Ciononostante, il metodo di \emph{progressive hedging} risulta utile per sfruttare la struttura
del problema ed è stato ampiamente utilizzato per costruire euristiche per problemi stocastici
multistadio misti-interi di difficile soluzione.


\subsection{Decomposizione di Benders e decomposizione L-shaped per la programmazione stocastica}
\paragraph{Kelley's cutting planes}
Si consideri il \textbf{problema convesso}
\(
\min_{x \in S} f(x),
\)
e si supponga che, dato un punto $x^k$, sia possibile calcolare non solo il valore della funzione,
ma anche un \textbf{subgradiente} $\gamma_k$, che esiste se la funzione è convessa sull’insieme $S$.
In altre parole, è possibile trovare una funzione affine tale che
\[
f(x^k)=\alpha_k+\gamma_k^T x^k, \qquad
f(x)\ge \alpha_k+\gamma_k^T x \;\; \forall x\in S.
\]
\hspace*{1.5em}La disponibilità di un tale iperpiano di supporto suggerisce la possibilità di approssimare
$f$ \textbf{dal basso}, tramite l’\textcolor{CadetBlue}{\textbf{inviluppo superiore}} degli iperpiani di supporto.\\
\hspace*{1.5em}L’algoritmo di \textbf{Kelley cutting planes} sfrutta questa idea costruendo e migliorando una funzione di bound inferiore fino a soddisfare un criterio di convergenza.
Se $S$ è \textbf{poliedrale}, occorre risolvere una sequenza di \textbf{LP}.
\begin{algorithm}
\caption{Metodo dei piani di taglio di Kelley}
\begin{algorithmic}[1]
\State Sia $x^1 \in S$ una soluzione ammissibile iniziale; \textbf{inizializzare} il contatore delle iterazioni $k \leftarrow 0$, l’upper bound $u_0 = f(x^1)$, il lower bound $l_0 = -1$ e la funzione di bound inferiore $\beta_0(x) = -\infty$.
\State \textbf{Incrementare} il contatore $k \leftarrow k+1$. Trovare un subgradiente di $f$ in $x^k$ tale che le condizioni $f(x^k)=\alpha_k+\gamma_k^T x^k, \;\;\;
f(x)\ge \alpha_k+\gamma_k^T x \;\; \forall x\in S$. siano soddisfatte.
\State \textbf{Aggiornare} l’upper bound ponendo $u_k = \min\{u_{k-1}, f(x^k)\}$ e aggiornare la funzione di bound inferiore ponendo $\beta_k(x) = \max\{\beta_{k-1}(x), \alpha_k + \gamma_k^T x\}$.
\State \textbf{Risolvere} il problema $l_k = \min_{x \in S} \beta_k(x)$ e sia $x^{k+1}$ la soluzione ottima ottenuta.
\State Se $u_k - l_k < \varepsilon$, \textbf{arrestare l’algoritmo}: $x^{k+1}$ è un’approssimazione soddisfacente della soluzione ottima; altrimenti, tornare a \textbf{[2]}.
\end{algorithmic}
\end{algorithm}

\paragraph{Proprietà di convessità dei programmi stocastici}
Si consideri il \textcolor{CadetBlue}{\textbf{problema SLP a due stadi con ricorso}}
\[
\min \ c^T x + \mathbb{E}_{\xi}\bigl[Q(x;\xi)\bigr]
\]
\[
\text{s.t. } Ax=b,\quad x\ge 0,
\]
dove
\[
Q(x,\xi)\equiv
\min_y
\left\{
q(\xi)^T y \ \big|\ W(\xi)y = h(\xi)-T(\xi)x,\ y\ge 0
\right\}.
\]
\hspace*{1.5em}Si parla di \textbf{fixed recourse} quando $W$ è deterministica, mentre si parla di \textbf{random recourse} nel caso generale.
Si consideri la \textcolor{CadetBlue}{\textbf{funzione di ricorso}} $\mathcal{Q}(x)\equiv \mathbb{E}[Q(x,\xi)]$ nel caso di costo di ricorso deterministico
\[
Q(x,\xi)\equiv
\min_y
\left\{
q^T y \ \big|\ Wy = h(\xi)-T(\xi)x,\ y\ge 0
\right\}.
\]
\hspace*{1.5em}Il \textcolor{CadetBlue}{\textbf{dominio effettivo}} di $\mathcal{Q}(x)$ consiste dei vettori $x$ che 
sono ammissibili per i vincoli di primo stadio e tali che $\mathcal{Q}(x)<+\infty$, cioè per cui il problema al secondo stadio
è (quasi sicuramente) ammissibile.
Si assume che, esclusa l’inammissibilità del secondo stadio, l’attesa sia sempre definita, cioè che le variabili aleatorie non siano heavy-tailed.\\
\hspace*{1.5em}Per dualità LP,
\(
Q(x,\xi)=
\max_{\pi}
\left\{
\bigl[h(\xi)-T(\xi)x\bigr]^T \pi \ \big|\ W^T \pi \le q
\right\}.
\)
Si denoti l’\textcolor{CadetBlue}{\textbf{insieme ammissibile del duale}} con
\(
\Pi=\{\pi \mid W^T\pi \le q\}.
\)
Poiché $q$ è deterministico, tale insieme è \textbf{non aleatorio}.

\paragraph{Dimostrazione di convessità (caso con regione duale non aleatoria)}
Siano $x_1,x_2\in \mathcal{D}$, $\lambda \in [0,1]$ e $x_\lambda=\lambda x_1+(1-\lambda)x_2$. Allora
\[
\mathcal{Q}(x_\lambda)=\int_{\Omega}Q(x_\lambda,\xi)\,dP
=\int_{\Omega}\max_{\pi\in\Pi}\left\{\bigl[h(\xi)-T(\xi)x_\lambda\bigr]^T\pi\right\}\,dP
\]
\[
=\int_{\Omega}\max_{\pi\in\Pi}
\left\{
\lambda\bigl[h(\xi)-T(\xi)x_1\bigr]^T\pi
+(1-\lambda)\bigl[h(\xi)-T(\xi)x_2\bigr]^T\pi
\right\}\,dP
\]
\[
\le
\lambda\int_{\Omega}\max_{\pi\in\Pi}\left\{\bigl[h(\xi)-T(\xi)x_1\bigr]^T\pi\right\}\,dP
+(1-\lambda)\int_{\Omega}\max_{\pi\in\Pi}\left\{\bigl[h(\xi)-T(\xi)x_2\bigr]^T\pi\right\}\,dP
\]
\[
=\lambda \mathcal{Q}(x_1)+(1-\lambda)\mathcal{Q}(x_2).\]

\paragraph{Convessità della funzione di ricorso}
La dimostrazione precedente è semplice perché la regione ammissibile duale è non aleatoria. In generale, la convessità della funzione di ricorso può essere mostrata anche in contesti più generali.  
In sintesi
\begin{itemize}[label=-]
\item La funzione di ricorso è tipicamente \textcolor{CadetBlue}{\textbf{differenziabile}} per distribuzioni continue.
\item La funzione di ricorso è \textcolor{CadetBlue}{\textbf{poliedrale}} per distribuzioni discrete.
\end{itemize}
In entrambi i casi, si può fare affidamento sui \textbf{cutting planes di Kelley} per risolvere il problema.
Dato lo \textbf{schema di decomposizione} del problema, i cutting planes possono essere ottenuti tramite una strategia di decomposizione.

\subsubsection{Decomposizione L-shaped}
\paragraph{Modello} Si consideri l'equivalente deterministico
\[
\begin{aligned}
\min \quad 
& c^T x + \sum_{s \in S} p_s \, q^T y^s \\
\text{s.t.} \quad 
& A x = b, \\
& W y^s + T_s x = h_s, \quad \forall s \in S, \\
& x,\; y^s \ge 0 .
\end{aligned}
\]
La matrice tecnologica del problema complessivo ha una struttura \textbf{dual block-angular}:
\[
\begin{bmatrix}
A & 0 & 0 & \cdots & 0\\
T_1 & W & 0 & \cdots & 0\\
T_2 & 0 & W & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
T_S & 0 & 0 & \cdots & W
\end{bmatrix}.
\]
\hspace*{1.5em}Dato un primo stadio fissato, si ottengono problemi di secondo stadio \textbf{indipendenti per scenario}.
Si riscrive il \textcolor{CadetBlue}{\textbf{problema a due stadi LP}} come
\[
\begin{aligned}
\min \quad 
& c^T x + \theta \\
\text{s.t.} \quad 
& A x = b, \\
& \theta \ge \mathcal{Q}(x), \\
& x \ge 0 .
\end{aligned}
\]
\hspace*{1.5em}Si costruisce un \textcolor{CadetBlue}{\textbf{master problem rilassato}} rilassando $\theta \ge \mathcal{Q}(x)$,
che viene approssimato tramite cutting planes:
\begin{itemize}[label=-]
\item \textcolor{CadetBlue}{\textbf{Optimality cuts}} della forma $\ \theta \ge \alpha^T x + \beta$;
\item \textcolor{CadetBlue}{\textbf{Feasibility cuts}} della forma $\ 0 \ge \alpha^T x + \beta$.
\end{itemize}
I coefficienti di ciascun taglio si ottengono risolvendo i sottoproblemi di scenario per una data decisione di primo stadio.

\paragraph{Optimality cuts}
Sia $\hat{x}$ la soluzione ottima del master iniziale e si consideri il duale del problema di secondo stadio per lo scenario $s$:
\[
Q_s(\hat{x}) \equiv \max (h_s-T_s\hat{x})^T \pi^s
\quad \text{s.t. } W^T \pi^s \le q_s.
\]
Data una soluzione duale ottima $\hat{\pi}^s$, valgono
\[
Q_s(\hat{x})=(h_s-T_s\hat{x})^T \hat{\pi}^s
\]
\[
Q_s(x)\ge (h_s-T_s x)^T \hat{\pi}^s \quad \forall x.
\]
La prima deriva dal fatto che $\hat{\pi}^s$ è ottima per $\hat{x}$, ma non necessariamente per un generico $x$.
Sommandola sugli scenari, si ottiene
\[
\mathcal{Q}(x)=\sum_{s\in S} p_s Q_s(x) \ge \sum_{s\in S} p_s (h_s-T_s x)^T \hat{\pi}^s.
\]
Quindi si aggiunge al master rilassato il seguente \textbf{optimality cut}:
\[
\theta \ge \sum_{s\in S} p_s (h_s-T_s x)^T \hat{\pi}^s.
\]

\paragraph{Feasibility cuts}
Se il ricorso non è completo, alcuni sottoproblemi di scenario possono essere inammissibili per una decisione di primo stadio $\hat{x}$.  
Si può allora sfruttare il duale del sottoproblema di scenario per trovare un \textbf{feasibility cut} che elimina $\hat{x}$ da ulteriori considerazioni.\\
\hspace*{1.5em}Si osservi che la \emph{regione ammissibile del duale non dipende dalle decisioni di primo stadio}, poiché $\hat{x}$ non compare nei vincoli.  
\textbf{Se il duale è inammissibile}, allora il problema di secondo stadio dello scenario corrispondente sarebbe
inammissibile per qualsiasi decisione di primo stadio; questo caso viene escluso e può indicare un errore di modellazione.\\
\hspace*{1.5em}Quando il \textbf{primale è inammissibile}, il duale è illimitato. Esiste dunque un \textcolor{CadetBlue}{\textbf{raggio estremo}}
della regione duale lungo il quale la soluzione duale tende a infinito.
Il duale del secondo stadio è illimitato se esistono variabili duali $\pi^\ast$ tali che
\(
W^T \pi^\ast \le 0,\quad (h_s-T_s\hat{x})^T \pi^\ast > 0.
\)
La fattibilità del primale richiede
\[
Wy = h_s - T_s x
\ \Rightarrow\
(\pi^\ast)^T Wy = (\pi^\ast)^T(h_s-T_s x)\le 0.
\]
Quindi
\(
(\pi^\ast)^T(h_s-T_s x)\le 0
\)
è un \emph{taglio valido che viene aggiunto al master rilassato}.

\paragraph{Schema iterativo}
L-shaped decomposition alterna
\begin{enumerate}[label=\arabic*.]
\item La soluzione del \textcolor{CadetBlue}{\textbf{master problem rilassato}}, che produce $\hat{\theta}$ e $\hat{x}$.
\item La soluzione dei corrispondenti \textcolor{CadetBlue}{\textbf{sottoproblemi di scenario}}.
\end{enumerate}
A ogni iterazione, vengono aggiunti tagli al master.
L’\textbf{algoritmo si ferma} quando la soluzione ottima del master soddisfa
\(
\hat{\theta} \ge \mathcal{Q}(\hat{x}).
\)
Questa condizione può essere rilassata se è sufficiente una soluzione near-optimal.

\paragraph{Variazioni sul tema}
Esistono varianti per gestire i tagli e migliorare la convergenza (ad esempio la
\textbf{regularized L-shaped decomposition}).  
L’idea può essere generalizzata a problemi multistadio:
\textbf{Nested Benders decomposition},
\textbf{Abridged nested Benders decomposition} e
\textbf{Stochastic Dual Dynamic Programming}.\\
\hspace*{1.5em}Nei metodi di \textbf{decomposizione stocastica} (\textbf{Higle e Sen})
si ha un’interazione tra \textbf{campionamento} e \textbf{ottimizzazione}:
l’\textbf{idea è costruire tagli asintoticamente validi}.\\
\hspace*{1.5em}Strategie alternative si basano sul vincolare la \textbf{funzione di ricorso}
superiormente e inferiormente, \textbf{partizionando il dominio} per ottenere
\textbf{bound stretti}.  
Quando i \textbf{metodi del simplesso} o gli \textbf{interior point methods}
riescono a risolvere un \textbf{LP stocastico}, spesso non c’è vantaggio nell’uso
della decomposizione; tuttavia, la \textbf{decomposizione} è necessaria per problemi
\textbf{realmente di grande scala}.

\subsection{Decomposizione Dantzig--Wolfe e generazione di colonne}
\paragraph{Problema di cutting stock}
Si devono tagliare rotoli (tutti di larghezza $L$) di un certo materiale in rotoli più corti, in modo da soddisfare una domanda assegnata.
Si introduca un insieme di $n$ item con domanda $d_i$ e larghezza $w_i \le L$, $i\in[n]$.
L’\textbf{obiettivo} è impacchettare gli item in modo da utilizzare il \textbf{numero minimo di rotoli}.
Questo è un esempio stilizzato di una vasta classe di problemi noti come \textbf{cutting-stock problems}. 
\paragraph{Modello MILP}
Siano disponibili $m$ rotoli, indicizzati da $j\in[m]$, e si introducano le variabili decisionali:
\begin{itemize}[label=-]
\item $\delta_j \in \{0,1\}$, uguale a $1$ se si usa il rotolo $j$.
\item $x_{ij}\in \mathbb{Z}_+$, numero di item $i$ tagliati dal rotolo $j$.
\end{itemize}
\[
\begin{aligned}
\min \quad 
& \sum_{j \in [m]} \delta_j \\
\text{s.t.} \quad 
& \sum_{i \in [n]} w_i x_{ij} \le L \delta_j, \quad j \in [m], \\
& \sum_{j \in [m]} x_{ij} = d_i, \quad i \in [n], \\
& \delta_j \in \{0,1\}, \quad x_{ij} \in \mathbb{Z}_+, \quad i \in [n],\ j \in [m] .
\end{aligned}
\]
Pur essendo corretto dal punto di vista teorico, il modello soffre di difficoltà legate a vincoli \textbf{big-$M$}
e a problemi di \textbf{simmetria} (molte soluzioni ottime equivalenti possono penalizzare i solver commerciali). 

\paragraph{Modello a pattern di taglio}
Si passa a una formulazione basata su un insieme di \textbf{cutting patterns}.
Ogni pattern, indicizzato da $k$, è associato al numero $a_{ik}$ di item $i$ ottenuti da un rotolo.
Dato un insieme $K$ di pattern, si introduce il numero intero $\gamma_k$ di rotoli tagliati secondo il pattern $k$
\[
\begin{aligned}
\min \quad
& \sum_{k \in K} \gamma_k \\
\text{s.t.} \quad
& \sum_{k \in K} a_{ik}\,\gamma_k = d_i, \quad i \in [n], \\
& \gamma_k \in \mathbb{Z}_+, \quad k \in K .
\end{aligned}
\]
\hspace*{1.5em}Questa formulazione \textcolor{CadetBlue}{\textbf{risolve la simmetria}}, ma la cardinalità di $K$ è enorme:
solo pochi pattern sono davvero utili e non è ovvio come restringere $K$ \emph{a priori}.
Si parte quindi con un numero limitato di pattern e li si genera dinamicamente tramite \textbf{column generation}. 

\paragraph{Generazione di una nuova colonna (by pricing)}
Si consideri un \textcolor{CadetBlue}{\textbf{LP-rilassato}} di una versione ristretta del modello
\[
\begin{aligned}
\min \quad
& \sum_{k \in K'} \gamma_k \\
\text{s.t.} \quad
& \sum_{k \in K'} a_{ik}\,\gamma_k = d_i, \quad i \in [n], \\
& \gamma_k \ge 0, \quad k \in K' .
\end{aligned}
\]
dove $K'\subset K$. Come migliorare la soluzione introducendo una nuova colonna?\\
\hspace*{1.5em}Siano $\pi_i$, $i\in[n]$, le variabili duali del vincolo $\sum_{k \in K'} a_{ik}\,\gamma_k = d_i$.
Dalla teoria del simplesso primale, conviene introdurre in base una variabile con \textbf{costo ridotto negativo}.
Per un pattern alternativo $k$, il costo ridotto è
\[
\hat{c}_k = 1 - \sum_{i\in[n]} \pi_i a_{ik}.
\]
\hspace*{1.5em} Si cerca la colonna $q$ con costo ridotto minimo, $\hat{c}_q = \min_k \hat{c}_k$.
Se $\hat{c}_q \ge 0$, la base corrente è ottima; altrimenti si introduce la variabile non basica $\gamma_q$ e si itera. 

\paragraph{Pricing come knapsack}
La minimizzazione si ottiene risolvendo un \textcolor{CadetBlue}{\textbf{problema intero di knapsack}}
\[
\begin{aligned}
\max \quad
& \sum_{i \in [n]} \pi_i y_i \\
\text{s.t.} \quad
& \sum_{i \in [n]} w_i y_i \le L, \\
& y_i \in \mathbb{Z}_+ .
\end{aligned}
\]
Questo produce un nuovo pattern, che viene introdotto se il valore ottimo è maggiore di $1$. 
Ciò fornisce un \textcolor{CadetBlue}{\textbf{lower bound}} per un algoritmo \textbf{branch-and-price}.\\
\hspace*{1.5em}In alternativa, si può generare un insieme di colonne e poi ripristinare l’integralità delle variabili $\gamma_k$ risolvendo un singolo IP (euristica).
In casi meno stilizzati, la complessità dei vincoli può essere “nascosta” nel sottoproblema di pricing.
Lo schema di column generation è al cuore della \textbf{decomposizione di Dantzig--Wolfe}. 

\subsubsection{Decomposizione di Dantzig--Wolfe}
\paragraph{Modello LP}
Si consideri un modello LP
\[
\begin{aligned}
\min \quad
& c^T x \\
\text{s.t.} \quad
& A x \ge b, \\
& D x \ge f, \\
& x \ge 0_n .
\end{aligned}
\]
Si separano i vincoli interpretando $Ax \ge b$ come vincoli \textbf{complicanti}, mentre $Dx \ge f$ (insieme alla non-negatività) come vincoli \textbf{facili}. \\
\hspace*{1.5em}Si definisce il \textcolor{CadetBlue}{\textbf{poliedro}}
\(
X := \{x\in\mathbb{R}^n \mid Dx \ge f,\ x \ge 0_n\}.
\)
Si assume che minimizzare una funzione lineare su $X$ sia facile (ad esempio perché $D$ è block-diagonal o per struttura favorevole come un network flow).
Per il teorema di \textbf{Minkowski--Weyl}, $X$ si può rappresentare come somma di un politopo e di un cono:
\(
X = \operatorname{conv}\{V\} + \operatorname{cone}\{D\},
\)
dove $V=\{v^1,\ldots,v^Q\}$ è l’insieme dei punti estremi e $D=\{d^1,\ldots,d^R\}$ l’insieme delle direzioni estreme. 

\paragraph{Rappresentazione mediante combinazioni convesse e coniche}
Il vincolo $x\in X$ si esprime come
\[
\begin{aligned}
& x = \sum_{q \in V} \lambda_q v^q + \sum_{r \in D} \mu_r d^r \\
& \sum_{q \in V} \lambda_q = 1, \\
& \lambda_q \ge 0, \quad q \in V; \qquad \mu_r \ge 0, \quad r \in D .
\end{aligned}
\]
dove il secondo vincolo è \textbf{di convessità}.
Sostituendo il valore della $x$ nel modello originale si ottiene un problema in $\lambda_q$ e $\mu_r$. \\
\hspace*{1.5em}Definendo gli scalari e vettori
\(
c_q := c^T v^q,\quad a_q := Av^q,\quad q\in V, \qquad
c_r := c^T d^r,\quad a_r := Ad^r,\quad r\in D,
\)
si ottiene il \textcolor{CadetBlue}{\textbf{master problem} (MP)}
\[
\begin{aligned}
\min \quad
& \sum_{q \in V} c_q \lambda_q + \sum_{r \in D} c_r \mu_r \\
\text{s.t.} \quad
& \sum_{q \in V} a_q \lambda_q + \sum_{r \in D} a_r \mu_r \ge b, \\
& \sum_{q \in V} \lambda_q = 1, \\
& \lambda_q \ge 0, \quad q \in V; \qquad \mu_r \ge 0, \quad r \in D .
\end{aligned}
\]
Il problema è che \emph{MP può coinvolgere un numero enorme di variabili} e non è in generale possibile enumerare tutti
i punti/direzioni estremi di $X$ a partire dalla descrizione per disuguaglianze. 

\paragraph{Restricted Master Problem (RMP)}
La strategia si basa su \textbf{column generation}.
Si inizializza un \textcolor{CadetBlue}{\textbf{Restricted Master Problem}} con un sottoinsieme $V'\subset V$
di punti estremi e $D'\subset D$ di direzioni estreme:
\[
\begin{aligned}
\min \quad
& \sum_{q \in V'} c_q \lambda_q + \sum_{r \in D'} c_r \mu_r \\
\text{s.t.} \quad
& \sum_{q \in V'} a_q \lambda_q + \sum_{r \in D'} a_r \mu_r \ge b, \\
& \sum_{q \in V'} \lambda_q = 1, \\
& \lambda_q \ge 0, \quad q \in V'; \qquad \mu_r \ge 0, \quad r \in D' .
\end{aligned}
\]
Siano $\pi$ e $\pi^0$ le variabili duali associate rispettivamente ai vincoli. 

\paragraph{Costo ridotto per punti estremi e direzioni estreme}
Nel simplesso standard, una colonna entra in base se il costo ridotto è negativo.
Per i punti estremi, si verifica se esiste $q\in V$ tale che
\[
\bar{c_q} = c_q -
\begin{bmatrix}
\pi^T & \pi^0
\end{bmatrix}
\begin{bmatrix}
a_q\\
1
\end{bmatrix}
= c_q - \pi^T a_q - \pi^0
= c_q - \pi^T A v^q - \pi^0 < 0.
\]
Per le direzioni estreme:
\[
\bar{c_r} = c_r -
\begin{bmatrix}
\pi^T & \pi^0
\end{bmatrix}
\begin{bmatrix}
a_r\\
0
\end{bmatrix}
= c_r - \pi^T a_r
= c_r - \pi^T A d^r < 0.
\]
\hspace*{1.5em}In linea di principio si dovrebbe risolvere il \textcolor{CadetBlue}{\textbf{problema ausiliario}}
\[
\min\left\{
\min_{q\in V} \bar{c_q},\ \min_{r\in D} \bar{c_r}
\right\},
\]
ma non è affrontabile per enumerazione completa.
Si riconduce allora a un \textbf{pricing problem}
\[
\begin{aligned}
\min \quad
& (c^T - \pi^T A)\, x \\
\text{s.t.} \quad
& D x \ge d, \\
& x \ge 0_n .
\end{aligned}
\]
Si trascura il termine costante $\pi^0$ e si usa lo stesso problema per trovare un vettore $x\in\mathbb{R}^n$ che giochi il ruolo di punto estremo o direzione estrema.
Sia $z^\ast_{PP}$ il valore ottimo del problema di pricing
\begin{itemize}[label=-]
\item Se \textcolor{CadetBlue}{$z^\ast_{PP}=-\infty$} (PP illimitato inferiormente), si è trovata una \textbf{direzione estrema} $x^\ast$ con costo ridotto negativo; si aggiunge a RMP una nuova variabile $\mu_r$ associata alla colonna
\[
\begin{bmatrix}
Ax^\ast\\
0
\end{bmatrix}.
\]
\item Se \textcolor{CadetBlue}{$-\infty < z^\ast_{PP} < \pi^0$} (PP limitato e valore inferiore alla variabile duale del vincolo di convessità), si è trovato un \textbf{nuovo punto estremo} $x^\ast$ con costo ridotto negativo; si aggiunge a RMP una nuova variabile $\lambda_q$ associata alla colonna
\[
\begin{bmatrix}
Ax^\ast\\
1
\end{bmatrix}.
\]
\item Se \textcolor{CadetBlue}{$z^\ast_{PP} \ge \pi^0$}, non esiste alcun punto/direzione utile da aggiungere e ci si può fermare.
\end{itemize}
\hspace*{1.5em}In termini intuitivi, le variabili duali ottime ottenute risolvendo RMP suggeriscono una direzione
\(
(c - A^T \pi)
\)
lungo la quale si “sonda” il poliedro $X$.
Se $X$ è un \textcolor{CadetBlue}{politopo} si può trovare solo un \textcolor{CadetBlue}{\textbf{punto estremo}};
se $X$ è \textcolor{CadetBlue}{illimitato} si può trovare una \textcolor{CadetBlue}{\textbf{direzione estrema}}.
Non serve trovare tutti i punti e tutte le direzioni: interessano solo quelli con costo ridotto negativo, perché
possono migliorare il valore ottenuto risolvendo RMP. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1cm}
\textcolor{DarkGreen}{\section{CAP 4 -- Sistemi MRP/ERP e approccio JIT}}
\paragraph{Recap}
I metodi classici di gestione delle scorte risultano inadeguati in presenza di strutture di prodotto
complesse, vincoli di capacità e contesti non \emph{make-to-stock}, poiché la propagazione dei fabbisogni
lungo la distinta base può generare una marcata \textbf{amplificazione della variabilità}. In tale contesto
si colloca l’evoluzione dalla \textbf{logica MRP} ai sistemi \textbf{MRPII} ed \textbf{ERP}, come risposta
operativa al problema del \emph{lot-sizing multilivello}.\\
\hspace*{1.5em}La \textcolor{DarkGreen}{\textbf{logica MRP}} si fonda sull’assunzione di
\textbf{capacità infinita}, in cui il vincolo di capacità è surrogato tramite un \textbf{lead time fissato a priori},
e su una procedura ricorsiva di esplosione dei fabbisogni a partire dal \textbf{Master Production Schedule}.
Gli ordini pianificati non sono esecutivi e il processo è intrinsecamente soggetto al fenomeno del
\textcolor{DarkGreen}{\textbf{nervosismo}}, per cui piccole variazioni dell’MPS possono produrre grandi variazioni negli ordini,
anche per effetto di bordo legato alla ripianificazione a \emph{rolling horizon}.\\
\hspace*{1.5em}A livello di \emph{shop floor}, la \textbf{Factory Physics} fornisce una chiave di lettura
fondamentale delle prestazioni dei sistemi produttivi, mettendo in relazione \textbf{throughput},
\textbf{flow time} e \textbf{work in process} attraverso la \textcolor{DarkGreen}{\textbf{legge di Little}}. Tale relazione evidenzia
come, a parità di throughput, la riduzione del WIP sia l’unico modo strutturale per ridurre i tempi di
attraversamento, e come un’elevata utilizzazione del sistema, in presenza di variabilità, conduca a
un’esplosione dei tempi di attesa e del WIP. La variabilità, sia prevedibile sia imprevedibile, rappresenta
quindi un fattore chiave nel deterioramento delle prestazioni operative e spiega il ricorso sistematico,
nei sistemi tradizionali, a buffer sotto forma di scorte, capacità in eccesso o lead time gonfiati.\\
\hspace*{1.5em}In contrapposizione alla logica \textbf{push} tipica dell’MRP, l’approccio
\textcolor{DarkGreen}{\textbf{Just-In-Time (Toyota)}} affronta il problema alla radice, mirando a \textbf{ridurre la variabilità alla
fonte} piuttosto che ad assorbirla a valle. Ciò avviene attraverso la \textbf{produzione livellata}
(\emph{production smoothing}), la riduzione sistematica dei \textbf{tempi di setup}, il controllo \textbf{pull}
del WIP e l’adozione di flussi regolari e ripetitivi. Strumenti come il \textbf{sistema kanban} e il
\textbf{goal chasing} consentono di sincronizzare il consumo dei componenti con l’avanzamento della linea
di assemblaggio, rendendo possibile il controllo delle linee a monte tramite segnali fisici di prelievo.
L’approccio Toyota mette così in luce il legame strutturale tra variabilità, livelli di magazzino e
prestazioni del sistema produttivo, mostrando come la riduzione del WIP e dei lead time sia
inseparabile da una gestione coerente della variabilità e dell’utilizzazione del sistema.


\subsection{Limiti degli approcci classici di gestione delle scorte}

\paragraph{Inadeguatezza dei modelli tradizionali}
I classici approcci di controllo delle scorte presentano \textbf{limiti severi} nei seguenti contesti
\begin{itemize}[label=-]
\item Ambienti \emph{non make-to-stock}, come \emph{make-to-order} e \emph{assemble-to-order}, in cui viene ignorata
la variabilità prevedibile.
\item Presenza di \textbf{vincoli di capacità produttiva}, per cui tali modelli risultano più adatti a problemi retail.
\item Strutture di prodotto \textbf{complesse}, rappresentate mediante distinte base multilivello.
\end{itemize}

\paragraph{Effetto di amplificazione della variabilità}
Anche in presenza di una domanda regolare per il prodotto finito, la propagazione dei fabbisogni lungo la distinta base
può indurre un’\textcolor{DarkGreen}{\textbf{amplificazione della variabilità}} sui componenti a valle.

\subsection{Evoluzione dai modelli MRP ai sistemi ERP}

\paragraph{Lot-sizing multilivello e difficoltà computazionali}
In linea di principio è possibile costruire un \emph{modello MILP} per il problema di \textbf{lot-sizing multilivello},
collegando domanda indipendente e domanda dipendente.  
Tali modelli risultano tuttavia difficili da risolvere, e erano computazionalmente impraticabili fino agli anni Settanta.\\
\hspace*{1.5em}Negli anni Settanta sono stati introdotti i sistemi \textcolor{DarkGreen}{\textbf{MRP (Material Requirements
Planning)}}, basati su una \textbf{logica a capacità infinita}.  
Il vincolo di capacità produttiva viene rilassato, consentendo un \textbf{disaccoppiamento parziale tra item}, ma non tra
domanda indipendente e dipendente.
L’interazione tra item viene anticipata e surrogata mediante un \textbf{lead time fissato a priori}.

\paragraph{Evoluzione verso MRPII ed ERP}
L’evoluzione successiva ha portato ai sistemi \textcolor{DarkGreen}{\textbf{MRPII (Manufacturing Resource Planning)}},
che includono strumenti di verifica della capacità come \textcolor{DarkGreen}{\textbf{RCCP}} (\emph{Rough Cut Capacity Planning}) e 
\textcolor{DarkGreen}{\textbf{CRP}} (\emph{Capacity Requirement Planning}): per verificare il soddisfacimento dei vincoli di capacità.
L’ulteriore evoluzione è rappresentata dai sistemi \textcolor{DarkGreen}{\textbf{ERP (Enterprise Resource Planning)}}, caratterizzati
dall’integrazione con le funzioni commerciali e finanziarie.

\subsection{Logica MRP}
\paragraph{Assunzione di capacità infinita}
La logica MRP assume \textbf{capacità infinita}: il vincolo di capacità produttiva non viene considerato esplicitamente,
ma è rappresentato indirettamente dal lead time.\\
\hspace*{1.5em}Il \textbf{lead time} impone di lanciare gli ordini di produzione o di acquisto con sufficiente anticipo
rispetto alla domanda.  
Il meccanismo di \textbf{lead time offsetting} consente di anticipare gli ordini pianificati rispetto ai fabbisogni.

\paragraph{Record MRP}
Ogni codice è descritto mediante un record MRP articolato per periodo, contenente: fabbisogni lordi, consegne attese,
magazzino disponibile, fabbisogni netti e ordini pianificati.\\
\hspace*{1.5em}Prima del lead time offsetting è necessario calcolare i \textbf{fabbisogni netti}, ottenuti nettificando i fabbisogni
lordi tenendo conto di:
giacenze disponibili (\textcolor{DarkGreen}{\emph{on-hand}}) e ordini già emessi (\textcolor{DarkGreen}{\emph{on-order}}).

\subsection{Esplosione dei fabbisogni e struttura del processo MRP}
\paragraph{Domanda indipendente e MPS}
Per i prodotti finiti (\emph{end item}), che costituiscono la radice della distinta base, i fabbisogni lordi sono definiti
dal \textcolor{DarkGreen}{\textbf{Master Production Schedule (MPS)}}.
La logica MRP procede in modo \textbf{ricorsivo} a partire dagli end item, scendendo lungo le distinte base ed esplodendo
i fabbisogni di ciascun codice fornendo i tempi di lancio degli ordini a tutti i livelli.

\paragraph{Ordini pianificati, rilascio e regole di lot-sizing}
I \emph{lotti} corrispondono ai fabbisogni netti secondo la regola \textcolor{DarkGreen}{\textbf{lot-for-lot}}.
È importante comprendere la dinamica di
un sistema MRP: gli ordini pianificati \textbf{non sono ordini esecutivi}; gli ordini pianificati dell’\emph{action bucket},
una volta rilasciati, vengono trasformati in consegne attese. Un’ulteriore differenza tra \textcolor{DarkGreen}{ordini pianificati} e \textcolor{DarkGreen}{ordini 
operativi} è che, quando un ordine di produzione viene rilasciato, vengono modificati i record relativi alle giacenze di
magazzino dei componenti; una parte della giacenza viene allocata per l’ordine ed è da considerarsi non disponibile 
nel calcolo dei fabbisogni netti.\\
\hspace*{1.5em}I \textbf{pacchetti MRP} permettono di specificare un orizzonte temporale di \emph{release}; solo gli 
ordini che ricadono all’interno di tale orizzonte dovrebbero essere rilasciati, in quanto gli altri sono soggetti a 
incertezze eccessive. Nel calcolo dei fabbisogni è possibile tenere conto di scorte di sicurezza; in questo caso, i 
fabbisogni netti vengono generati non quando il magazzino disponibile va sotto zero, ma \emph{quando va sotto un certo livello 
di soglia}. Esiste una gamma di regole di lot-sizing, a quantità fissa o variabile, oppure euristiche per la 
minimizzazione dei costi totali di giacenza e di ordinazione.

\subsection{Il problema del nervosismo}
\paragraph{Nervosismo}Il dimensionamento dei lotti a partire dai livelli alti della distinta base può generare effetti
non intuitivi in presenza di variazioni del MPS.  
Le regole a quantità variabile sono soggette al fenomeno del \textcolor{DarkGreen}{\textbf{nervosismo}} (\emph{nervousness}).\\
\hspace*{1.5em}Il nervosismo si manifesta quando \textbf{piccole variazioni nei fabbisogni} producono
\textbf{grandi variazioni negli ordini pianificati}, anche a livelli inferiori della distinta base. Il fenomeno può portare
a: ordini urgenti, instabilità del piano e risultati anti-intuitivi.

\paragraph{Effetto di bordo, nervosismo e strategie di mitigazione}
Un altro punto da considerare è l’\textcolor{DarkGreen}{\textbf{effetto di bordo}} dovuto alla ripianificazione
\emph{rolling horizon}, in cui si aggiunge un \emph{time bucket} all’orizzonte di pianificazione. L’aggiunta del
fabbisogno relativo a tale time bucket può alterare l’accorpamento dei fabbisogni, con esiti imprevedibili.\\
\hspace*{1.5em}Esistono diversi
modi per evitare il fenomeno del nervosismo. L’adozione di regole a quantità fissa permette di filtrare naturalmente
variazioni di piccola entità, al costo di un aumento nel livello delle scorte. È anche possibile adottare strategie di
gestione differenziata dell’orizzonte temporale di pianificazione, dette strategie di \textcolor{DarkGreen}{\textbf{time fencing}}.
Nell’immediato non è possibile cambiare nulla nell’MPS; in un periodo intermedio è possibile alterare l’MPS solo dopo
specifica analisi e autorizzazione; è invece possibile cambiare l’MPS a piacere nei periodi più lontani.\\
\hspace*{1.5em}Inoltre, uno
strumento utile per evitare il nervosismo e per risolvere situazioni anomale è l’uso dei \textcolor{DarkGreen}{\textbf{firm planned orders}};
si tratta di ordini che non possono essere modificati dall’MRP quando le condizioni cambiano, ma soltanto dietro istruzione
del pianificatore.

\subsection{Master Production Scheduling e Legge di Little}
\paragraph{Master Production Scheduling (MPS)}
Il \textcolor{DarkGreen}{\textbf{Master Production Schedule (MPS)}} costituisce l’input primario per la logica MRP ed è
basato in parte su ordini cliente e in parte su attività di \emph{forecasting} (demand planning). Nella logica MRP
tradizionale, l’MPS può essere validato mediante moduli \textbf{RCCP (Rough Cut Capacity Planning)}. Non è detto che l’MPS
venga costruito esclusivamente per i codici alla radice delle distinte base: può esistere domanda indipendente per parti di
ricambio e, in contesti \textcolor{DarkGreen}{\emph{assemble-to-order} (ATO)}, può essere preferibile adottare una struttura a due livelli, con un
MPS a livello di moduli e un \emph{Final Assembly Scheduling} a livello superiore.

\paragraph{Capacity Requirements Planning (CRP)}
La logica MRP è basata su un’assunzione di \textbf{capacità infinita}. È tuttavia possibile effettuare una verifica a posteriori del carico di lavoro rispetto alla capacità effettivamente disponibile tramite moduli di \textbf{Capacity Requirements Planning (CRP)}. La risoluzione manuale di eventuali situazioni di non ammissibilità risulta complessa, a meno di ricorrere a modelli di ottimizzazione o a procedure euristiche a capacità finita. Inoltre, per evitare ritardi, si tende spesso a gonfiare il lead time presunto, generando work in process che a sua volta allunga ulteriormente i lead time, dando luogo a un potenziale \textbf{circolo vizioso}.

\paragraph{Factory Physics}
A livello di \emph{shop floor}, le misure di prestazione fondamentali sono il \textbf{throughput},
il \textbf{flow time} (strettamente legato al lead time e comprensivo di tempi di attesa, lavorazione e movimentazione) e
il \textbf{work in process (WIP)}, associato ai materiali in coda. Idealmente, si desidererebbe un throughput elevato con
flow time e WIP contenuti; tuttavia, il raggiungimento simultaneo di tali obiettivi è fortemente influenzato dalla
variabilità, sia prevedibile sia imprevedibile.

\paragraph{Legge di Little}
Un risultato fondamentale della teoria delle code è la \textcolor{DarkGreen}{\textbf{legge di Little}}, che esprime la
relazione tra WIP, throughput e flow time
\[
\text{WIP} = \text{throughput} \times \text{flow time}.
\]
\hspace*{1.5em}La legge mette in evidenza il legame strutturale tra il livello di materiali in lavorazione e il
tempo di attraversamento del sistema.

\paragraph{Modello di una singola macchina}
Si consideri una singola macchina dotata di un buffer per il WIP. Si introducono le seguenti grandezze: il
tempo medio di attesa in coda \( W_q \), il tempo medio di lavorazione \( t_s \), il tasso medio di
servizio \( \mu = 1/t_s \), il tasso medio di arrivo \( \lambda \), che in condizioni di equilibrio coincide con
il throughput, e la lunghezza media della coda \( L \), che rappresenta il WIP. In tale contesto, la legge di Little
può essere riscritta come
\[
L = \lambda \,(W_q + t_s).
\]
L’\textbf{utilizzazione del sistema} è definita come \( u = \lambda / \mu \), ed è compresa tra 0 e 1.

\paragraph{Effetto della variabilità}
Una \textbf{coda} del tipo \textbf{\( G/G/1 \)} non è trattabile analiticamente in forma chiusa; tuttavia, una formula
approssimata, esatta nel caso \( M/M/1 \), fornisce una stima del \textcolor{DarkGreen}{tempo medio di attesa in coda}
\[
W_q \approx 
\left(
\frac{C_a^2 + C_s^2}{2}
\right)
\left(
\frac{u}{1-u}
\right)
t_s,
\]
dove \( C_a \) e \( C_s \) sono i \textcolor{DarkGreen}{\textbf{coefficienti di variazione dei tempi di interarrivo e di servizio}},
rispettivamente. La relazione mostra come l’attesa in coda cresca rapidamente all’aumentare dell’utilizzazione e della variabilità.

\paragraph{Buffering Law}
Per ovviare agli effetti della variabilità è necessario introdurre dei \textbf{buffer}, che possono assumere la forma di
magazzino o WIP, capacità in eccesso oppure tempo, sotto forma di lead time gonfiato. In assenza di una riduzione della
variabilità, il sistema paga un prezzo in termini di WIP elevato, capacità sottoutilizzata e peggioramento del livello di
servizio al cliente, manifestato da vendite perse, lead time lunghi e consegne in ritardo.

\paragraph{Produzione livellata e fonti di variabilità}
Uno dei fondamenti dell’approccio \emph{Toyota} tradizionale è la \textcolor{DarkGreen}{\textbf{produzione livellata}} (\emph{production smoothing}).
La variabilità non è legata esclusivamente a eventi casuali, come i guasti alle macchine, ma può derivare anche da
batching dovuto ai tempi di setup, batching nella movimentazione (\emph{wait to move}), scarso coordinamento
nell’assemblaggio (\emph{wait to match}) e variabilità della domanda riflessa nell’MPS.

%%%%%%%%%%%%%%%%
\subsection{Approccio Just-In-Time (Toyota)}

\paragraph{Principi dell’approccio Just-In-Time}
L’approccio \textcolor{DarkGreen}{\textbf{Just-In-Time (JIT)}}, sviluppato nell’ambito del sistema Toyota, si
basa sull’idea di ridurre la variabilità mediante la ripetizione di un \textbf{mix di produzione ripetitivo}
(\emph{mixed-model}), realizzando una produzione livellata (\emph{production smoothing}).\\
\hspace*{1.5em}Il presupposto
fondamentale è la \emph{riduzione o l’annullamento dei tempi di setup}, che consente di produrre lotti piccoli e frequenti.
Il controllo del \textbf{work in process} avviene tramite un approccio \textcolor{DarkGreen}{\textbf{pull}}, basato sul prelievo fisico
dei materiali, in contrapposizione alla logica \textcolor{DarkGreen}{\textbf{push}}, basata sul rilascio di ordini pianificati a partire
da previsioni di fabbisogno. Lo strumento chiave del controllo pull è il \textcolor{DarkGreen}{\textbf{sistema kanban}}, con possibile
alternativa nel sistema \textcolor{DarkGreen}{\textbf{CONWIP}}.

\paragraph{Sequenziamento e regolarità dei flussi}
A parità di mix produttivo, esistono diverse sequenze di assemblaggio che realizzano lo stesso numero di prodotti finiti.
Il problema del \textcolor{DarkGreen}{\textbf{Toyota Goal Chasing}} consiste nello scegliere una sequenza che renda il più possibile
\textbf{regolare il fabbisogno di componenti} sulle linee laterali che alimentano la linea principale di assemblaggio.
\emph{L’obiettivo è consentire il \textcolor{DarkGreen}{controllo delle linee a monte mediante un sistema pull}, che richiede flussi regolari
e prevedibili}.

\paragraph{Formalizzazione del problema}
Si consideri una linea di assemblaggio che realizza \( N \) prodotti finiti sulla base di \( M \) moduli prodotti su linee
laterali, con una distinta base piatta. Sia \( b_{ij} \) il numero di componenti di tipo \( j \) richiesti per assemblare
un’unità del prodotto finito \( i \), e sia \( Q_i \) il numero di finiti di tipo \( i \) previsti nel mix ripetitivo.
Il \textcolor{DarkGreen}{fabbisogno per ciclo dei componenti} di tipo \( j \) è dato da
\[
N_j = \sum_{i=1}^{N} b_{ij} Q_i.
\]
\hspace*{1.5em}Indicando con \( Q = \sum_{i=1}^{N} Q_i \) il numero totale di assemblaggi per ciclo, il \textcolor{DarkGreen}{\textbf{consumo cumulato ideale}}
dei componenti di tipo \( j \) al passo \( k \) dovrebbe essere pari a
\(
\frac{k N_j}{Q}
\).

\paragraph{Funzione obiettivo del goal chasing}
Sia \( X_{jk} \) il \emph{consumo cumulato effettivo} del componente \( j \) al passo \( k \).
L’obiettivo del problema di goal chasing è \textbf{minimizzare la distanza tra consumo ideale e consumo reale},
misurata tramite la funzione
\[
\sum_{k=1}^{Q} \sum_{j=1}^{M}
\left(
\frac{k N_j}{Q} - X_{jk}
\right)^2.
\]

\paragraph{Ruolo dei tempi di setup nel JIT}
Nel JIT viene posta forte enfasi sulla riduzione dei \textbf{tempi di setup}, poiché essa consente di ridurre la dimensione dei lotti e, di conseguenza, il livello medio delle giacenze. Tuttavia, l’impatto dei tempi di setup deve essere analizzato congiuntamente agli altri fattori del sistema produttivo. In particolare, la riduzione dei livelli di magazzino non dipende esclusivamente dalla diminuzione dei setup, ma anche dal tasso di produzione e dal grado di saturazione del sistema.

\subsubsection{Modello di rotazione ciclica dei prodotti}

\paragraph{Impostazione del modello}
Si consideri una linea su cui ruotano ciclicamente \( N \) prodotti. Per ciascun prodotto \( i \) si introducono il
tasso di produzione \( p_i \) (pezzi per unità di tempo), il tasso di domanda \( d_i < p_i \), assunto costante, e
il tempo di setup \( s_i \), assunto indipendente dalla sequenza. Il periodo di rotazione \( T_c \) deve essere
ridotto al fine di contenere il livello medio di giacenza.\\
\hspace*{1.5em}Indicando con \( T_i \) la durata del lotto del prodotto \( i \) in ciascuna rotazione, in \textcolor{DarkGreen}{condizioni di equilibrio}
deve valere
\[
p_i T_i = d_i T_c
\qquad \Rightarrow \qquad
T_i = \frac{d_i}{p_i} T_c.
\]
\hspace*{1.5em}Inoltre, il \textcolor{DarkGreen}{\textbf{periodo di rotazione}} deve soddisfare il vincolo
\[
T_c \geq \sum_{i=1}^{N} s_i + \sum_{i=1}^{N} T_i.
\]
L’eventuale \emph{slack} è utile per assorbire guasti, ritardi ed effettuare attività di manutenzione.

\paragraph{Limite inferiore del periodo di rotazione}
Sostituendo l’espressione di \( T_i \) nel vincolo precedente, si ottiene il \textcolor{DarkGreen}{\textbf{limite inferiore}}
\[
T_c \geq 
\frac{\sum_{i=1}^{N} s_i}
{1 - \sum_{i=1}^{N} \frac{d_i}{p_i}}.
\]
\hspace*{1.5em}Il risultato mostra che, oltre ai tempi di setup, il \textcolor{DarkGreen}{periodo di rotazione} è fortemente influenzato dal \emph{rapporto
tra tassi di domanda e tassi di produzione}, evidenziando un legame diretto con i fenomeni di congestione già
osservati nei modelli di coda.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1cm}
\textcolor{ForestGreen}{\section{CAP 5 -- Schedulazione nella produzione e nei servizi}}
\paragraph{Recap}
La \textbf{schedulazione} affronta l’assegnazione di \textbf{risorse} a \textbf{job} nel tempo,
rispettando \textbf{vincoli tecnologici} e di \textbf{capacità} e ottimizzando una
\textbf{misura di prestazione}.  
Rispetto alla \textbf{pianificazione della produzione} (ad esempio a livello \textbf{MRP}),
la schedulazione opera a un livello di \textbf{dettaglio superiore}, assumendo come dati
tempi di lavorazione, precedenze e vincoli operativi.\\
\hspace*{1.5em}I job sono descritti come sequenze di \textbf{operazioni} soggette a \textbf{precedenze},
\textbf{tempi di processo}, \textbf{date di rilascio} e \textbf{due date}.  
Le assunzioni classiche prevedono lavorazione su \textbf{una macchina per volta},
\textbf{una sola operazione per macchina} e \textbf{assenza di interruzioni}, pur ammettendo
varianti come \textbf{pre-emption}, \textbf{batch} e \textbf{lot streaming}.
Le diverse strutture di flusso (\textbf{macchina singola}, \textbf{parallele},
\textbf{flow shop}, \textbf{job shop}, \textbf{open shop}) determinano la complessità del problema.
Le prestazioni sono valutate tramite \textbf{funzioni di penalità} dei tempi di completamento,
come \textbf{$C_i$}, \textbf{$F_i$}, \textbf{$L_i$}, \textbf{$T_i$}, \textbf{$E_i$} e \textbf{$U_i$},
eventualmente aggregate in forma \textbf{min-sum} o \textbf{min-max} e pesate tramite
\textbf{priorità $w_i$}.  
Alcune misure risultano \textbf{equivalenti} fino a costanti, mentre
$\textbf{L}_{\max}$ implica l’ottimalità anche per $\textbf{T}_{\max}$.\\
\hspace*{1.5em}Nel caso di \textcolor{ForestGreen}{\textbf{misure di prestazione regolari}}, ossia
\textbf{non decrescenti} nei tempi di completamento, il \textbf{timing} può essere ricavato
direttamente dal \textbf{sequencing} e la ricerca può essere limitata a
\textbf{soluzioni attive}.  
Per \textcolor{ForestGreen}{\textbf{misure non regolari}} (come earliness–tardiness),
sequencing e timing non coincidono e la soluzione ottima non è necessariamente
\textbf{attiva né semiattiva}, rendendo il problema più complesso.\\
\hspace*{1.5em}La \textcolor{ForestGreen}{\textbf{notazione di Graham}} $\alpha|\beta|\gamma$ fornisce una
classificazione compatta dei problemi di schedulazione in base al layout delle macchine,
ai vincoli aggiuntivi e alla misura obiettivo.  
Alcuni casi ammettono soluzioni \textbf{polinomiali} (regole \textbf{EDD}, \textbf{WSPT},
algoritmo di \textbf{Johnson}), ma nella maggior parte dei casi i problemi sono
\textbf{NP-hard} e richiedono \textbf{euristiche} e \textbf{metaeuristiche}.
La modellazione tramite \textcolor{ForestGreen}{\textbf{grafo disgiuntivo}} rappresenta
vincoli tecnologici e di capacità, e il \textbf{cammino critico} determina il \textbf{makespan}.
Da tale rappresentazione deriva un modello \textbf{MILP} per $J//C_{\max}$, utile come base
concettuale per strategie di ricerca locale.  
In questo contesto, solo perturbazioni degli archi disgiuntivi sul
\textbf{cammino critico} sono efficaci.\\
\hspace*{1.5em}La procedura \textcolor{ForestGreen}{\textbf{shifting bottleneck}} decompone il problema
$J//C_{\max}$ in una sequenza di problemi su macchina singola del tipo
\textbf{$1/r_i/L_{\max}$}, utilizzando \textbf{teste} e \textbf{code} per costruire
\textbf{due date locali}.  
Il collo di bottiglia viene identificato iterativamente come la macchina con
\textbf{$L_{\max}$} peggiore e congelato fino alla completa schedulazione del sistema.

\subsection{Modelli classici di machine scheduling}

\paragraph{Scheduling} Risolvere un problema di \textbf{schedulazione} richiede di assegnare \textbf{risorse} a
\textbf{job} da eseguire nel tempo, rispettando \textbf{vincoli tecnologici} e di \textbf{capacità}, in modo da
ottimizzare una \textbf{misura di prestazione}.  
Un \textcolor{ForestGreen}{\textbf{problema di scheduling}} si pone a un livello di dettaglio
superiore rispetto a un problema di \textbf{pianificazione della produzione} (ad esempio a livello \textbf{MRP}).
Esiste una grande varietà di problemi di scheduling, ma si inizia con la definizione di un \textbf{problema minimale}.

\paragraph{Caratteristiche dei job}
I job sono caratterizzati da
\begin{itemize}[label=-]
    \item Un \textbf{set di operazioni} da eseguire, legate da \textbf{vincoli di precedenza}.
    \item \textbf{Tempi di lavorazione} per l’esecuzione su ogni macchina.
    \item \textbf{Date di rilascio} e \textbf{date di consegna}.
\end{itemize}
\hspace*{1.5em}Assunzioni comuni sono che un job possa essere in lavorazione su \textbf{al più una macchina per volta},
che ogni macchina possa lavorare \textbf{un job per volta}, e che le lavorazioni \textbf{non possano essere interrotte}.
Ognuna di tali assunzioni può essere violata (\textbf{lot streaming}, \textbf{processori batch}, \textbf{pre-emption}).

\paragraph{Soluzioni e diagrammi di Gantt}
Una soluzione di un problema di scheduling è caratterizzata dalle \textbf{sequenze di lavorazione} sulle macchine.
Una soluzione può essere visualizzata mediante un \textcolor{ForestGreen}{\textbf{diagramma di Gantt}}, che rappresenta graficamente l’allocazione
temporale dei job sulle macchine. 

\subsection{Tipi di flusso}
\paragraph{Strutture di precedenza}
Esistono diverse \textbf{strutture di precedenza} tra le operazioni dei job, che caratterizzano la
\textbf{tecnologia del ciclo di lavorazione}.  
Sono possibili \textbf{strutture lineari}, oppure \textbf{strutture arbitrarie} rappresentabili mediante \textbf{grafi}.
Le precedenze si traducono in \textbf{vincoli} di un problema di ottimizzazione.
Il \textcolor{ForestGreen}{\textbf{flusso dei materiali}} caratterizza diverse strutture in termini di macchine
\begin{itemize}[label=-]
    \item \textbf{Macchina singola}.
    \item \textbf{Macchine parallele} (identiche: $p_{im}=p_i$, correlate: $p_{im}=p_{ism}$, scorrelate).
    \item \textbf{Flow shop}.
    \item \textbf{Job shop}.
    \item \textbf{Open shop}.
\end{itemize}
\hspace*{1.5em}Oltre a tali strutture prototipali, si osservano varianti sul tema, e sono possibili \textbf{strutture ibride}
(ad esempio \textbf{flexible flow shop}).

\subsection{Misure di prestazione}

\paragraph{Funzioni di penalità} Si indica con \textbf{$C_i$} il \textbf{tempo di completamento} del job $i\in[n]$,
ovvero il tempo di completamento della sua \textbf{ultima operazione}.
Un tipico modo per costruire misure di prestazione è associare ad ogni job $J_i$ un \textbf{termine di penalità}
$\gamma_i(C_i)$.
I termini tipicamente utilizzati sono
\begin{itemize}[label=-]
    \item \textbf{Tempo di completamento}: $\gamma_i(C_i)=C_i$.
    \item \textbf{Flow time}: $\gamma_i(C_i)=F_i=C_i-r_i$.
    \item \textbf{Lateness}: $\gamma_i(C_i)=L_i=C_i-d_i$.
    \item \textbf{Tardiness}: $\gamma_i(C_i)=T_i=\max\{C_i-d_i,0\}$.
    \item \textbf{Earliness}: $\gamma_i(C_i)=E_i=\max\{d_i-C_i,0\}$.
    \item \textbf{Indicatore di ritardo}:
    \(
    \gamma_i(C_i)=U_i=
    \begin{cases}
    1 & \text{se } C_i>d_i,\\
    0 & \text{se } C_i\le d_i.
    \end{cases}
    \)
\end{itemize}

\paragraph{Pesi e priorità dei job}
È possibile associare \textbf{pesi $w_i$} ai job, allo scopo di esprimere una \textbf{priorità}.  
Si può per esempio valutare per ogni job $J_i$ la \textbf{tardiness pesata} $w_iT_i$.
\\ \hspace*{1.5em}Sulla base di questi ``\textbf{mattoncini}'' si possono costruire misure di prestazione \textbf{aggregate},
ottenendo funzioni del tipo:
\(
\min\sum_{i=1}^n\gamma_i(C_i),
\qquad
\min\max_{i=1,\dots,n}\gamma_i(C_i).
\)

\paragraph{Principali misure di prestazione aggregate}
Le \textcolor{ForestGreen}{principali misure di prestazione aggregate} sono
\begin{itemize}[label=-]
    \item \textbf{Flow time totale}: $\sum_i F_i$.
    \item \textbf{Flow time totale pesato}: $\sum_i w_iF_i$.
    \item \textbf{Massima lateness}: $L_{\max}=\max_i L_i$.
    \item \textbf{Tardiness totale pesata}: $\sum_i w_iT_i$.
    \item \textbf{Makespan}: $C_{\max}=\max_i C_i$.
    \item \textbf{Numero di job in ritardo}: $\sum_i U_i$.
\end{itemize}

\paragraph{Equivalenza tra misure di prestazione}
Alcune misure sono tra di loro \textbf{equivalenti}, nel senso che una soluzione ottima rispetto a una di esse è ottima
anche per l’altra.
Ad esempio, \emph{la \textbf{lateness totale} è equivalente al \textbf{flow time totale}}
\[
\sum_{i=1}^n L_i
=
\sum_{i=1}^n C_i-\sum_{i=1}^n d_i
=
\sum_{i=1}^n F_i+\sum_{i=1}^n(r_i-d_i).
\]
\hspace*{1.5em}Le due misure differiscono per una \textbf{costante}. Inoltre, \textcolor{ForestGreen}{una soluzione ottima
rispetto a \textbf{$L_{\max}$} è ottima anche rispetto a \textbf{$T_{\max}$}} (ma non è assicurato il viceversa):
\(
T_{\max}=\max\{L_{\max},0\}.
\)

\paragraph{Misure di prestazione regolari}
Le misure di prestazione finora elencate sono \textbf{funzioni non decrescenti} dei tempi di completamento;
si parla in questo caso di \textcolor{ForestGreen}{\textbf{misure di prestazione regolari}}.
Nel caso di misure regolari, le informazioni di \textbf{tempistica} possono essere ricavate dal
\textbf{sequenziamento}, in quanto ogni operazione è eseguita \textbf{al più presto} (\textbf{soluzioni attive}).

\paragraph{Misure di prestazione non regolari}
Nel caso di \textcolor{ForestGreen}{\textbf{misure non regolari}}, ciò non è più vero e il problema risulta
\textbf{più complesso}.
Tali misure sono utili per tenere conto dell’\textbf{inopportunità di completare un job prima della due date},
ad esempio adottando come misura
\[
\sum_{i=1}^n\bigl(w_i^T T_i + w_i^T E_i\bigr).
\]
\hspace*{1.5em}In questo caso esiste una regione in cui la funzione \textbf{decresce} al crescere del tempo di
completamento, e \textbf{sequencing e timing non coincidono} e la soluzione ottima non è necessariamente
\textcolor{ForestGreen}{\textbf{attiva o semiattiva}}. Una schedulazione si dice \textbf{semiattiva} se, fissato
il sequencing, ogni operazione è eseguita il più presto possibile senza violare l’ordine dei job su alcuna macchina.
Mentre, una schedulazione è \textbf{attiva} se non esiste alcuna operazione che possa essere anticipata senza ritardarne almeno un’altra; ogni soluzione attiva è anche semiattiva, ma non viceversa.


\paragraph{Notazione di Graham}
Per classificare i problemi di schedulazione si utilizza la codifica a tre campi \textcolor{ForestGreen}{\textbf{$\alpha|\beta|\gamma$}}
dovuta a \textcolor{ForestGreen}{\textbf{Graham}}.\\
\hspace*{1em}\textbf{Campo $\alpha$: layout delle macchine}
Il campo \textcolor{ForestGreen}{$\alpha$} descrive il \textbf{layout delle macchine}
\begin{itemize}[label=-]
    \item \textbf{$1$}: macchina singola.
    \item \textbf{$P,Q,R$}: macchine parallele identiche, correlate e scorrelate.
    \item \textbf{$F$}: flow shop (ad esempio $F2$).
    \item \textbf{$J$}: job shop.
\end{itemize}
\hspace*{1em}\textbf{Campo $\beta$: vincoli aggiuntivi}
Il campo \textcolor{ForestGreen}{$\beta$} descrive la presenza di eventuali \textbf{vincoli aggiuntivi}
\begin{itemize}[label=-]
    \item \textbf{res}: risorse aggiuntive.
    \item \textbf{prec}: vincoli di precedenza tra job.
    \item \textbf{$r_i$}: tempi di rilascio non nulli.
    \item \textbf{$\bar d_i$}: deadline hard.
    \item \textbf{$s_{ij}$}: setup dipendenti dalla sequenza.
    \item \textbf{perm}: flow shop a permutazione, la frequenza di job si mantiene su tutte le macchine.
    \item \textbf{no-wait}: assenza di buffer tra operazioni: ragioni tecnologiche impongono che le operazioni vengano eseguite consecutivamente senza interruzioni.
\end{itemize}
\hspace*{1em}\textbf{Campo $\gamma$: misura di prestazione}
Il campo \textcolor{ForestGreen}{$\gamma$} descrive la \textbf{misura di prestazione} da ottimizzare.

\subsection{Algoritmi di soluzione}
\paragraph{Algoritmi polinomiali}Esistono alcuni casi semplici risolvibili mediante \textcolor{ForestGreen}{\textbf{algoritmi polinomiali}}
\begin{itemize}[label=-]
    \item \textbf{$1//L_{\max}$}: regola \textbf{EDD} che ordina i job in ordine di \textbf{due date} crescente.
    \item \textbf{$1//\sum_i w_i c_i$}: regola \textbf{WSPT} (weigthed shortest processing time) che ordina i job in ordine decrescente del rapporto $w_i/p_i$.
    \item \textbf{$F2//C_{\max}$}: algoritmo di \textbf{Johnson} (in questo caso la soluzione ottima applica la stessa sequenza sulle due macchine).
\end{itemize}

\paragraph{Problemi NP-hard ed euristiche}
In generale, i problemi di schedulazione sono \textbf{NP-hard} e vengono affrontati
mediante \textbf{euristiche}, sia costruttive sia iterative (\textbf{metaeuristiche} basate su ricerca locale).
Le \textbf{regole di priorità} costituiscono una strategia costruttiva di base.

\paragraph{Regola ATC (Apparent Tardiness Cost)} Esistono anche regole non banali, come la
\textcolor{ForestGreen}{\textbf{regola ATC (Apparent Tardiness Cost)}},
proposta per problemi \textbf{$J//\sum_i w_i T_i$}.
L’espressione della priorità del job $J_i$ sulla macchina $M_j$ al tempo $t$ è
\[
\frac{w_i}{p_{ij}}
\exp\!\left(
-
\left[
\frac{d_i - t - p_{ij} -
\sum_{q=j+1}^{m_i}(W_{iq}+p_{iq})}{k \bar{p}}
\right]^+
\right),
\]
dove $w_i$ è il peso del job $J_i$ e $p_{ij}$ è il suo tempo di processo sulla macchina
$M_j$; $W_{iq}$ è una stima del tempo
di attesa di $J_i$ sulla macchina $M_q$; $m_i$ è il numero di macchine che il job deve
ancora visitare; $k$ è un parametro da selezionare e $\bar{p}$ è il tempo di processo
medio dei job in coda.

\paragraph{Interpretazione del termine di slack}
L’espressione
\[
t^* = d_i - p_{ij} -
\sum_{q=j+1}^{m_i}(W_{iq}+p_{iq}),
\]
può essere interpretata come il \textcolor{ForestGreen}{\emph{massimo tempo di inizio}} di $J_i$ su $M_j$, ottenuto
sottraendo un lead time stimato dalla due date $d_i$.
Se $t \le t^*$ si può ritenere di essere \textbf{in tempo}, altrimenti si corre il rischio di
arrivare in ritardo.
\textbf{Se si è in tempo}, il termine tra parentesi quadre è \emph{positivo}, e la priorità cresce
esponenzialmente al crescere di $t$. \\
\hspace*{1.5em}\textbf{Quando si è in ritardo}, il termine tra parentesi quadre è \emph{negativo}, per cui
l’argomento dell’esponenziale è zero. La priorità è quindi $w_i/p_{ij}$, che coincide
con la regola WSPT.

\paragraph{Relazione con la tardiness totale pesata}
\emph{Se molti job sono in ritardo, la tardiness totale pesata diventa una funzione obiettivo
quasi equivalente al tempo di completamento totale pesato}
\[
\sum_i w_i T_i =
\sum_i w_i \max\{C_i-d_i,0\}
\approx
\sum_i w_i C_i - \sum_i w_i d_i .
\]
\hspace*{1.5em}Il problema di minimizzazione di $\sum_i w_i C_i$ è risolto, su macchina singola,
dalla \textbf{regola WSPT}.

\paragraph{Lookahead e ricerca locale}
La \textbf{miopia} delle regole di priorità può essere ridotta introducendo un certo grado di
\textcolor{ForestGreen}{\textbf{lookahead}}, ad esempio mediante strategie di \emph{beam search}.
Una vasta classe di algoritmi iterativi si basa su perturbazioni locali della soluzione
corrente, che definiscono una struttura di vicinato.\\
I principali problemi sono
\begin{itemize}[label=-]
    \item Sfuggire a \textbf{minimi locali} (tabu search, algoritmi genetici, questi richiedono attenta codifica e decodifica delle soluzioni).
    \item Esplorare \textbf{grandi vicinati} in maniera efficiente (LNS, Large Neighborhood
    Search, utile anche per misure non regolari).
    \item Evitare la \textbf{creazione di cicli}.
\end{itemize}
Molte di tali questioni richiedono una formulazione mediante un \textbf{grafo disgiuntivo}.

\paragraph{Grafi disgiuntivi}
In un \textcolor{ForestGreen}{\textbf{grafo disgiuntivo}}, i nodi rappresentano le operazioni, con l’aggiunta di nodi
dummy iniziali e finali.
Gli \textbf{archi congiuntivi} rappresentano vincoli di precedenza tecnologici, associati ai
cicli di ciascun job.
Gli \textbf{archi disgiuntivi} vanno orientati e rappresentano i vincoli di capacità per ogni
macchina (una clique per macchina).
Il \textbf{cammino critico}, ossia il cammino di lunghezza massima dal nodo iniziale al nodo
finale, fornisce il makespan.

\subsection{Modello MILP per $J//C_{max}$}

\paragraph{Formulazione} Sulla base del grafo disgiuntivo possiamo costruire un \textcolor{ForestGreen}{modello MILP} per il problema $J//C_{max}$.
Sia $N=\{0,1,\dots,N-1,N\}$ l’insieme dei nodi, dove $0$ e $N$ sono nodi dummy.
Sia $P$ l’insieme degli archi congiuntivi e $D$ l’insieme degli archi disgiuntivi.
La variabile binaria $x_{ij}$, se posta pari a $1$, indica che l’operazione $i$ precede
l’operazione $j$.
\begin{align*}
\min \quad & C_N \\
\text{s.t.}\quad
& C_j \ge C_i + p_j, && \forall (i,j)\in P, \\
& C_j \ge C_i + p_j - M(1-x_{ij}), && \forall (i,j)\in D, \\
& C_i \ge C_j + p_i - M x_{ij}, && \forall (i,j)\in D, \\
& x_{ij}\in\{0,1\}, && \forall (i,j)\in D, \\
& C_i \ge p_i, && \forall i\in N .
\end{align*}
\hspace*{1.5em}Il modello può essere adattato ad altre misure di prestazione, ma non è trattabile in
pratica a causa di bound deboli. Tuttavia costituisce una base utile per
metaeuristiche e approcci LNS (\emph{destroy and repair}).

\paragraph{Il ruolo del cammino critico}
In una strategia di ricerca locale, perturbazioni arbitrarie delle sequenze possono
creare cicli. Nel problema $J//C_{\max}$, \emph{se si perturbano archi disgiuntivi appartenenti al
cammino critico, non si possono creare cicli, e tali perturbazioni sono le sole utili}.

\subsection{Procedura Shifting Bottleneck}
\paragraph{Idea generale}
La procedura \textcolor{ForestGreen}{\textbf{shifting bottleneck}}, originariamente pensata per il problema
$J//C_{\max}$, sfrutta il grafo disgiuntivo per \emph{decomporre il problema} in una
sequenza di problemi del tipo $1/r_i/L_{\max}$, per i quali sono disponibili algoritmi \emph{branch-and-bound} efficienti.
Se una macchina $M_b$ è il \textbf{collo di bottiglia}, le altre macchine possono essere
trattate come \emph{risorse a capacità infinita}, eliminando i relativi archi disgiuntivi corrispondenti alle altre macchine
e mantenere solo quelle corrispondenti alle altre macchine.

\paragraph{Teste e code nel grafo disgiuntivo}
Si consideri il nodo $O_{ib}$ del grafo, corrispondente all’operazione del job $J_i$
sulla macchina $M_b$, e sia $C_{ib}$ il suo tempo di completamento.
Si consideri il \textbf{cammino critico} dal nodo fittizio iniziale al nodo $O_{ib}$.
La sua lunghezza $h_{ib}$ è la somma dei tempi di processo delle operazioni di $J_i$
che precedono l’operazione $O_{ib}$, e ne costituisce la \textbf{testa}, ovvero il
\textbf{tempo di rilascio} dell’operazione $O_{ib}$.
In modo analogo si definisce la \textbf{coda} $t_{ib}$ come la lunghezza del cammino
critico dal nodo $O_{ib}$ al nodo fittizio finale. In questo caso $t_{ib}$ è la somma
dei tempi di processo delle operazioni che seguono $O_{ib}$.

\paragraph{Approssimazione del makespan}
Si definisce una data di consegna per il job $J_i$ sulla macchina $M_b$,
approssimando il makespan come
\(
C_{\max} = \max_i \{C_i\} \;\approx\; \max_i \{C_{ib} + t_{ib}\}.
\)
Se si sottrae una costante arbitraria $K$ dalla misura di prestazione, la soluzione
ottima del problema non cambia, ottenendo il problema equivalente
\[
\max_i \{C_{ib} + t_{ib}\} - K
= \max_i \{C_{ib} - (K - t_{ib})\}
= \max_i \{C_{ib} - d_{ib}\},
\]
dove è stata introdotta una \textbf{due date locale}
\(
d_{ib} = K - t_{ib}.
\)
Più lunga è la coda $t_{ib}$, più stretta è la data di consegna $d_{ib}$.

\paragraph{Riduzione a $1/r_i/L_{max}$}
Il problema $J//C_{\max}$ si riconduce quindi a un problema
$1/r_i/L_{\max}$, in cui:
\(
r_{ib} = h_{ib}, \; d_{ib} = K - t_{ib}.
\)
I tempi di rilascio sono dati dalle \textbf{teste}, mentre le date di consegna
dipendono dalle \textbf{code}.

\paragraph{Identificazione del collo di bottiglia}
Un modo ragionevole per identificare il \textbf{collo di bottiglia} consiste nel
risolvere i problemi $1/r_i/L_{\max}$ per tutte le macchine, definendo per ciascuna
le teste e le code delle operazioni.
Il collo di bottiglia è la macchina che presenta il valore di $L_{\max}$ peggiore.
Dopo avere risolto i problemi su macchina singola, si individua il collo di bottiglia
$M_b$ e la relativa sequenza. È quindi possibile orientare gli archi disgiuntivi
corrispondenti a $M_b$ e rischedulare le altre macchine.
In pratica, si risolvono i problemi su macchina singola per le rimanenti $M-1$
macchine, aggiornando teste e code. Il procedimento prosegue fino ad avere
schedulato tutte le macchine, congelandole una per volta.
Sono possibili \emph{raffinamenti iterativi}, e la strategia si estende facilmente al problema
$J/r_i/L_{max}$; sono state proposte anche ulteriori estensioni.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF 6 %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1cm}
\textcolor{BurntOrange}{\section{CAP 6 -- Fondamenti microeconomici del pricing e modelli di scelta discreta}}
\paragraph{Driver del profitto}
Il profitto è determinato da quattro fattori fondamentali ed è descritto dal seguente modello semplice:
\textcolor{BurntOrange}{\(
\text{Profitto} = (\text{Prezzo} - \text{Costo variabile}) \cdot \text{Volume} - \text{Costo fisso}.
\)}

\paragraph{Sensibilità del profitto ai fattori} \emph{Quale sarebbe l'impatto di una variazione di ciascun fattore, supponendo che gli altri
rimangano invariati?}
Si considerano variazioni percentuali dei fattori nell’intervallo da $-10\%$ a $+10\%$ e il loro impatto percentuale sul profitto.
Un aumento del prezzo del $10\%$ comporta un raddoppio del profitto. Al contrario, una riduzione del costo variabile del $10\%$ implica un aumento del profitto pari al $50\%$. Gli altri due fattori hanno un impatto minore.
Il grafico risulta simmetrico rispetto alle variazioni positive e negative.

\paragraph{Osservazioni}
\emph{È importante notare che la modifica del prezzo è essenzialmente istantanea e priva di costo, a differenza degli investimenti in pubblicità o riduzione dei costi}.
Tuttavia, il \textbf{modello è poco realistico} poiché il prezzo influisce sul volume e il costo non è necessariamente
una funzione affine del volume. È quindi necessario considerare le \textcolor{BurntOrange}{interazioni tra i fattori},
pur utilizzando questo modello grezzo per sviluppare intuizioni.

\paragraph{Catene causali}
È necessario \textbf{modellare il legame tra prezzo e volume} (domanda) ed essere consapevoli delle possibili catene causali
\begin{itemize}[label=-]
    \item Prezzo $\rightarrow$ Volume $\rightarrow$ Ricavi $\rightarrow$ Profitto.
    \item Prezzo $\rightarrow$ Volume $\rightarrow$ Costi $\rightarrow$ Profitto.
    \item Prezzo $\rightarrow$ Prezzi dei concorrenti $\rightarrow$ Quota di mercato $\rightarrow$ Volume $\rightarrow$ Ricavi $\rightarrow$ Profitto.
    \item Prezzo del produttore $\rightarrow$ Prezzo del rivenditore $\rightarrow$ Volume $\rightarrow$ Ricavi $\rightarrow$ Profitto.
\end{itemize}

\paragraph{Pricing e revenue management}
L’\textbf{obiettivo} di un’impresa viene comunemente identificato nella \textcolor{BurntOrange}{\textbf{massimizzazione del profitto}}.
Tuttavia, l’analisi precedente mostra che ciò non si riduce alla sola minimizzazione dei costi.
\\ \hspace*{1.5em}Se il profitto è ricavo meno costo, si aprono ulteriori possibilità. Le strategie di \emph{revenue e yield management}
sono diventate comuni a partire dall’industria aerea.

\paragraph{Strategie di pricing}
Le \textcolor{BurntOrange}{\textbf{principali strategie di pricing}} includono
\begin{itemize}[label=-]
\item La strategia di pricing apparentemente più ovvia è quella \textbf{basata sui costi}: si valuta il \textbf{costo} di un prodotto e si applica un \textbf{mark-up}. Questo approccio semplice \textbf{non tiene conto} della reazione dei \textbf{consumatori} e dei \textbf{competitor}.
\item Un’altra idea, piuttosto comune in \textbf{economia}, è che il \textbf{prezzo} derivi dall’\textbf{equilibrio tra domanda e offerta}. Questo porta al concetto di \textbf{pricing basato sulla domanda}, che può essere affrontato tramite \textbf{modelli semplici} nel caso di \textbf{monopolio}.
\item Un ulteriore elemento è il \textbf{ruolo della concorrenza}. Nella letteratura classica di \textbf{microeconomia}, questo aspetto può essere analizzato tramite \textbf{modelli di teoria dei giochi}. Tali modelli possono essere utilizzati anche per descrivere l’\textbf{interazione tra impresa e consumatore}, e non solo tra imprese.
\end{itemize}

\paragraph{Tipologie di interazione}
Le \textbf{forme di competizione e interazione} sono molteplici
\begin{itemize}[label=-]
    \item Competizione tra imprese (oligopolio vs. monopolio).
    \item Interazioni lungo la supply chain (produttori e rivenditori).
    \item Comportamento strategico dei clienti.
\end{itemize}

\paragraph{Pricing nella pratica}
È necessario un cambio di paradigma:
\textcolor{BurntOrange}{\(
\text{Design--Build--Price} \rightarrow \text{Price--Design--Build}.
\)}
\\ \hspace*{1.5em}Occorre selezionare una strategia di posizionamento coerente:
luxury, premium, medium, low, ultra-low.
La strategia di pricing dipende dagli attributi chiave del prodotto o servizio:
funzionali, emotivi, simbolici, etici.

\paragraph{Scelta del modello}
È necessario scegliere il modello appropriato in funzione del contesto
\begin{itemize}[label=-]
    \item Acquisti ripetuti vs. acquisti singoli.
    \item Beni di prima necessità vs. beni voluttuari.
    \item Business-to-business (B2B) vs. business-to-consumer (B2C).
\end{itemize}

\paragraph{Meccanismi alternativi}
Il prezzo non è l’unico strumento per migliorare profitto e ricavi. Esistono diversi meccanismi alternativi:
\begin{itemize}[label=-]
    \item \textbf{Bundling e tying}, sconti di quantità, pacchetti e servizi ancillari; talvolta unbundling.
    \item Utilizzo di \textbf{coupon}.
    \item \textbf{Abbonamenti} personalizzati.
    \item Gestione della disponibilità di \textbf{classi differenti}.
    \item \textbf{Overbooking} per gestire no-show e capacità non stoccabile.
    \item \textbf{Condivisione del rischio} e progettazione di contratti e incentivi.
\end{itemize}

\subsection{Pricing e funzioni di domanda}

\paragraph{Un modello introduttivo: relazione lineare tra domanda e prezzo}
La domanda è una variabile casuale, influenzata da una molteplicità di fattori, incluso (ma non solo) il prezzo.
Come punto di partenza si introduce un \textbf{modello semplice} basato su una \textcolor{BurntOrange}{\textbf{funzione di domanda lineare}}
 che lega prezzo e domanda
\[
d(p) = \alpha - \beta p.
\]

\paragraph{Interpretazione dei parametri}
Un’ipotesi apparentemente ovvia è $\textcolor{BurntOrange}{\beta} > 0$, ma esistono controesempi forniti dai beni di lusso e dai casi in
cui il \textbf{prezzo} \emph{funge da segnale di qualità}.
L’intercetta \textcolor{BurntOrange}{$\alpha$} rappresenta la \textbf{domanda} quando $p = 0$, un livello di
prezzo per il quale il modello ha poco senso.
In altri modelli, la domanda tende all’infinito per $p \to 0$, ma se la popolazione dei consumatori è finita,
$\alpha$ può essere interpretato come la dimensione della popolazione.

\paragraph{Dominio di validità}
Poiché la domanda non può essere negativa, il modello ha senso nell’intervallo di prezzo
\(
p \in [0, p_{\text{lim}}],
\)
dove si definisce il prezzo limite
\(
p_{\text{lim}} = \frac{\alpha}{\beta}
\)
come il prezzo massimo per il quale la domanda è positiva.

\paragraph{Forma troncata della funzione di domanda}
Il modello può essere riscritto in una forma potenzialmente migliore come
\[
d(p) = (\alpha - \beta p)^+.
\]

\paragraph{Funzione di domanda inversa}
In microeconomia è comune \textbf{invertire la funzione di domanda} per ottenere il prezzo di mercato quando
una quantità $q$ viene prodotta e offerta sul mercato. Invertendo l’equazione $d(p)=\alpha-\beta p$,
si ottiene la \textcolor{BurntOrange}{\textbf{funzione di
domanda inversa}}
\[
p(q) = a - bq,
\]
dove $a = \alpha / \beta$ e $b = 1 / \beta$.
Questa forma è utile nello studio della \emph{competizione basata sulle quantità nella microeconomia elementare},
ma risulta meno naturale nel contesto di una modellazione statistica appropriata per il pricing management.
Tuttavia, esistono mercati in cui il prezzo emerge da meccanismi complessi (ad esempio aste) che dipendono dalla
disponibilità totale.

\paragraph{Massimizzazione dei ricavi e del profitto}
Un \textbf{obiettivo} naturale è la \textcolor{BurntOrange}{\textbf{massimizzazione del profitto}}.
Nella sua forma più semplice, il \emph{profitto} è dato da \emph{ricavi meno costi}, e può essere espresso come
funzione del prezzo o della quantità.
La seconda scelta è più naturale per esprimere i costi
\[
\pi(q) = p(q)\cdot q - c(q) = r(q) - c(q),
\]
dove la funzione di costo $c(q)$ è una funzione generica della quantità e può tenere conto di costi fissi,
economie o diseconomie di scala. Si introduce inoltre la \textcolor{BurntOrange}{\textbf{funzione di ricavo}}
\(
r(q) := p(q)\cdot q.
\)

\paragraph{Condizione di ottimalità}
Assumendo che tutte le funzioni siano differenziabili e che la funzione di profitto complessiva sia \emph{concava},
si applica la \textbf{condizione di ottimalità del primo ordine}
\[
\pi'(q^*) = r'(q^*) - c'(q^*) = 0 \;\;\Rightarrow\;\; r'(q^*) = c'(q^*).
\]
\hspace*{1.5em}La \textbf{quantità ottimale} è tale per cui il \textcolor{BurntOrange}{ricavo marginale} e il \textcolor{BurntOrange}{costo marginale} coincidono.
Se il ricavo marginale è maggiore del costo marginale, il beneficio in termini di ricavi derivante da un aumento della produzione supera l’aumento dei costi, e conviene aumentare $q$. Se il costo marginale è maggiore del ricavo marginale, conviene ridurre $q$.
\\ \hspace*{1.5em}Nel caso di \textbf{funzione di domanda inversa} lineare e costo variabile costante,
il \emph{profitto è una funzione quadratica \textcolor{BurntOrange}{concava}}
\[
\pi(q) = (a - bq)\cdot q - cq = (a - c)\cdot q - bq^2,
\]
e la \emph{condizione di ottimalità del primo ordine} è:
\(
\pi'(q) = (a - c) - 2bq,
\)
da cui si ottiene
\[
q^* = \frac{a - c}{2b}.
\]

\paragraph{Osservazioni}
\textcolor{BurntOrange}{La massimizzazione del profitto non è equivalente alla minimizzazione dei costi.}
La minimizzazione dei costi è appropriata quando una certa domanda deve comunque essere soddisfatta e occorre scegliere
tra diverse tecnologie produttive o piani di produzione nel tempo.
In alcuni settori il costo marginale di un’unità aggiuntiva è trascurabile (industria aerea, servizi web) o il costo di
produzione è un costo sommerso (markdown management). In questi casi può essere sensata la massimizzazione dei ricavi,
naturalmente espressa come funzione del prezzo
\[
\max r(p) = p\cdot d(p) = p\cdot(\alpha - \beta p) = \alpha p - \beta p^2.
\]
\hspace*{1.5em}La \textbf{soluzione ottima per la massimizzazione dei ricavi} è
\[
p^* = \frac{\alpha}{2\beta}
\quad\Rightarrow\quad
d^* = \frac{\alpha}{2}, \;
r^* = \frac{\alpha^2}{4\beta}.
\]

\paragraph{Interpretazione geometrica}
La soluzione può essere \textbf{interpretata geometricamente}: il prezzo ottimale è il 
\emph{punto medio} dell’intervallo dei prezzi e massimizza l’area del rettangolo associato ai ricavi.

\paragraph{Stima del modello}
Nella \textcolor{BurntOrange}{microeconomia elementare} si utilizzano \textbf{modelli deterministici},
ma anche un modello di domanda lineare apparentemente banale come
\(
d(p) = \alpha - \beta p
\)
contiene parametri ignoti che devono essere stimati.

\paragraph{Apprendimento e sperimentazione}
Anche assumendo che il modello lineare sia corretto, esiste un problema rilevante: il costo della procedura di stima.
\emph{È necessario apprendere online piuttosto che offline, e la pianificazione degli esperimenti è cruciale.
Quali prezzi utilizzare per apprendere efficacemente i parametri?}
L’attenzione dovrebbe concentrarsi sulla pendenza $\beta$, poiché l’intercetta $\alpha$ è principalmente un costrutto matematico.
\\ \hspace*{1.5em}L’\textcolor{BurntOrange}{\textbf{errore standard della stima}} della pendenza è dato da
\[
SE_\beta =
\frac{\sigma_\varepsilon}{\sqrt{\sum_{k=1}^n (p_k - \bar{p})^2}},
\]

dove $\sigma_\varepsilon$ è la deviazione standard (ignota) degli errori.
\emph{Questo mostra che maggiore è la dispersione dei prezzi sperimentati, migliore è l’apprendimento}.
Tuttavia, prezzi molto bassi o molto elevati possono danneggiare i ricavi, generando un trade-off tra
apprendimento e costo dell’apprendimento.

\paragraph{Problemi pratici aggiuntivi}
Nella pratica emergono ulteriori difficoltà. In primo luogo, non tutti i \textbf{prezzi commerciali} sono effettivamente applicabili. Inoltre, \textbf{grandi variazioni di prezzo} tendono a enfatizzare la \textbf{non linearità} del comportamento della domanda. Un altro aspetto rilevante è il \textbf{ruolo del tempo}: nei \textbf{beni di moda} il tempo è un fattore essenziale e il \textbf{periodo di apprendimento} risulta spesso limitato.  
Si aggiungono poi i \textbf{fattori confondenti}, poiché una variazione della domanda può dipendere da elementi diversi dal prezzo stesso. Vi sono anche \textbf{problemi operativi ed esecutivi}, come ad esempio la necessità di modificare i cartellini dei prezzi in un negozio fisico. Infine, una \textbf{sperimentazione eccessiva} sui prezzi può generare \textbf{insoddisfazione nei consumatori} e indurre \textbf{comportamenti strategici}.

\paragraph{La funzione di costo}
È comune considerare una componente di costo fisso e una variabile; la forma più semplice di \textcolor{BurntOrange}{costo variabile è \textbf{lineare}}
\[
c(q) = F + cq.
\]
\hspace*{1.5em}Una \textbf{funzione di costo alternativa} è
\(
c(q) =
\begin{cases}
F + cq & \text{se } q > 0, \\
0 & \text{se } q = 0,
\end{cases}
\)
che può essere scritta come
\[
c(q) = F \cdot \delta(q) + cq,
\]
dove $\delta(q) = 1$ se $q > 0$ e $\delta(q) = 0$ altrimenti.
Per evitare ambiguità, si parla di \textcolor{BurntOrange}{\textbf{costo fisso}} nel primo caso e di \textcolor{BurntOrange}{\textbf{carica fissa}} nel secondo.
Si noti che questa distinzione può dipendere dalla scala temporale e dal livello decisionale gerarchico: alla scala temporale appropriata, tutti i costi sono variabili.

\paragraph{Costo marginale e costo medio}
Data una funzione di costo, si definiscono il \textbf{costo marginale} $c'(q)$ e il \textbf{costo medio} $\dfrac{c(q)}{q}$.


\paragraph{Regolarità della funzione di costo}
Per definire il costo marginale è necessario assumere la \emph{differenziabilità}.
Oltre alle cariche fisse, discontinuità possono essere introdotte da sconti di quantità all-units, mentre sconti incrementali introducono punti angolosi.
Quando la produzione è discreta e $q$ assume valori interi, il costo marginale può essere approssimato come
\(
c(q+1) - c(q).
\)

\paragraph{Economie e diseconomie di scala}
Il \textbf{costo marginale} può essere crescente o decrescente.
Quando il costo marginale è \textcolor{BurntOrange}{decrescente}, assumendo la differenziabilità, si ha $c''(q) < 0$,
e quindi la funzione di costo è concava. Questo modella un’\textcolor{BurntOrange}{\textbf{economia di scala}},
ossia un aumento dell’efficienza all’aumentare della produzione.
Al contrario, una funzione di costo convessa, caratterizzata da un costo marginale \textcolor{BurntOrange}{crescente},
modella una \textcolor{BurntOrange}{\textbf{diseconomia di scala}}.

\paragraph{Funzioni di domanda}
Esiste una grande varietà di modelli di domanda
\begin{itemize}[label=-]
    \item Si può \textbf{modellare} la domanda aggregata dell’intero mercato, di un canale selezionato o di una parte del mercato, oppure la domanda di un singolo individuo.
    \item Si possono considerare acquisti ripetuti oppure no, a seconda della natura del bene o servizio (beni stock vs. beni flusso) e dell’orizzonte temporale.
    \item Si può considerare o meno l’\textbf{incertezza}.
\end{itemize}
\hspace{1.5em}
I \textbf{fattori di input} possono includere tempo, vendite passate, prezzi dei concorrenti o di beni alternativi.
L’\textbf{output} del modello può essere una variabile
\begin{itemize}[label=-]
    \item \textbf{Reale} (domanda aggregata, eventualmente approssimante un valore intero elevato).
    \item \textbf{Intera} (quantità acquistata da un singolo consumatore).
    \item \textbf{Binaria} (acquisto o non acquisto).
    \item \textbf{Categorica} o un vettore (bundle o portafoglio di beni acquistati).
\end{itemize}

\paragraph{La funzione di domanda lineare}
Per scopi principalmente illustrativi, si è già considerata la funzione di domanda lineare:
\(
d(p) = (\alpha - \beta p)^+.
\)\\ \hspace*{1.5em} 
È naturale interrogarsi sulla \textbf{validità di un modello così semplice}
\begin{itemize}[label=-]
    \item Come può essere giustificato dal punto di vista economico? È necessario indagare una fondazione microeconomica per il modello di domanda.
    \item È un modello validato empiricamente o è contraddetto dal buon senso e dal comportamento reale dei consumatori?
\end{itemize}
\hspace*{1.5em}Assumendo la \emph{differenziabilità}, una caratteristica fondamentale di ogni funzione di domanda è 
la sua \textbf{pendenza} $d'(p)$, che per buon senso dovrebbe essere \emph{negativa}, come nel modello lineare.
In effetti è solitamente negativa, ma esistono eccezioni: segnale di qualità e beni di lusso e consumo vistoso.\\
\hspace*{1.5em}Possono quindi esistere intervalli di prezzo per i quali la pendenza è \emph{positiva}, un fatto
confermato da evidenze empiriche.

\paragraph{Funzioni di domanda: elasticità puntuale}
Poiché il valore della pendenza dipende dalla scala, ovvero dalle unità di misura della domanda, una misura
più appropriata è l’\textcolor{BurntOrange}{\textbf{elasticità di prezzo della domanda}}
\[
\frac{\delta d / d}{\delta p / p}
=
\frac{\delta d}{\delta p} \cdot \frac{p}{d},
\]
facendo tendere $\delta p \to 0$, si ottiene l’\textcolor{BurntOrange}{\textbf{elasticità puntuale}}
\[
\varepsilon(p) = -\frac{d'(p)p}{d(p)}, 
\]
dove il segno \emph{meno} è introdotto per comodità, poiché la pendenza è tipicamente negativa.\\ \hspace*{1.5em}
Le \emph{funzioni di domanda} possono essere classificate come
\begin{itemize}[label=-]
    \item \textbf{Elastiche}, quando $\varepsilon(p) > 1$ (perfettamente elastiche se pari a $\infty$).
    \item A \textbf{elasticità unitaria}, quando $\varepsilon(p) = 1$.
    \item \textbf{Anelastiche}, quando $\varepsilon(p) < 1$ (perfettamente anelastiche se pari a 0).
\end{itemize}
\emph{La natura della domanda può dipendere dal punto specifico considerato.}

\paragraph{Esempio: funzione di domanda lineare}
\textcolor{BurntOrange}{\emph{L’elasticità non deve essere confusa con la pendenza.}}
Il modello di domanda lineare presenta una pendenza costante, ma ciò non implica un’elasticità puntuale costante.\\ \hspace*{1.5em}
Per una funzione di domanda lineare, l’\textbf{elasticità} è
\(
\varepsilon(p) =
\frac{\beta p}{\alpha - \beta p}.
\)
Essa cresce da zero a $+\infty$ nell’intervallo $[0, p_{\text{lim}}]$.
L’elasticità è pari a 1 per
\[
p = \frac{\alpha}{2\beta} = p^*,
\]
ossia nel \emph{punto medio dell’intervallo dei prezzi}.
La domanda è \textbf{anelastica} per \textcolor{BurntOrange}{$p < p^*$} ed \textbf{elastica} per \textcolor{BurntOrange}{$p > p^*$}.
Nel limite, quando \textcolor{BurntOrange}{$p \to p_{\text{lim}} = \alpha/\beta$}, diventa \textbf{perfettamente elastica}:
una piccola riduzione di prezzo fa passare la domanda da $d = 0$ a $d > 0$.
\textcolor{BurntOrange}{\emph{L’elasticità non è costante per una funzione di domanda lineare.}}

\paragraph{Esempio: funzione di domanda a elasticità costante}
È naturale chiedersi \emph{se esista una funzione di domanda con elasticità costante}.
Imponendo
\(
\frac{d'(p)p}{d(p)} = -\varepsilon,
\)
si ottiene l’equazione differenziale ordinaria
\(
d'(p) = -\varepsilon \cdot \frac{d(p)}{p},
\)
che ammette come soluzione
\(
d(p) = c \cdot p^{-\varepsilon},
\)
dove $c = d(1)$.

\paragraph{Stima tramite trasformazione logaritmica}
È possibile derivare un \emph{legame tra domanda a elasticità costante e domanda lineare} mediante una
trasformazione dei dati basata sui logaritmi
\[
d = c p^{-\varepsilon}
\;\Rightarrow\;
\log d = \log c - \varepsilon \log p.
\]
\hspace*{1.5em}In questo modo, gli strumenti standard di \textcolor{BurntOrange}{regressione lineare} possono essere
utilizzati per stimare il modello.
\textcolor{BurntOrange}{\emph{L’uso dei logaritmi è utile per semplificare modelli basati su effetti moltiplicativi anziché additivi.}}
\hspace*{1.5em}In termini di incrementi, per $x > 0$ vale
\[
\frac{d \log x}{dx} = \frac{1}{x}
\quad\Rightarrow\quad
\delta \log x \approx \frac{\delta x}{x}.
\]
Di conseguenza, l’\textbf{elasticità} può essere riscritta come
\[
\frac{\delta d / d}{\delta p / p}
\approx
\frac{d \log d(p)}{d \log p}.
\]
\hspace*{1.5em}
Nel caso di \textbf{domanda a elasticità costante}, il \textcolor{BurntOrange}{\textbf{ricavo}} è
\(
R(p) = p \cdot c p^{-\varepsilon} = c p^{1-\varepsilon}.
\)
Il ricavo è costante nel caso di elasticità unitaria, crescente nel caso di domanda anelastica e decrescente nel caso di domanda elastica.
\textcolor{BurntOrange}{\emph{Pertanto, una riduzione di prezzo aumenta i ricavi solo se la domanda è elastica.}}

\paragraph{Interpretare i modelli di domanda: willingness-to-pay}
Come si possono confrontare diversi modelli di domanda? La funzione di domanda lineare è un modello sensato?
Un \textbf{modello lineare} può essere (nel migliore dei casi) un’\textcolor{BurntOrange}{\textbf{approssimazione locale}}, ma è necessario comprendere più a
fondo la razionalità economica alla base di tale modello.
Ciò significa che occorre capire che cosa determina realmente la domanda.
\\ \hspace*{1.5em}
Un meccanismo di base dietro i modelli di domanda è il concetto di \textcolor{BurntOrange}{\textbf{prezzo di riserva o
willingness-to-pay}}: ogni consumatore è caratterizzato dal prezzo massimo al quale è disposto ad acquistare un bene.
In generale, si può produrre una \emph{funzione di domanda a tratti costante}.
\\ \hspace*{1.5em}L’intuizione suggerisce che, nel limite continuo, assumendo una willingness-to-pay uniformemente
distribuita, si ottenga una funzione di domanda lineare.
Funzioni di domanda alternative possono essere definite assumendo \emph{distribuzioni differenti} del prezzo di
riserva e studiando la sensibilità della domanda rispetto al prezzo.

\paragraph{Surplus del consumatore e discriminazione di prezzo}
Si consideri un consumatore disposto a pagare 100 per un bene. Se il prezzo è 70, si dice che il \textbf{consumer surplus}
è la differenza tra willingness-to-pay e prezzo pagato.
Quando viene applicato un prezzo unico, una parte di consumer surplus è ottenuta dall’insieme dei consumatori che acquistano.
\\ \hspace*{1.5em}\textcolor{BurntOrange}{\textbf{Geometricamente}}, ciò è visualizzabile come il triangolo
$CS$, con area
\[
CS = \frac{1}{2}\left(\frac{\alpha}{\beta} - p_0\right)\,(\alpha - \beta p_0)
= \frac{1}{2\beta}\,(\alpha - \beta p_0)^2,
\]
dove $p_0$ è il \textbf{prezzo scelto}.
Si noti che il consumer surplus è nullo quando $p_0 = p_{\lim} = \alpha/\beta$, ed è pari a $\alpha^2/(2\beta)$, ossia l’area totale del triangolo, quando $p_0 = 0$.
Il \textbf{ricavo raccolto} è rappresentato dall’area rettangolare $REV$.
\\ \hspace*{1.5em}
In \emph{microeconomia}, il consumer surplus è una misura approssimata dell’utilità del consumatore derivante dallo scambio.
Guardando dalla prospettiva opposta, il consumer surplus è, in un certo senso, ricavo perso per l’impresa.
Lo stesso vale per il triangolo $LR$, che rappresenta \textbf{ricavo perso} dovuto ai consumatori che non acquistano perché
$p_0$ eccede la loro willingness-to-pay.
\\ \hspace*{1.5em}Considerando solo i ricavi, \emph{l’obbligo di scegliere un solo prezzo genera perdite su entrambi i lati}:
consumer surplus (opportunità mancata di profitto maggiore) e vendite perse.
Idealmente, l’impresa vorrebbe applicare un prezzo diverso a ciascun individuo, pari alla sua willingness-to-pay (assumendo costo marginale nullo).
\\ \hspace*{1.5em}
Naturalmente, fissare un prezzo individuale non è pratico, per non dire apertamente illegale.
Come può un’impresa estrarre almeno parzialmente consumer surplus?
\textcolor{BurntOrange}{La chiave è la discriminazione di prezzo basata sulla segmentazione dei consumatori.}

\paragraph{Esempio di discriminazione di prezzo}
Applicando discriminazione di prezzo, l’impresa riesce a estrarre consumer surplus.
D’altra parte, una sottopopolazione che sarebbe esclusa dallo scambio può partecipare.
In generale, \emph{non è detto che si ottenga una soluzione win--win}.
Inoltre, \emph{potrebbe non essere possibile discriminare in modo netto}.
Esistono \textbf{diversi tipi di discriminazione}
\begin{itemize}[label=-]
    \item \textbf{Discriminazione completa}: ogni consumatore paga un prezzo specifico.
    \item \textbf{Segmentazione diretta}: segmenti identificabili pagano prezzi diversi.
    \item \textbf{Segmentazione indiretta}: si offrono varianti di prodotto/servizio e i consumatori scelgono; ciò funziona anche con caratteristiche non identificabili.
\end{itemize}

\paragraph{Formalizzare willingness-to-pay e consumer surplus}
Dopo un’introduzione informale, si passa a un quadro più formale.
La \textcolor{BurntOrange}{\textbf{willingness-to-pay}} \textbf{è il prezzo massimo (prezzo di riserva) che un consumatore è disposto a pagare per un bene}.
La distribuzione della willingness-to-pay nella popolazione determina la forma della \emph{funzione di domanda}.
\\ \hspace*{1.5em}Si consideri un modello continuo e si definisca $w(x)$ come densità della willingness-to-pay.
Essa svolge un ruolo simile alla densità di una variabile casuale, nel senso che
\[
\int_{p_1}^{p_2} w(x)\,dx
\]
è la frazione di popolazione con willingness-to-pay tra $p_1$ e $p_2$.
Quindi
\[
d(p) = D_{\max}\int_{p}^{+\infty} w(x)\,dx,
\]
dove $D_{\max}=d(0)$ è la \textbf{domanda massima ottenibile} (assumendo una popolazione finita).
Poiché
\(
d'(p) = -D_{\max}\,w(p),
\)
si può legare domanda e willingness-to-pay tramite
\[
w(p) = -\frac{d'(p)}{D_{\max}}.
\]

\paragraph{Surplus del consumatore aggregato}
Se un consumatore disposto a pagare $p_1$ paga invece $p_0<p_1$, gode di un consumer surplus $p_1-p_0$.
Dato $w(x)$, si integra il surplus al prezzo $p_0$ sulla popolazione che acquista e si definisce il 
\textbf{consumer surplus totale (netto)}
\[
S(p_0)=D_{\max}\int_{p_0}^{+\infty} w(x)\,(x-p_0)\,dx -\int_{p_0}^{+\infty} d'(x)\,(x-p_0)\,dx
= -\int_{p_0}^{+\infty} d'(x)x\,dx + p_0\int_{p_0}^{+\infty} d'(x)\,dx
\]
\[
= -\left[\left.d(x)\,x\right|_{p_0}^{+\infty}-\int_{p_0}^{+\infty} d(x)\,dx\right]-p_0d(p_0)
= \int_{p_0}^{+\infty} d(x)\,dx.
\]
Quindi, il \textcolor{BurntOrange}{\textbf{consumer surplus}} è \emph{l’area sotto la curva di domanda},
a destra del prezzo applicato $p_0$.

\paragraph{Obiettivo di estrarre consumer surplus}
Un \textbf{obiettivo naturale} per l’impresa è \textbf{``estrarre'' consumer surplus} almeno parzialmente tramite una
forma di discriminazione di prezzo.
\emph{Se l’impresa è in grado di discriminare, migliorerà il proprio profitto}.
L’intuizione suggerisce che ciò avvenga necessariamente a spese dei consumatori.
L’esempio seguente mostra che non è necessariamente così, se consumatori con willingness-to-pay minore possono
acquistare a un prezzo minore.

\paragraph{La funzione logit}
\textbf{Integrare la densità di willingness-to-pay}, analogamente a quanto si fa con una PDF per ottenere una CDF,
\textbf{fornisce una funzione di domanda}.
Un modello di domanda lineare non deriva da una distribuzione sensata della willingness-to-pay;
è ragionevole ipotizzare una funzione ``a campana'' con un massimo (\emph{moda}) concentrato attorno al prezzo di
riserva più tipico.
\\ \hspace*{1.5em}
Nella letteratura di statistica e marketing si utilizzano due funzioni per questo e altri scopi,
che portano a modelli probit e logit.
I modelli \textcolor{BurntOrange}{\textbf{probit}} derivano dalla CDF di una normale standard, mentre i modelli \textcolor{BurntOrange}{\textbf{logit}} si basano sulla funzione
logistica (sigmoide).

\paragraph{Definizione della funzione logistica}
La \textcolor{BurntOrange}{\textbf{funzione logistica}} è definita come
\[
L(x)=\frac{1}{1+e^{-x}}=\frac{e^x}{1+e^x},
\]
e presenta la classica forma a S.
Si osservi che $L(0)=0.5$ e che la funzione logit ha la \emph{proprietà di simmetria}:
\(
L(-x)=1-L(x).
\)
\\ \hspace*{1.5em}La \textbf{derivata} è
\[
L'(x)=\frac{e^x}{(1+e^x)^2}=L(x)\,(1-L(x)).
\]
La derivata ha la \emph{proprietà di simmetria} $L'(x)=L'(-x)$ e il suo massimo si ha in $x=0$.
\\ \hspace*{1.5em}Poiché le funzioni di domanda devono essere \emph{decrescenti}, si deve
ribaltare l’asse $x$ da sinistra a destra e introdurre \emph{fattori di scala} e traslazione sensati nella funzione logit,
ottenendo la seguente funzione di domanda logit (decrescente)
\[
d(p)=\frac{c\,e^{-(\alpha+\beta p)}}{1+e^{-(\alpha+\beta p)}},
\]
dove $\beta,\,c>0$.
\textcolor{BurntOrange}{Più grande è $\beta$, maggiore è la sensibilità al prezzo} e la funzione è più ripida per
\(
\alpha+\beta p^*=0 \;\Rightarrow\; p^*=-\frac{\alpha}{\beta},
\)
il che suggerisce $\alpha<0$.
Questo è il valore per cui la densità di willingness-to-pay è massima e può essere considerato un ``\textcolor{BurntOrange}{\textbf{prezzo di mercato}}''.
\\ \hspace*{1.5em}
La \textbf{willingness-to-pay} corrispondente è
\[
w(x)=-\frac{d'(p)}{d(0)}
=\frac{K\,e^{-(\alpha+\beta x)}}{\left(1+e^{-(\alpha+\beta x)}\right)^2},
\]
dove $K=\beta c/d(0)$.

\paragraph{Esempio: un modello ibrido}
In alcuni casi estremi, il prezzo di un bene potrebbe anche essere negativo per incrementare le vendite del bene complementare.
Questo può sembrare strano ma, in realtà, alcuni beni possono essere venduti sotto costo per ottenere ricavi dai beni complementari.
Ad esempio, una stampante laser permette di vendere cartucce toner, e un rasoio permette di vendere lame.
\\ \hspace*{1.5em}Una pratica comune nel revenue management è l’\textcolor{BurntOrange}{\textbf{inventory rationing}}, ossia la restrizione delle vendite.
Questo è lo strumento principale nel revenue management basato sulle quantità, come si vedrà.
Il \emph{disaccoppiamento} tramite razionamento fornisce gradi di libertà aggiuntivi.
\\ \hspace*{1.5em}L’idea del \textcolor{BurntOrange}{\textbf{modello ibrido}} è che prezzo e quantità non siano necessariamente variabili decisionali alternative.
Nella \textbf{prima formulazione}, si assume implicitamente che la domanda coincida con le vendite: fissati i prezzi, le quantità vendute sono determinate in modo automatico dalla funzione di domanda.
Questo porta però a soluzioni che, pur essendo ottimali dal punto di vista dei prezzi, possono risultare \textbf{non realistiche} in presenza di \textbf{vincoli di capacità}, perché la domanda generata potrebbe non essere fisicamente realizzabile.
\\ \hspace*{1.5em}Nella \textbf{seconda formulazione}, invece, si introduce un \textbf{disaccoppiamento tra domanda e vendite}.
La funzione di domanda non determina più direttamente le quantità vendute, ma fornisce un \textbf{vincolo superiore} sulle quantità offerte.
Questo consente di applicare il \textbf{razionamento delle scorte}, limitando le vendite anche quando la domanda sarebbe più elevata.
In questo modo, il modello acquisisce \textbf{gradi di libertà aggiuntivi}, permettendo di combinare decisioni di prezzo e di quantità in modo più coerente con i vincoli operativi.

\subsection{Modelli di teoria dei giochi}
\paragraph{Problemi decisionali con più decisori}
Talvolta le decisioni di pricing non possono essere prese ignorando il contesto complessivo.
In un contesto \textcolor{BurntOrange}{B2C}, i consumatori strategici sono un problema.
Le interazioni strategiche sono ancora più rilevanti in un contesto \textcolor{BurntOrange}{B2B}, dove i produttori possono essere collegati
da una supply chain in cui beni intermedi vengono scambiati e trasformati, oppure dove produttori e retailer che
distribuiscono beni finali interagiscono.
In tale contesto, i prezzi possono svolgere un ruolo importante come strumento per coordinare azioni e condividere rischi.
Infine, che dire della competizione tra imprese? Occorre distinguere tra competizione in termini di prezzi o di quantità,
e tra beni/servizi identici o differenziati.
Questi problemi possono essere (almeno parzialmente) affrontati con \textcolor{BurntOrange}{\textbf{modelli di teoria dei giochi}}.

\paragraph{Ipotesi semplificative}
Per semplicità, si considerano \emph{giochi stilizzati}
\begin{itemize}[label=-]
    \item Ci sono solo due decisori (giocatori); ciascun giocatore ha un \textbf{obiettivo} (payoff) che vuole massimizzare e non esiste cooperazione.
    \item Ciascun giocatore prende \textbf{una sola decisione}; quindi non si considerano giochi sequenziali con decisioni multiple nel tempo.
    \item Si assume \textbf{informazione completa} e common knowledge: non c’è incertezza sui dati del problema né sui meccanismi che mappano decisioni in payoff; i due giocatori condividono la stessa visione del mondo e le regole del gioco, conoscono gli incentivi della controparte e ciascuno sa che l’altro possiede tutte le informazioni rilevanti.
\end{itemize}
\hspace*{1.5em}Per avvicinarsi a una formalizzazione, si consideri il problema 
\[
\begin{aligned}
\max \ & \pi_1(x_1,x_2) + \pi_2(x_1,x_2) \\
\text{s.t.}\ & x_1 \in S_1, \quad
              x_2 \in S_2.
\end{aligned}
\]
La funzione obiettivo può essere interpretata come un \emph{profitto dipendente da due variabili decisionali},
$x_1$ e $x_2$, che devono appartenere ai rispettivi insiemi ammissibili $S_1$ e $S_2$.
\\ \hspace*{1.5em}
Anche se i vincoli su $x_1$ e $x_2$ sono separabili, \textbf{non è possibile decomporre il problema complessivo},
poiché le due decisioni interagiscono tramite le funzioni di profitto $\pi_1(x_1;x_2)$ e $\pi_2(x_1;x_2)$.
Risolvendo il problema si ottengono decisioni ottime $x_1^*$ e $x_2^*$ e il profitto totale ottimo
\(
\pi_{1+2}^*=\pi_1(x_1^*,x_2^*)+\pi_2(x_1^*,x_2^*).
\)
Così facendo si assume un \textbf{decisore unico} che prende entrambe le decisioni, oppure una coppia di decisori cooperativi che scelgono $x_1$ e $x_2$ condividendo l’obiettivo di massimizzare la somma dei profitti.

\paragraph{Caso non cooperativo} 
Nel caso realistico di \textbf{due decisori non cooperativi} con profitti $\pi_1(x_1,x_2)$ e $\pi_2(x_1,x_2)$,
\[
\begin{aligned}
\text{Decisore 1:}\quad
\begin{aligned}
\max \ & \pi_1(x_1,x_2) \\
\text{s.t.}\ & x_1 \in S_1
\end{aligned}
\qquad\qquad
\text{Decisore 2:}\quad
\begin{aligned}
\max \ & \pi_2(x_1,x_2) \\
\text{s.t.}\ & x_2 \in S_2
\end{aligned}
\end{aligned}
\]
Questi due problemi, scritti così, non hanno senso: quale valore di $x_2$ usare nell'altro problema? E viceversa.
È necessario chiarire come i due decisori muovono.
\begin{enumerate}[label=\arabic*.]
    \item Una possibilità è che i decisori agiscano in \textcolor{BurntOrange}{\textbf{modo sequenziale}}. Ad esempio, il decisore 1 sceglie $x_1 \in S_1$ prima che il decisore 2 scelga $x_2 \in S_2$. In tal caso, il decisore 1 è il leader e il decisore 2 è il follower. Nel prendere la decisione, il decisore 1 può cercare di anticipare la reazione del decisore 2 a ciascun valore possibile di $x_1$.
    \item Un’altra possibilità è che le due decisioni siano prese \textcolor{BurntOrange}{\textbf{simultaneamente}}. In tal caso, servono strumenti concettuali per capire quali decisioni aspettarsi.
\end{enumerate}

\paragraph{Equilibrio e perdita di performance}
La teoria dei giochi mira a fornire una previsione sensata di una \textbf{soluzione di equilibrio} $(x_1^e,x_2^e)$,
che dipende dalle ipotesi sulla struttura del gioco.
Qualunque equilibrio si ottenga, esso non può produrre un profitto totale maggiore di $\pi_{1+2}^*$, poiché
necessariamente vale
\[
\pi_{1+2}^e=\pi_1(x_1^e,x_2^e)+\pi_2(x_1^e,x_2^e)
\le
\pi_1(x_1^*,x_2^*)+\pi_2(x_1^*,x_2^*)=\pi_{1+2}^*.
\]
Se questa disuguaglianza fosse violata, $(x_1^*,x_2^*)$ non sarebbe soluzione ottima del problema iniziale.
Questo significa che, \textbf{decentralizzando le decisioni, il sistema complessivo è verosimilmente incapace di raggiungere la performance ottima globale}.

\textcolor{Red}{\textbf{FINIRE CAP 6 -- SLIDE 54/90}}

\paragraph{Giochi con decisioni continue}
Nel seguito si considera l’equilibrio di Nash nel caso in cui ogni giocatore disponga di un continuo di azioni possibili.
Si analizza prima il comportamento di due imprese che competono in termini di quantità. Entrambe massimizzano il profitto, ma influenzano reciprocamente il prezzo di mercato tramite la quantità totale prodotta. Il prezzo è comune a entrambe, poiché si assume un prodotto perfettamente identico.
Questa competizione si chiama competizione di Cournot; il caso in cui le imprese competono sui prezzi si chiama competizione di Bertrand.
Nel caso di competizione sui prezzi, si distingue tra prodotti omogenei o differenziati.

\subsection{Competizione di Cournot}

\paragraph{Impostazione del modello}
Un gioco a mosse simultanee in cui le azioni sono quantità conduce all’equilibrio di Cournot--Nash.
Ciò può essere rilevante in mercati, come l’energia, dove le imprese scelgono quantità e i prezzi sono determinati tramite meccanismi d’asta.
Si considera un modello molto semplice in cui ciascuna impresa ha solo costo variabile:
\[
TC_i(q_i)=c_i q_i,\qquad i=1,2,
\]
dove $TC_i$ è il costo totale dell’impresa $i$, $c_i$ è il costo variabile, e $q_i$ è la quantità prodotta dall’impresa $i$.
La quantità totale sul mercato è $Q=q_1+q_2$ e influenza il prezzo secondo una funzione di domanda inversa lineare:
\[
P(Q)=a-bQ,\qquad a,b>0,\qquad a\ge c_i.
\]
Questo modello implica che tutta la produzione venga venduta sul mercato.
Il profitto dell’impresa $i$ è:
\[
\pi_i(q_1;q_2)=P(q_1+q_2)q_i-TC_i(q_i)
=
\bigl[a-b(q_1+q_2)\bigr]q_i-c_iq_i,\qquad i=1,2.
\]

\paragraph{Funzioni di reazione (best response)}
Assumendo decisioni simultanee, si può determinare l’equilibrio tramite le funzioni di best response $R_i(q_j)$.
La condizione di stazionarietà per l’impresa 1 è:
\[
\frac{\partial \pi_1(q_1;q_2)}{\partial q_1}
=
a-2bq_1-bq_2-c_1=0
\;\Rightarrow\;
R_1(q_2)=\frac{a-c_1}{2b}-\frac{1}{2}q_2. \tag{7}
\]
Analogamente, per l’impresa 2:
\[
R_2(q_1)=\frac{a-c_2}{2b}-\frac{1}{2}q_1. \tag{8}
\]

\paragraph{Sistema per l’equilibrio di Cournot}
Per trovare l’equilibrio si individua l’intersezione delle due funzioni di risposta, cioè si risolve:
\[
\begin{cases}
q_1^c = R_1(q_2^c)\\
q_2^c = R_2(q_1^c)
\end{cases}
\]
(dove il superscritto $c$ denota l’equilibrio di Cournot; le funzioni di risposta sono rette a pendenza negativa).
Si risolve quindi il sistema lineare:
\[
\begin{cases}
q_1^c = \dfrac{a-c_1}{2b}-\dfrac{1}{2}q_2^c\\[6pt]
q_2^c = \dfrac{a-c_2}{2b}-\dfrac{1}{2}q_1^c
\end{cases}
\]
ottenendo:
\[
q_1^c=\frac{a-2c_1+c_2}{3b},
\qquad
q_2^c=\frac{a-2c_2+c_1}{3b}. \tag{9}
\]

\paragraph{Prezzo di equilibrio e profitti}
Il prezzo di equilibrio risulta:
\[
p^c=\frac{a+c_1+c_2}{3}. \tag{10}
\]
Il profitto di ciascuna impresa è:
\[
\pi_i^c=(p^c-c_i)q_i^c
=
\left(\frac{a+c_i+c_j}{3}-c_i\right)
\left(\frac{a-2c_i+c_j}{3b}\right)
=
\frac{(a-2c_i+c_j)^2}{9b}
=
b\,(q_i^c)^2. \tag{11}
\]
Se un’impresa riduce il proprio costo, aumenta sia la quantità prodotta sia il profitto.
Se le imprese hanno la stessa tecnologia ($c_1=c_2$), si ottiene una soluzione simmetrica $q_1^c=q_2^c$, come atteso.

\subsection{Competizione di Bertrand}

\paragraph{Impostazione del problema}
Si considerino due imprese che competono sui prezzi per un bene omogeneo, sotto l’ipotesi semplice di domanda e costi lineari.
È necessario specificare la reazione dei consumatori ai prezzi.
Con prodotto omogeneo, è sensato assumere che i consumatori acquistino dal venditore più economico; se i prezzi sono uguali, si assume che il mercato sia diviso equamente tra le due imprese.
Non si considerano vincoli di capacità.

\paragraph{Quantità venduta in funzione dei prezzi}
La quantità venduta dall’impresa $i$ (con $i=1,2$ e $j=2,1$ per indicare il concorrente) è:
\[
q_i=
\begin{cases}
0 & \text{se } p_i \ge \alpha/\beta,\\
0 & \text{se } p_i > p_j,\\
\dfrac{\alpha-\beta p}{2} & \text{se } p_i=p_j \equiv p < \alpha/\beta,\\
\alpha-\beta p_i & \text{se } p_i < \min\{p_j,\alpha/\beta\}.
\end{cases} \tag{12}
\]
L’idea è che non ci sia domanda se il prezzo supera il prezzo limite $\alpha/\beta$ oppure se è strettamente maggiore di quello del concorrente.
Se i prezzi coincidono (e sono sotto il limite) la domanda viene divisa a metà; altrimenti l’impresa più economica cattura tutta la domanda.

\paragraph{Definizione di equilibrio di Bertrand--Nash}
Un equilibrio di Bertrand--Nash è una quadrupla di prezzi e quantità,
\[
(p_1^b,\;p_2^b,\;q_1^b,\;q_2^b),
\]
tale che:
\begin{enumerate}[label=\arabic*.]
    \item $p_1^b$ risolve $\max_{p_1}\ \pi_1(p_1;p_2^b)=(p_1-c_1)q_1$, per $p_2=p_2^b$;
    \item $p_2^b$ risolve $\max_{p_2}\ \pi_2(p_1^b;p_2)=(p_2-c_2)q_2$, per $p_1=p_1^b$;
    \item le quantità $q_1$ e $q_2$ sono date dall’equazione (12).
\end{enumerate}
Chiaramente, i prezzi non saranno fissati al di sotto dei costi marginali $c_1$ e $c_2$, rispettivamente.

\paragraph{Undercutting e (assenza di) equilibrio con costi diversi}
Il concetto chiave è l’undercutting.
Se $c_1<c_2$, l’impresa 1 può undercut l’impresa 2 fissando un prezzo strettamente minore di $p_2$.
In senso stretto non esiste equilibrio, poiché non c’è un massimo (ma solo un supremo).
In effetti, c’è una discontinuità (a differenza di Cournot): una diminuzione infinitesima del prezzo può generare un salto della domanda per un’impresa.

\paragraph{Equilibrio con costi marginali uguali}
Si può mostrare che esiste equilibrio quando il costo marginale è lo stesso $c$ per entrambe le imprese:
\[
p_1=p_2=c,
\qquad
q_1=q_2=\frac{\alpha-\beta c}{2}.
\]
Il risultato si dimostra facilmente per assurdo: per qualunque altra configurazione dei prezzi, un’impresa avrebbe incentivo a deviare.

\paragraph{Conseguenze e discreto passo di prezzo}
Il risultato netto è che può non esserci profitto per nessuna impresa.
Se i costi marginali sono diversi, per trovare un massimo (e un equilibrio) è necessario che i prezzi possano variare solo di un ammontare minimo $\varepsilon$ (ad esempio un centesimo di euro).
Se $c_2-c_1>\varepsilon$, allora l’impresa 1 può undercut impostando $p_1=p_2-\varepsilon$, e l’equilibrio risultante è:
\[
p_2=c_2,\qquad p_1=c_2-\varepsilon,\qquad
q_1=\alpha-\beta(c_2-\varepsilon),\qquad q_2=0.
\]

\paragraph{Osservazioni finali}
Lasciando da parte le tecnicalità, questa previsione è in parte in contrasto con i risultati empirici.
Nella pratica, la struttura dei costi non è semplicemente lineare, esistono vincoli di capacità e i beni non sono realmente omogenei.
Possono emergere ulteriori complicazioni, per cui non è detto che esista una sola impresa sul mercato.
Guardando il risultato in modo opposto, esso suggerisce che le imprese dovrebbero differenziare le proprie offerte e sfruttare l’eterogeneità dei consumatori.

\subsection{Parte 2 -- Competizione di prezzo con beni non omogenei: modello di Hotelling}

\paragraph{Beni differenziati e scelta del consumatore}
Per trattare beni non omogenei (differenziati), è necessario un modello di scelta del consumatore.
Si consideri un modello unidimensionale delle preferenze dei consumatori.
I beni prodotti da due imprese, $A$ e $B$, sono caratterizzati da una feature, misurata su una scala da $0$ a $L$.
Per semplicità, si trascurano i costi di produzione.
L’impresa $A$ produce un bene con feature al livello $a$, e l’impresa $B$ produce un bene con feature al livello $L-b$, a distanza $b$ dal livello massimo.
Questa rappresentazione geometrica è nota come \emph{Hotelling's linear street model}.
Ogni consumatore ha un livello preferito per la feature, rappresentato dalla sua posizione sulla ``strada''.
Si assuma che i consumatori siano uniformemente distribuiti sulla strada, dal livello $0$ al livello $L$.
Ogni consumatore ha una marca preferita, quella più vicina ai propri gusti.
Se i due prezzi $p_A$ e $p_B$ sono uguali, ciascun consumatore acquista semplicemente la marca più vicina.
Tuttavia, esiste un trade-off tra prezzo e soddisfazione.

\paragraph{Costo di trasporto e utilità}
Si immagini di esprimere la (dis)utilità di ciascun consumatore misurando un costo di ``trasporto'' $\tau$.
Per un consumatore in posizione $x$ sulla scala, l’utilità è:
\[
u(x)=
\begin{cases}
-p_A-\tau |x-a| & \text{se sceglie la marca } A,\\
-p_B-\tau |x-(L-b)| & \text{se sceglie la marca } B.
\end{cases}
\]
Si cerca un consumatore critico, in posizione $x_i$, tale che $a<x_i<L-b$, indifferente tra le due marche:
\[
-p_A-\tau(x_i-a)=-p_B-\tau(L-b-x_i),
\]
da cui segue:
\[
x_i=\frac{p_B-p_A}{2\tau}+\frac{L-b+a}{2}. \tag{13}
\]
Come controllo di coerenza, quando i prezzi sono uguali, $x_i$ è il punto medio tra le due posizioni delle marche.
L’impresa $A$ cattura tutta la domanda nell’intervallo $[0,x_i)$, quindi la sua domanda è $x_i$, mentre la domanda per la marca $B$ è:
\[
L-x_i=\frac{p_A-p_B}{2\tau}+\frac{L+b-a}{2}.
\]
Ci si può chiedere se esista un equilibrio di Bertrand--Nash nei prezzi, a posizioni fissate.

\paragraph{Problemi di massimizzazione dei profitti}
L’impresa $A$, dato $p_B$, risolve:
\[
\max_{p_A}\ \pi_A
=
\left(\frac{p_Bp_A-p_A^2}{2\tau}+\frac{L-b+a}{2}\right)p_A,
\]
mentre l’impresa $B$, dato $p_A$, risolve:
\[
\max_{p_B}\ \pi_B
=
\left(\frac{p_Ap_B-p_B^2}{2\tau}+\frac{L+b-a}{2}\right)p_B.
\]

\paragraph{Condizioni del primo ordine ed equilibrio nei prezzi}
Le condizioni di stazionarietà del primo ordine sono:
\[
\frac{\partial \pi_A}{\partial p_A}
=
\frac{p_B-2p_A}{2\tau}+\frac{L-b+a}{2}=0,
\qquad
\frac{\partial \pi_B}{\partial p_B}
=
\frac{p_A-2p_B}{2\tau}+\frac{L+b-a}{2}=0,
\]
che forniscono i prezzi di equilibrio:
\[
p_A^e=\frac{\tau(3L-b+a)}{3},
\qquad
p_B^e=\frac{\tau(3L+b-a)}{3}.
\]

\paragraph{Profitto dell’impresa A}
Il profitto per l’impresa $A$ è:
\[
\pi_A^e=x_i^e\,p_A^e=\frac{\tau(3L-b+a)^2}{18}.
\]

\paragraph{Caso simmetrico}
Nel caso simmetrico $a=b$ (che non implica che le posizioni coincidano), i prezzi sono uguali e il profitto si riduce a:
\[
\frac{\tau L^2}{2}
\]
per entrambe le imprese.
Si osserva che, maggiore è il costo di trasporto, maggiori sono i prezzi.
Ciò vale anche in contesti con switching costs.

\paragraph{Esistenza di prezzi di equilibrio positivi}
Non si deve dare per scontato che esista una coppia di prezzi di equilibrio strettamente positivi.
Si può mostrare che:
\begin{enumerate}[label=\arabic*.]
    \item se le marche sono omogenee (localizzate nello stesso punto), allora $p_A^e=p_B^e=0$ a causa dell’undercutting;
    \item se le marche non sono troppo vicine, allora valgono gli equilibri precedenti; altrimenti, non esiste equilibrio (si veda la discussione successiva);
    \item non esiste equilibrio nel gioco in cui sia le posizioni sia i prezzi sono variabili decisionali.
\end{enumerate}

\subsection{Esistenza di equilibrio nel modello di Hotelling}

\paragraph{Caso simmetrico e condizione su $a$}
Si consideri il caso simmetrico $a=b$.
Dalla discussione precedente si dovrebbe avere:
\[
p_A^e=p_B^e=\tau L,
\qquad
\pi_A^e=\pi_B^e=\frac{\tau L^2}{2}.
\]
In realtà, si può dimostrare che ciò vale solo per $a\le L/4$.

\paragraph{Prezzo limite per catturare tutto il mercato}
Si assuma $p_B=\tau L$, il prezzo di equilibrio, e si cambi $p_A$.
L’impresa $A$ cattura tutto il mercato se anche il consumatore in $L-b$ sceglie $A$.
Ciò accade se il prezzo $p_A$ più il costo di trasporto è minore di $p_B=\tau L$.
Osservando che la distanza tra $A$ e $B$ è $L-2a$, $A$ cattura il mercato se:
\[
p_A<\tau L-\tau(L-2a)=2a\tau=p_{A,\lim}.
\]
In questo caso, il profitto per $A$ è una funzione lineare crescente di $p_A$ e il suo valore limite è:
\[
\pi_A^{\triangle}=p_{A,\lim}L=2a\tau L.
\]
Per il prezzo limite c’è un’ambiguità, poiché i consumatori a destra di $B$ sono indifferenti; si può assumere che quel segmento di mercato sia diviso in parti uguali.
In ogni caso, esiste una discontinuità nel profitto di $A$ in funzione del prezzo.

\paragraph{Profitto quando il mercato è diviso}
Per $p_A>p_{A,\lim}$, il mercato si divide e, dalla (13), si ottiene:
\[
\pi_A=
\left[\frac{p_B-p_A}{2\tau}+\frac{L-b+a}{2}\right]p_A
=
\left[\frac{\tau L-p_A}{2\tau}+\frac{L}{2}\right]p_A
=
p_A L-\frac{p_A^2}{2\tau}.
\]
All’equilibrio, quando $p_A=\tau L$, si ottiene:
\[
\pi_A^e=\frac{\tau L^2}{2}.
\]
Poi, quando $p_A$ diventa abbastanza grande, l’impresa $B$ cattura tutto il mercato e $\pi_A$ scende a 0.

\paragraph{Condizione di esistenza dell’equilibrio}
Esiste un equilibrio (cioè esiste un massimo e non solo un supremo) se:
\[
\pi_A^e \ge \pi_A^{\triangle},
\]
ossia:
\[
\frac{\tau L^2}{2}\ge 2a\tau L
\;\Rightarrow\;
a\le \frac{L}{4}.
\]
Ciò significa che le imprese devono essere sufficientemente distanti per garantire l’esistenza di un equilibrio.

\subsection{Giochi simultanei vs. giochi sequenziali}

\paragraph{Motivazione dei giochi sequenziali}
Finora si è assunto che le due imprese competano simultaneamente e debbano prendere decisioni simili.
Talvolta è più naturale assumere che uno dei due giocatori muova per primo.
Ad esempio, un produttore può dover decidere il prezzo all’ingrosso, e un retailer la quantità acquistata (assumendo prezzo di vendita fissato).

\paragraph{Da Cournot a von Stackelberg}
Si consideri quindi che cosa accade nel gioco sulle quantità se si assume che l’impresa 1 (leader) scelga la quantità $q_1$ prima dell’impresa 2 (follower).
Diversamente dal gioco simultaneo, l’impresa 2 conosce la decisione di $1$ prima di decidere; quindi ha informazione perfetta.
L’analisi del gioco sequenziale porta all’equilibrio di von Stackelberg.

\paragraph{Problema del leader}
L’impresa 1 prende la decisione conoscendo la best response dell’impresa 2, data dall’equazione (8).
Il problema del leader è:
\[
\max_{q_1}\ \pi_1^s
=
P\bigl(q_1+R_2(q_1)\bigr)q_1-c_1q_1
=
\left[
a-b\left(
q_1+\frac{a-c_2}{2b}-\frac{q_1}{2}
\right)
\right]q_1-c_1q_1,
\]
dove il superscritto $s$ si riferisce alla competizione di von Stackelberg.

\paragraph{Quantità ottima del leader}
Applicando la condizione di stazionarietà si ottiene:
\[
q_1^s=\frac{a-2c_1+c_2}{2b}
=\frac{3}{2}\,q_1^c.
\]
L’impresa 1 produce di più nel gioco sequenziale rispetto a Cournot.

\paragraph{Quantità del follower}
Sostituendo in $R_2(q_1)$ si ottiene:
\[
q_2^s
=
\frac{a-c_2}{2b}-\frac{a-2c_2+c_1}{4b}
=
\frac{a-3c_2+2c_1}{4b}
=
\frac{3}{4}q_2^c+\frac{c_1-c_2}{4b}. \tag{15}
\]
La produzione dell’impresa 2 è una frazione di quella del gioco di Cournot, più un termine positivo se l’impresa 1 è meno efficiente dell’impresa 2.

\paragraph{Confronto dei profitti}
È interessante confrontare i profitti delle due imprese in questo gioco.
Ciò è semplice quando i costi marginali sono uguali; l’idea è illustrata con un esempio numerico.

\subsection{Esempio}

\paragraph{Dati}
Due imprese hanno lo stesso costo marginale, $c_1=c_2=5$, e il mercato è caratterizzato dalla funzione prezzo/quantità:
\[
P(Q)=120-Q.
\]
Si confrontano tre casi:
\begin{enumerate}[label=\arabic*.]
    \item le due imprese colludono e operano come un cartello (o come due rami di un monopolista);
    \item le imprese non cooperano e muovono simultaneamente (gioco di Cournot);
    \item le imprese non cooperano e muovono sequenzialmente (gioco di von Stackelberg).
\end{enumerate}

\paragraph{Caso 1: monopolio/cartello}
Nel primo caso si lavora con l’output aggregato $Q$. Il monopolista risolve:
\[
\max\ \pi^m=(120-Q)Q-5Q.
\]
Applicando la condizione di stazionarietà:
\[
120-2Q-5=0 \;\Rightarrow\; Q^m=57.50.
\]
Il prezzo e il profitto risultano:
\[
p^m=120-57.5=62.50,
\qquad
\pi_{1+2}^m=(62.50-5)\cdot 57.50=3306.25.
\]

\paragraph{Caso 2: Cournot (simultaneo)}
Nel secondo caso, la soluzione (9) è simmetrica:
\[
q_1^c=q_2^c=\frac{120-10+5}{3}=38.33.
\]
La produzione totale e il prezzo sono:
\[
Q^c=2\cdot 38.33=76.77,
\qquad
p^c=120-76.77=43.33.
\]
Il profitto di ciascuna impresa è:
\[
\pi_1^c=\pi_2^c=(q_1^c)^2=1469.19.
\]
Il profitto complessivo è:
\[
\pi_{1+2}^c=2\cdot 1469.19=2938.89 < 3306.25=\pi_{1+2}^m.
\]
Il monopolista restringe l’output per aumentare il prezzo, ottenendo un profitto totale maggiore rispetto alla competizione di Cournot.
La collusione produce quindi un profitto maggiore della competizione.

\paragraph{Caso 3: von Stackelberg (sequenziale)}
Nel gioco sequenziale, usando (14) e (15), si ottiene:
\[
q_1^s=\frac{120-10+5}{2}=57.5,
\qquad
q_2^s=\frac{120-10+5}{4}=28.75.
\]
Rispetto al gioco simultaneo, l’output dell’impresa 1 aumenta mentre quello dell’impresa 2 diminuisce.
La produzione totale e il prezzo sono:
\[
Q^s=57.5+28.75=86.25,
\qquad
p^s=120-86.25=33.75.
\]
Il prezzo è inferiore rispetto ai due casi precedenti e la distribuzione del profitto è asimmetrica:
\[
\pi_1^s=(33.75-5)\cdot 57.5=1653.13,
\]
\[
\pi_2^s=(33.75-5)\cdot 28.75=826.56,
\]
\[
\pi_{1+2}^s=1653.13+826.56=2479.69.
\]
Il profitto totale del gioco sequenziale è inferiore a quello del gioco simultaneo; tuttavia, il leader ha un vantaggio netto e il suo profitto è maggiore nel gioco sequenziale.

\subsection{Parte 2 -- Conviene sempre muovere per primi?}

\paragraph{Vantaggio del leader e limiti}
L’esempio giocattolo precedente mostra che il privilegio di muovere per primi può dare un vantaggio al leader.
Data la struttura del gioco, è facile vedere che il leader nel gioco sequenziale non può fare peggio che nel gioco simultaneo; infatti, potrebbe comunque produrre la stessa quantità del gioco di Cournot.
Tuttavia, ciò non vale in generale.
In particolare, quando esistono asimmetrie informative o elementi casuali, la scelta del leader, o il suo esito in presenza di incertezza, può fornire al follower informazione utile.
L’esempio seguente mostra che essere i primi a muovere non è sempre desiderabile.

\paragraph{Esempio: battle of the sexes (Juliet muove per prima)}
Si consideri il battle of the sexes, assumendo che Juliet abbia il privilegio di muovere per prima.
\[
\begin{array}{c|cc}
 & \text{Romeo: Horror} & \text{Romeo: Shopping} \\
\hline
\text{Juliet: Horror} & (1,3) & (0,0) \\
\text{Juliet: Shopping} & (0,0) & (3,1)
\end{array}
\]
Qualunque sia la scelta di Juliet, Romeo sceglierà la mossa che gli consente di godere della sua compagnia.
Di conseguenza, Juliet sceglierà certamente \emph{shopping} ed è certamente felice di muovere per prima.

\paragraph{Un caso diverso: Morticia}
La situazione è molto diversa per i payoff seguenti:
\[
\begin{array}{c|cc}
 & \text{Romeo: Cinema} & \text{Romeo: Restaurant} \\
\hline
\text{Morticia: Cinema} & (5,-100) & (0,1) \\
\text{Morticia: Restaurant} & (0,1) & (5,-100)
\end{array}
\]
In questo caso, Romeo è indifferente tra andare al cinema o al ristorante.
Ciò che teme davvero è una serata con Morticia.
È facile vedere che questo gioco non ha un equilibrio di Nash, poiché uno dei due giocatori ha sempre incentivo a deviare.
Un equilibrio può essere trovato ammettendo strategie miste, in cui i giocatori selezionano un’azione secondo una distribuzione di probabilità, legata all’incertezza sulla mossa del concorrente.
Qui non si considerano strategie miste, ma il punto importante è che, in questo caso, nessun giocatore vorrebbe muovere per primo.

\paragraph{Interpretazione economica dei due giochi}
Si è notato che la prima versione del battle of the sexes è un gioco di coordinamento stilizzato per due imprese che devono adottare uno standard comune; nella seconda versione, un’impresa vuole adottare lo stesso standard del concorrente, mentre l’altra impresa vorrebbe selezionarne uno differente.

\subsection{Pricing e double marginalization}

\paragraph{Impostazione B2B: produttore e retailer}
Come applicazione interessante al pricing, si consideri un contesto B2B, in cui un produttore e un retailer interagiscono tramite prezzi (si veda Tirole, 2003, Capitolo 4).
È interessante confrontare decisioni di prezzo e profitti complessivi della supply chain in due contesti:
\begin{enumerate}[label=\arabic*.]
    \item impresa verticalmente integrata, che gestisce produzione e distribuzione (un solo decisore prende tutte le decisioni);
    \item schema decentralizzato, in cui il produttore decide il prezzo all’ingrosso e il retailer decide il prezzo di mercato.
\end{enumerate}
In entrambi i casi si considera un mercato con funzione di domanda lineare:
\[
d(p)=1-p,
\]
dove $p$ è il prezzo di mercato, e si assume una struttura di costo lineare con costo marginale $c<1$.

\paragraph{Caso 1: impresa verticalmente integrata}
Il problema complessivo dell’impresa integrata è:
\[
\max_{p}\ (p-c)\,(1-p).
\]
Risolvendo, si ottengono prezzo ottimo, domanda e profitto:
\[
p_{vi}^*=\frac{1+c}{2},
\qquad
d_{vi}^*=\frac{1-c}{2},
\qquad
\pi_{vi}^*=\frac{(1-c)^2}{4},
\]
rispettivamente.

\paragraph{Caso 2: supply chain decentralizzata}
Se la catena non è integrata, ciascun attore imposta un prezzo.
Si assuma che il produttore sia il leader in un gioco sequenziale e fissi un prezzo all’ingrosso $p_w$.
Serve la funzione di risposta del retailer, che imposta il prezzo di mercato $p_m$ risolvendo, dato $p_w$:
\[
\max_{p_m}\ (p_m-p_w)\,(1-p_m).
\]
Questo produce la best response:
\[
R_m(p_w)=\frac{1+p_w}{2}.
\]

\paragraph{Problema del produttore e prezzo all’ingrosso di equilibrio}
Il problema del produttore diventa:
\[
\max_{p_w}\ (p_w-c)\left[1-\frac{1+p_w}{2}\right],
\]
che fornisce il prezzo di equilibrio:
\[
p_{w;dec}^*=\frac{1+c}{2}.
\]

\paragraph{Prezzo retail e domanda nel caso decentralizzato}
Il prezzo retail è maggiore che nel caso integrato (si ricordi $c<1$):
\[
p_{m;dec}^*=\frac{3+c}{4},
\]
e la domanda di mercato è più piccola:
\[
d_{dec}^*=\frac{1-c}{4}.
\]

\paragraph{Profitto complessivo nel caso decentralizzato}
Il profitto complessivo del controllo decentralizzato è la somma dei due profitti:
\[
\pi_{dec}^*=(p_{w;dec}^*-c)\,d_{dec}^*+(p_{m;dec}^*-p_{w;dec}^*)\,d_{dec}^*
=
\frac{(1-c)^2}{8}+\frac{(1-c)^2}{16}
=
\frac{3(1-c)^2}{16}
<
\pi_{vi}^*.
\]

\paragraph{Double marginalization e coordinamento della supply chain}
Questo tipo di problema è noto come \textbf{double marginalization} ed è dovuto al fatto che entrambi i giocatori applicano un markup al costo marginale che osservano, e tale costo percepito cresce lungo la catena.
In un contesto più realistico, si dovrebbe considerare anche l’incertezza della domanda, che introduce temi di risk sharing.
In tal caso, politiche di prezzo (two-part tariffs e buyback contracts) possono essere utilizzate per superare il problema e coordinare la supply chain.

\subsection{Parte 3 -- Modelli di scelta discreta}

\subsection{Conjoint analysis}

\paragraph{Idea di base e utilizzi}
La \emph{conjoint analysis} è un metodo ampiamente utilizzato nel marketing (e in altri campi) per progettare prodotti e servizi, valutare l’impatto del prezzo e costruire simulatori di mercato.
L’idea di base è che i consumatori reagiscono ad alternative rappresentate da combinazioni di valori degli attributi.
Esistono alcune varianti della conjoint analysis, ma le principali sono:
\begin{itemize}[label=-]
    \item \emph{ratings-based conjoint analysis} (quella tradizionale);
    \item \emph{choice-based conjoint analysis} (la più comune).
\end{itemize}

\paragraph{Modello a rating}
Il rating di preferenza $Y_i$ per l’alternativa $i$ può essere modellato come:
\[
Y_i=\sum_{k=1}^{m} U_k(x_{ik})+\varepsilon_i,
\]
dove ciascuna funzione $U_k$ cattura la \emph{partworth} per l’attributo $k$, ed $\varepsilon_i$ è un termine di errore.
Si noti che chiedere ai consumatori di dare priorità agli attributi è tipicamente inutile (``tutto è importante!''). Chiedere un rating $Y_i$ può essere una scelta migliore.

\paragraph{Limiti e interazioni}
La conjoint analysis classica inferisce le partworth sulla base dei rating forniti dai consumatori per un insieme di alternative (stimoli, profili, ecc.).
Il carattere additivo del modello è un chiaro limite, ma esistono modi per introdurre interazioni tra attributi, che possono essere rilevanti per valori specifici (ad esempio ``rosso'' e ``Ferrari'').

\paragraph{Dati di scelta: revealed vs. stated preferences}
Tuttavia, assegnare rating non è il modo in cui i consumatori prendono decisioni.
Per costruire un modello di preferenza migliore, si devono raccogliere dati da scelte effettive tra un insieme di alternative (inclusa l’opzione di non acquistare).
Si possono avere:
\begin{itemize}[label=-]
    \item \emph{revealed preference data}, dove si osservano scelte reali di mercato;
    \item \emph{stated preference data}, dove si analizza la reazione di un panel di consumatori a un insieme di stimoli accuratamente progettati (alternative ipotetiche).
\end{itemize}
La \emph{choice-based conjoint analysis} si basa su stated preferences, poiché offre grande flessibilità nella generazione degli stimoli e nella raccolta dati (al costo di una possibile perdita di realismo).

\paragraph{Design degli esperimenti e stima dei modelli}
Un ruolo chiave è svolto da un’attenta progettazione degli esperimenti (design ortogonali, ecc.), per massimizzare l’informazione raccolta senza imporre un carico cognitivo eccessivo all’utente (troppe alternative per scelta e troppe scelte).
Oltre alle questioni classiche di design of experiments, è importante:
\begin{itemize}[label=-]
    \item scegliere (ed eventualmente combinare) attributi e livelli;
    \item considerare proibizioni (combinazioni prive di senso);
    \item evitare profili incoerenti (prestazioni migliori a prezzo più basso);
    \item gestire con attenzione variabili ordinali.
\end{itemize}
Un modello tipico stimabile tramite CBC è un modello \emph{multinomial logit} (MNL).
Esistono versioni sofisticate di CBC, basate su modelli Bayesiani gerarchici, per tenere conto dell’eterogeneità dei consumatori.
Valutando le partworth utilities, si può prevedere la market share di beni non attualmente offerti, così come l’impatto delle decisioni di prezzo.

\subsection{Microfondazioni dei modelli MNL: utilità casuali}

\paragraph{Utilità rappresentativa e componente casuale}
Si assuma che il consumatore scelga uno tra $n$ beni, in base all’utilità di ciascuna alternativa.
L’utilità del bene $i\in[n]$ è $V_i+\varepsilon_i$, ossia somma di una componente deterministica e una componente casuale.
La componente deterministica è spesso detta \emph{representative utility}.
L’alternativa $i$ è scelta se:
\[
V_i+\varepsilon_i > V_j+\varepsilon_j,\qquad \forall j\neq i.
\]
Quindi, la probabilità di scelta dell’alternativa $i$ è:
\[
\pi_i=\mathbb{P}\{V_i+\varepsilon_i>V_j+\varepsilon_j,\ \forall j\neq i\}
=
\mathbb{P}\{\varepsilon_j<\varepsilon_i+V_i-V_j,\ \forall j\neq i\}.
\]

\paragraph{Probabilità condizionata e integrazione}
Assumendo indipendenza tra le componenti casuali, condizionatamente a $\varepsilon_i$, si ha:
\[
\pi_i\mid \varepsilon_i=\prod_{j\neq i} F_j(\varepsilon_i+V_i-V_j),
\]
dove $F_j$ è la DF di $\varepsilon_j$.
Quindi:
\[
\pi_i=\int_{-\infty}^{+\infty}\prod_{j\neq i} F_j(\varepsilon_i+V_i-V_j)\, f_i(s)\,ds,
\]
dove $f_i$ è la PDF di $\varepsilon_i$.

\paragraph{Caso i.i.d. Gumbel}
Si assuma ora che tutte le componenti casuali siano i.i.d. e seguano una distribuzione di Gumbel, con DF e PDF:
\[
F(x)=\exp\!\left(-e^{-x}\right),\qquad
f(x)=e^{-x}\exp\!\left(-e^{-x}\right).
\]
Allora:
\[
\pi_i=
\int_{-\infty}^{+\infty}
\prod_{j\neq i}
\exp\!\left(-e^{-(s+V_i-V_j)}\right)\;
e^{-s}\exp\!\left(-e^{-s}\right)\,ds.
\]
Per calcolare l’integrale, si osserva che $s+V_i-V_j=s$ per $j=i$, quindi si può includere il fattore $\exp(-e^{-s})$ nel prodotto e raccogliere i termini che non dipendono da $j$:
\[
\pi_i=
\int_{-\infty}^{+\infty}
\prod_{j=1}^{n}
\exp\!\left(-e^{-(s+V_i-V_j)}\right)\; e^{-s}\,ds
=
\int_{-\infty}^{+\infty}
\exp\!\left(-\sum_{j=1}^{n} e^{-(s+V_i-V_j)}\right)\; e^{-s}\,ds
\]
\[
=
\int_{-\infty}^{+\infty}
\exp\!\left(-e^{-s}\sum_{j=1}^{n} e^{-(V_i-V_j)}\right)\; e^{-s}\,ds.
\]
Si applica ora un cambio di variabile $t=e^{-s}$, così che $-e^{-s}ds=dt$ e i limiti inferiore e superiore diventano rispettivamente $+\infty$ e $0$.

\paragraph{Derivazione del modello MNL}
Correggendo per il cambio di segno:
\[
\pi_i=
\int_{0}^{1}
\exp\!\left(-t\sum_{j=1}^{n} e^{-(V_i-V_j)}\right)\,dt
=
\left.
\frac{-\exp\!\left(-t\sum_{j} e^{-(V_i-V_j)}\right)}
{\sum_{j} e^{-(V_i-V_j)}}
\right|_{0}^{1}
=
\frac{1}{\sum_{j} e^{-(V_i-V_j)}}
=
\frac{e^{V_i}}{\sum_{j} e^{V_j}},
\]
che corrisponde al modello MNL.

\paragraph{Assunzione IIA}
Il modello MNL è comune nel machine learning e nella classificazione ed è relativamente semplice da gestire in modelli di ottimizzazione (ad esempio decisioni di assortimento ottimo).
Tuttavia, ha limitazioni definite, in particolare l’assunzione IIA (\emph{Independence from Irrelevant Alternatives}).
Si considerino gli odds relativi di scegliere gli item $i$ e $k$:
\[
\frac{\pi_i}{\pi_k}
=
\frac{e^{V_i}/\sum_{j}e^{V_j}}{e^{V_k}/\sum_{j}e^{V_j}}
=
e^{V_i-V_k},
\]
che non dipendono da nessun’altra alternativa.
Quindi gli odds relativi non cambiano quando nuove alternative vengono aggiunte al choice set.

\paragraph{Esempio (red--blue bus problem)}
Si consideri una decisione sul modo di viaggio tra auto e bus (un bus blu), assumendo utilità rappresentative uguali, quindi $\pi_c=\pi_{bb}=1/2$.
Si introduca ora un bus rosso, per cui è ragionevole assumere $\pi_{rb}=\pi_{bb}$.
Con un modello MNL si ottiene $\pi_c=\pi_{rb}=\pi_{bb}=1/3$, mentre sarebbe più ragionevole assumere una suddivisione della probabilità di scegliere l’autobus, quindi $\pi_c=1/2$ e $\pi_{rb}=\pi_{bb}=1/4$.
I modelli \emph{nested logit} sono un possibile approccio per aggirare l’IIA, quando necessario.
Se si assume che la componente casuale dell’utilità sia normalmente distribuita, si ottiene un modello \emph{probit}.

\subsection{Supplemento tecnico -- EVT e massimi a blocchi}

\paragraph{Distribuzione dei massimi a blocchi}
Nella \emph{Extreme Value Theory} (EVT), tra le altre cose, si studia la distribuzione dei massimi a blocchi.
Si consideri un campione i.i.d. di taglia $n$ da una distribuzione con funzione di distribuzione $F_X(x)$.
Qual è la distribuzione di $M_n=\max\{X_1,\dots,X_n\}$? Che cosa accade quando $n\to\infty$?
Per indipendenza:
\[
F_{\max}(x)=\mathbb{P}\{M_n\le x\}
=
\mathbb{P}\{X_1\le x,\dots,X_n\le x\}
=
\mathbb{P}\{X_1\le x\}\cdots \mathbb{P}\{X_n\le x\}
=
F_X(x)^n.
\]

\paragraph{Esempio: uniforme}
Per $U\sim U(0,1)$:
\[
F_U(x)=x \ \Rightarrow\ F_{\max}(x)=x^n,\qquad x\in[0,1].
\]
Quando $n\to\infty$, la distribuzione collassa in una distribuzione degenere con massa di probabilità concentrata in $x=1$.
Inoltre, in questo caso:
\[
\mathbb{E}[M_n]=\frac{n}{n+1}.
\]
Un caso degenere come questo, dovuto al supporto limitato, non è particolarmente interessante.
Se si considera una variabile casuale con supporto illimitato verso l’alto, la distribuzione si sposta verso $+\infty$. Quindi?

\paragraph{Normalizzazione: confronto con il CLT}
Nel caso della somma $S_n=\sum_{k\in[n]}X_k$, rilevante per il teorema del limite centrale, si normalizza definendo sequenze $a_n$ e $b_n$:
\[
\lim_{n\to\infty}\mathbb{P}\left\{\frac{S_n-a_n}{b_n}\le x\right\}=\Phi(x),
\qquad
a_n=n\mathbb{E}(X_1),
\qquad
b_n=\sqrt{n\operatorname{Var}(X_1)}.
\]

\paragraph{Normalizzazione per i massimi}
Per ottenere una distribuzione limite per i massimi a blocchi, si introducono sequenze normalizzanti $d_n$ e $c_n$ e si verifica se:
\[
\lim_{n\to\infty}\mathbb{P}\left\{\frac{M_n-d_n}{c_n}\le x\right\}
=
\lim_{n\to\infty}F_X^n(c_nx+d_n)
=
H(x), \tag{16}
\]
per una funzione di distribuzione non degenere $H(x)$.

\paragraph{Esempio: esponenziale}
Si consideri una distribuzione esponenziale con rate $\beta>0$, per cui:
\[
F(x)=1-e^{-\beta x},\qquad x\ge 0.
\]
Scegliendo $c_n=1/\beta$ e $d_n=(\log n)/\beta$ si ottiene:
\[
F_X^n(c_nx+d_n)=\left(1-\frac{1}{n}e^{-x}\right)^n,\qquad x\ge -\log n,
\]
e:
\[
\lim_{n\to\infty}F_X^n(c_nx+d_n)=\exp\!\left(-e^{-x}\right).
\]

\paragraph{Definizione: distribuzione GEV}
La funzione di distribuzione della \emph{generalized extreme value} (GEV) standard è:
\[
H_\xi(x)=
\begin{cases}
\exp\!\left(-(1+\xi x)^{-1/\xi}\right), & \xi\neq 0,\\[4pt]
\exp\!\left(-e^{-x}\right), & \xi=0,
\end{cases}
\]
dove $1+\xi x>0$.
Introducendo un parametro di location $\mu\in\mathbb{R}$ e un parametro di scala $\sigma>0$, si ottiene la famiglia:
\[
H_{\xi;\mu;\sigma}(x):=
H_\xi\!\left(\frac{x-\mu}{\sigma}\right).
\]
Il parametro $\xi$ definisce la forma di una famiglia di distribuzioni simili (dello stesso tipo).
Si ricorda che quando due variabili casuali $V$ e $W$ differiscono in distribuzione per un parametro di scala $a>0$ e un parametro di location $b\in\mathbb{R}$, cioè $V \overset{d}{=} aW+b$, allora hanno lo stesso tipo (sono simili).

\paragraph{Interpretazione del parametro di forma}
La figura mostra densità per alcuni valori del parametro di forma (assumendo $\mu=0$, $\sigma=1$):
\begin{itemize}[label=-]
    \item se $\xi>0$, si ha una distribuzione di Fr\'echet;
    \item se $\xi=0$, si ha una distribuzione di Gumbel;
    \item se $\xi<0$, si ha una distribuzione di Weibull.
\end{itemize}
La distribuzione di Weibull ha un estremo destro finito.
La distribuzione di Fr\'echet presenta una decrescita più lenta della Gumbel.
Nota: la distribuzione di Gumbel, tra le altre cose, svolge un ruolo chiave nei modelli di scelta discreta.

\paragraph{Dominio di attrazione e teorema FTG}
Se la condizione (16) vale per una $H$ non degenere, si dice che la funzione di distribuzione $F$ appartiene al massimo dominio di attrazione di $H$: $F\in MDA(H)$.
Nel caso della distribuzione esponenziale, si ha $F\in MDA(H_0)$.

\paragraph{Teorema (Fisher--Tippett--Gnedenko)}
Se $F\in MDA(H)$ per una $H$ non degenere, allora $H$ deve essere una distribuzione GEV di tipo $H_\xi$ per qualche valore di $\xi$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF 7 %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1cm}
\textcolor{DarkRed}{\section{CAP 7 -- Il principio della programmazione dinamica}}

\paragraph{Recap}
La \textcolor{DarkRed}{\textbf{programmazione dinamica (DP)}} è un \textbf{principio generale e flessibile} per la risoluzione di \textbf{problemi decisionali dinamici multistadio}, basato sulla \textbf{decomposizione} del problema complessivo in una sequenza di problemi più semplici a \textbf{singolo stadio}. Nei problemi stocastici, a differenza dei problemi multiperiodo deterministici, le decisioni non vengono fissate interamente all’istante iniziale, ma si adattano nel tempo alle realizzazioni osservate dei fattori di rischio, rendendo necessario un approccio \textbf{\textcolor{DarkRed}{closed-loop}} e l’uso di \textbf{strategie non anticipative}.
\\ \hspace*{1.5em}
L’applicazione della DP richiede una struttura \textbf{markoviana} del sistema, in cui lo stato \(s_t\) riassume l’informazione rilevante del passato e consente di descrivere l’evoluzione attraverso un’equazione di transizione del tipo \(s_{t+1}=g_{t+1}(s_t,x_t,\xi_{t+1})\). Le decisioni \(x_t\) vengono prese dopo aver osservato lo stato corrente, mentre gli input esogeni si realizzano nell’intervallo temporale successivo e influenzano lo stato futuro. In questo contesto, lo stato, le decisioni e i fattori di rischio possono essere discreti o continui e l’orizzonte temporale può essere finito o infinito.
\\ \hspace*{1.5em}
I problemi di programmazione dinamica sono tipicamente formulati tramite una \textbf{funzione obiettivo additiva nel tempo}, spesso con l’introduzione di un \textbf{fattore di sconto} e di un \textbf{contributo terminale}, la cui valutazione può richiedere un ragionamento \emph{ad hoc}. Quando i contributi immediati dipendono da variabili aleatorie, si lavora con valori attesi condizionati che, in pratica, possono essere stimati solo tramite campionamento o simulazione, come avviene nel \textbf{reinforcement learning}.
\\ \hspace*{1.5em}
Il \textbf{problema del cammino minimo} su una rete diretta aciclica rappresenta un esempio deterministico fondamentale per introdurre la \textbf{value function} e il principio di \textbf{programmazione dinamica all’indietro}. Associando a ciascun nodo un valore pari al costo minimo per raggiungere il nodo terminale, emerge una proprietà di \textbf{annidamento} dei sottocammini ottimi e si ottiene una ricorsione sui valori che parte dalla condizione terminale. Questo esempio chiarisce il ruolo del valore dello stato successivo nella scelta locale delle decisioni.
\\ \hspace*{1.5em}
Nel \textbf{lot-sizing} dinamico a singolo prodotto, l’inventario costituisce una \textbf{variabile di stato}, la quantità ordinata una \textbf{variabile di controllo} e la domanda agisce come input esogeno deterministico o come fattore di rischio stocastico. In presenza di domanda incerta, modelli diversi derivano da ipotesi alternative sul comportamento dei clienti, distinguendo tra \emph{vendite perse} e \emph{backlog}, con effetti diretti sulla dinamica e sulla struttura dei costi.
\\ \hspace*{1.5em}
La formalizzazione generale dei problemi decisionali dinamici conduce all’\textcolor{DarkRed}{\textbf{equazione di Bellman}}, che esprime una condizione di ottimalità ricorsiva bilanciando \textbf{contributo immediato} e \textbf{valore atteso dello stato successivo}. Da essa si ricava una \textbf{politica ottima in feedback} della forma \(x_t^\star=\mu_t^\star(s_t)\). Nei problemi a orizzonte finito la soluzione si ottiene naturalmente tramite una procedura \textbf{backward}, mentre nei problemi a orizzonte infinito la value function è caratterizzata come \textbf{punto fisso} di un opportuno operatore.

\paragraph{Che cos'è la programmazione dinamica?}
La \textcolor{DarkRed}{\textbf{programmazione dinamica (Dynamic Programming, DP)}} non è un algoritmo come l’algoritmo del simplesso per la programmazione lineare. È piuttosto un \textbf{principio notevolmente generale e flessibile} che può essere utilizzato per progettare una vasta gamma di algoritmi di ottimizzazione.
\\ \hspace*{1.5em}
L’\textbf{idea centrale} si basa sulla \textbf{decomposizione di un problema decisionale dinamico multistadio} in una \textbf{sequenza di problemi più semplici a singolo stadio}.

\paragraph{Problemi multistadio e multiperiodo}
Un problema decisionale multistadio non deve essere confuso con un problema \emph{multiperiodo}. Si consideri
un problema a orizzonte finito in cui si devono selezionare $T$ decisioni $x_t$, applicate in una sequenza di
istanti temporali $t = 0,1,2,\ldots,T-1$. La domanda fondamentale è: \emph{quando vengono prese queste decisioni}?
\\ \hspace*{1.5em}
In un \textcolor{DarkRed}{problema statico multiperiodo}, tutte le decisioni vengono prese al tempo $t=0$,
come è naturale in un problema deterministico (\textcolor{DarkRed}{\textbf{approccio open-loop}}). In un \textcolor{DarkRed}{problema stocastico},
invece, si osserva nel tempo una traiettoria campionaria dei fattori di rischio rilevanti e si adattano le decisioni
lungo il percorso.
\\ \hspace*{1.5em}
Ciò che serve è una \textbf{strategia}, ovvero una \textbf{regola per prendere decisioni dopo aver osservato le
realizzazioni casuali} (\textbf{\textcolor{DarkRed}{approccio closed-loop}}).

\paragraph{Ambito di applicazione della programmazione dinamica}
La programmazione dinamica non è un approccio universale, poiché la sua applicazione richiede una struttura specifica
nel modello del sistema, che deve essere \textbf{markoviana}.
\\ \hspace*{1.5em}Nonostante ciò, la programmazione dinamica può essere applicata a una vasta gamma di problemi
\begin{itemize}[label=-]
\item \textbf{Problemi continui e discreti}, dove la natura discreta o continua può riguardare:
variabili di stato, variabili decisionali e rappresentazione del tempo.
\item \textbf{Problemi deterministici e stocastici}.
\item \textbf{Problemi a orizzonte finito e infinito}.
\end{itemize}
\hspace*{1.5em}
All’interno di questa ampia collezione di problemi, può essere necessario risolvere problemi a \emph{dimensione finita o
infinita}. Per alcune strutture, la \textcolor{DarkRed}{programmazione dinamica} rappresenta \textbf{l’unico approccio risolutivo praticabile}.

\paragraph{Problemi decisionali dinamici}
Si considerano modelli a tempo discreto ed è necessario chiarire la distinzione tra
\textbf{istanti temporali} e \textbf{intervalli di tempo}.
\\ \hspace*{1.5em}
Le \emph{decisioni} vengono prese agli \emph{istanti} temporali, ma possono essere \emph{implementate} lungo un \emph{intervallo} di tempo,
non necessariamente in modo istantaneo. Inoltre, le decisioni devono essere prese prima di osservare la realizzazione
dei fattori di rischio, che si manifestano nell’intervallo temporale successivo.
\\ \hspace*{1.5em}
Si adotta la seguente \textbf{convenzione}
\begin{itemize}[label=-]
\item Gli \textcolor{DarkRed}{\textbf{istanti temporali}} sono indicizzati da $t = 0,1,2,\ldots$; in tali istanti si osserva lo stato del sistema e si prende una decisione.
\item L’\textcolor{DarkRed}{\textbf{intervallo di tempo}} $t$ è l’intervallo tra gli istanti $t-1$ e $t$; dopo aver applicato la decisione presa all’istante $t-1$, il sistema evolve durante l’intervallo successivo e raggiunge un nuovo stato all’istante $t$.
Durante tale intervallo può realizzarsi un input esterno, eventualmente casuale, che influenza la transizione di stato.
\end{itemize}

\paragraph{Orizzonte finito e infinito}
Un problema può avere un \textbf{orizzonte finito}, definito sugli istanti $t=0,1,\ldots,T$, oppure un
\textbf{orizzonte infinito}, definito sugli istanti $t=0,1,\ldots$. Il primo istante temporale è $t=0$,
mentre il primo intervallo temporale è indicizzato da $t=1$. In un problema a orizzonte finito, vi
sono $T$ intervalli di tempo e $T+1$ istanti temporali.
\\ \hspace*{1.5em}
La \textcolor{DarkRed}{dinamica del sistema} è rappresentata da un’\textcolor{DarkRed}{\textbf{equazione di transizione di stato}} del tipo
\[
s_{t+1} = g_{t+1}(s_t, x_t, \xi_{t+1}),
\]
dove
\begin{itemize}[label=-]
\item $g_{t+1}$ è la \textbf{funzione di transizione} sul periodo $t+1$.
\item $s_t$ è il \textbf{vettore delle variabili di stato} all’istante $t$, che riassumono tutta l’informazione rilevante del passato;
$s_0$ è lo \textbf{stato iniziale}, tipicamente noto, mentre $s_T$ è lo stato terminale.
\item $x_t$ è il \textbf{vettore delle variabili decisionali} all’istante $t$, dette anche variabili di controllo.
\item $\xi_{t+1}$ è un \textbf{fattore esogeno} che si realizza durante l’intervallo $t+1$.
\end{itemize}
\hspace*{1.5em}Le decisioni $x_t$ vengono prese all’istante di tempo $t$, dopo aver osservato lo stato $s_t$; lo stato successivo $s_{t+1}$ dipende dalla realizzazione della variabile aleatoria $\xi_{t+1}$.

\paragraph{Variabili di stato e processi}
Si distinguono diverse tipologie di \textbf{variabili di stato}
\begin{itemize}[label=-]
\item \textcolor{DarkRed}{\textbf{Variabili di stato fisiche}}, che descrivono risorse fisiche o finanziarie e sono direttamente influenzate dalle decisioni.
\item \textcolor{DarkRed}{\textbf{Variabili di stato informative}}, come il prezzo di un’azione finanziaria, non influenzate dall’azione di un singolo decisore.
\item \textcolor{DarkRed}{\textbf{Variabili di stato di credenza}}, rilevanti in problemi con incertezza sui parametri del modello.
\end{itemize}
\hspace*{1.5em}Si definiscono:
il \textbf{processo dei dati} $(\xi_t)_{t \ge 1}$,
il \textbf{processo decisionale} $(x_t)_{t \ge 0}$ e 
il \textbf{processo di stato} $(s_t)_{t \ge 0}$,
che possono essere deterministici o stocastici.

\paragraph{Informazione e non anticipatività}
Nei problemi decisionali dinamici sotto incertezza, due aspetti sono cruciali
\begin{enumerate}
\item Come modellare la \emph{disponibilità dell’informazione}.
\item Come sono \emph{correlati i processi} dei dati, dello stato e delle decisioni.
\end{enumerate}
\hspace*{1.5em}
Si introduce la notazione
\(
x_{[t]} := (x_0, x_1, \ldots, x_t), \; \xi_{[t]} := (\xi_1, \xi_2, \ldots, \xi_t),
\)
utile per rappresentare la storia osservata fino all'istante di tempo $t$.
\\ \hspace*{1.5em}
La decisione $x_t$ può dipendere solo dalle realizzazioni dei fattori di rischio fino all’istante $t$ incluso. Una politica decisionale implementabile è detta \textbf{non anticipativa}.

\paragraph{Dipendenza temporale dei fattori di rischio}
I vettori casuali del processo dei dati possono presentare diversi livelli di dipendenza:
\emph{indipendenza interstadio}, \emph{dipendenza markoviana} e \emph{dipendenza dall’intera storia osservata}.
\\ \hspace*{1.5em}
È inoltre necessario specificare se la distribuzione dei fattori di rischio dipende dal tempo o dallo stato e
dalle decisioni.
\\ \hspace*{1.5em}
L’\textbf{obiettivo} è \textcolor{DarkRed}{\textbf{ottimizzare una funzione obiettivo additiva nel tempo}},
pari alla somma dei costi o dei ricavi generati in ciascun istante.

\subsection{Problemi a orizzonte finito scontati}
\paragraph{Definizione}Un problema stocastico a orizzonte finito $T$ può essere formulato
\[
\operatorname{opt} \ \mathbb{E}_0 \left[
\sum_{t=0}^{T-1} \gamma^t f_t(s_t,x_t) + \gamma^T F_T(s_T)
\right],
\]
dove $\operatorname{opt}$ indica una minimizzazione o massimizzazione. La notazione $\mathbb{E}_0[\cdot]$ viene 
utilizzata per indicare che l’aspettativa è valutata all’istante di tempo $t = 0$; si tratta quindi di
un’aspettativa incondizionata, poiché non è stata ancora osservata alcuna realizzazione dei fattori di
rischio (ad eccezione di quelle che hanno condotto allo stato iniziale $s_0$).
\\ \hspace*{1.5em}La \textcolor{DarkRed}{\textbf{funzione obiettivo}} include
\begin{itemize}[label=-]
\item Il \textbf{contributo immediato} $f_t(s_t,x_t)$ in cui incorriamo prendendo la decisione $x_t$ quando siamo nello stato $s_t$.
\item Il \textbf{contributo terminale} $F_T(s_T)$, che dipende solamente dallo stato terminale $s_T$.
\end{itemize}

\paragraph{Fattore di sconto}
Il fattore di sconto $\gamma \in (0,1)$ è tipico nelle applicazioni finanziarie, anche se non è strettamente necessario
in un problema a orizzonte finito perchè la funzione obiettivo è ben definita. Individuare un modo appropriato per associare un valore allo stato terminale può richiedere un ragionamento \emph{ad hoc}.


\paragraph{Contributi stocastici e apprendimento}
Il contributo immediato può dipendere dalla realizzazione del fattore di rischio
\(
h_t(s_t,x_t,\xi_{t+1}).
\)
In tal caso, si può definire un'attesa condizionata
\[
f_t(s_t,x_t) = \mathbb{E}_t \left[ h_t(s_t,x_t,\xi_{t+1}) \right].
\]
In pratica, il calcolo dell’attesa può essere difficile o impossibile. In tali casi, si
dispone solo di campioni osservabili tramite simulazione Monte Carlo o sperimentazione online,
come avviene nel \textbf{reinforcement learning}.

\subsection{Problemi a orizzonte infinito}
\paragraph{Problemi scontati a orizzonte infinito}
Un \textbf{problema di programmazione dinamica scontato a orizzonte infinito} ha la forma
\[
\operatorname{opt}\ \mathbb{E}_0 \left[
\sum_{t=0}^{\infty} \gamma^t f(s_t,x_t)
\right],
\]
dove la sommatoria è ben definita se tutti i contributi sono positivi e limitati e si utilizza un fattore di sconto appropriato $\gamma < 1$.
\\ \hspace*{1.5em}
La programmazione dinamica scontata è naturale nelle applicazioni di business ed economia. Talvolta il fattore di sconto non ha una reale giustificazione economica ed è utilizzato solo come espediente per rendere trattabile un problema a orizzonte infinito.
\\ \hspace*{1.5em}
In alcune applicazioni ingegneristiche si preferisce considerare una media nel tempo
\[
\operatorname{opt}\ \lim_{T \to \infty} \mathbb{E}_0 \left[
\frac{1}{T} \sum_{t=0}^{T-1} f(s_t,x_t)
\right].
\]
Questo conduce a formulazioni di programmazione dinamica basate sul contributo medio per stadio.
\\ \hspace*{1.5em}
Esistono anche problemi con orizzonte non definito, nei quali il processo decisionale termina quando viene
raggiunto uno stato obiettivo.

\paragraph{Politiche decisionali}
Ad ogni istante temporale occorre selezionare una decisione $x_t$. Si può introdurre un vincolo astratto del tipo:
\(
x_t \in X(s_t),
\)
che impone che la decisione al tempo $t$ appartenga a un insieme ammissibile $X$, eventualmente dipendente dallo stato corrente $s_t$.

\paragraph{Caso deterministico e stocastico}
In un contesto \textcolor{DarkRed}{\textbf{deterministico}}, questo vincolo sarebbe sufficiente,
poiché al tempo $t=0$ si dovrebbe individuare una sequenza di decisioni $(x_t)_{t\ge 0}$ da implementare nel tempo.
Nel caso \textcolor{DarkRed}{\textbf{stocastico}}, tuttavia, entra in gioco un vincolo fondamentale.
Le decisioni devono essere \textcolor{DarkRed}{\textbf{non anticipative}}, cioè non possono dipendere da informazioni future non ancora osservate.
\\ \hspace*{1.5em}
Una \emph{politica decisionale chiusa} e \emph{non anticipativa} è naturalmente espressa nella forma
\[
x_t = \mu_t(s_t) \in X(s_t),
\]
dove $\mu_t(\cdot)$ è una funzione che associa allo stato al tempo $t$ una decisione ammissibile.
\\ \hspace*{1.5em}
Una politica può essere vista come una \textbf{sequenza di funzioni}
\[
\mu \equiv \bigl( \mu_0(\cdot), \mu_1(\cdot), \ldots, \mu_{T-1}(\cdot) \bigr),
\]
una per ciascun istante temporale in cui è richiesta una decisione.
\\ \hspace*{1.5em}
Nel caso di problemi a \emph{orizzonte infinito}, si cercano politiche \textbf{stazionarie}, caratterizzate dalla stessa funzione $\mu(\cdot)$ in ogni stadio.

\paragraph{Formulazione del problema in termini di politiche}
Una formulazione più precisa del problema a orizzonte finito è
\[
\operatorname{opt}_{\mu \in \mathcal{M}}
\mathbb{E}_0 \left[
\sum_{t=0}^{T-1} \gamma^t f_t(s_t,\mu_t(s_t))
+ \gamma^T F_T(s_T)
\right],
\]
dove $\mathcal{M}$ è l’insieme delle politiche ammissibili, tali che $x_t=\mu_t(s_t)\in X(s_t)$ per ogni istante temporale.
\\ \hspace*{1.5em}
Sono state introdotte \textbf{politiche deterministiche}. Esistono tuttavia casi in cui politiche
randomizzate possono essere necessarie o utili.
\\ \hspace*{1.5em}
Le \textbf{politiche randomizzate} possono essere richieste per gestire vincoli probabilistici sugli stati:
\(
\mathbb{P}\{ s_t \in G \} \ge 1 - \alpha,
\)
dove $\alpha$ è una \textbf{probabilità accettabile di violazione del vincolo}.
Le politiche randomizzate risultano inoltre necessarie nel contesto del trade-off tra esplorazione e sfruttamento nel reinforcement learning.

\paragraph{Un esempio: il lot-sizing dinamico a singolo prodotto}
Si consideri il problema di lot-sizing non capacitated a singolo prodotto
\[
\begin{aligned}
\min \ & \sum_{t=0}^{T-1} \bigl[ c x_t + \phi \cdot \delta(x_t) \bigr]
      + \sum_{t=1}^{T} h I_t\\
\text{s.t.}\ & I_{t+1} = I_t + x_t - d_{t+1}, \qquad t=0,1,\ldots,T-1, \\
            & x_t \ge 0,\; I_t \ge 0.
\end{aligned}
\]
L’\textbf{obiettivo} è soddisfare la domanda al costo minimo, che comprende:
un \emph{costo di mantenimento a magazzino}, con tasso $h$ per unità e per unità di tempo e 
un \emph{costo di ordinazione}, composto da una parte variabile con coefficiente $c$ e da un costo fisso $\phi$, sostenuto solo quando si ordina una quantità positiva.
Per rappresentare il costo fisso si introduce la funzione
\[
\delta(x) =
\begin{cases}
1 & \text{se } x>0,\\
0 & \text{se } x=0.
\end{cases}
\]
\hspace*{1.5em}
Il \textbf{livello di inventario} $I_t$ è una variabile di stato, la quantità ordinata $x_t$ è una variabile di
controllo, e la domanda $d_t$ può essere interpretata come input esogeno nel caso deterministico o come fattore
di rischio nel caso stocastico.

\paragraph{Domanda incerta e modelli alternativi}
Come si dovrebbe adattare il modello nel caso in cui si consideri l’incertezza della domanda? Esistono due modelli di base, che dipendono da assunzioni specifiche sul comportamento dei clienti
\begin{enumerate}
\item Nel caso di vendite perse, l’equazione di transizione diventa:
\(
I_{t+1} = \max \{0, I_t + x_t - d_{t+1}\}.
\)
Si introduce inoltre una penalità $q$ per ogni unità di domanda insoddisfatta, aggiungendo il termine:
\(
\sum_{t=1}^{T} q \max \{0, d_t - (I_{t-1}+x_{t-1}) \}.
\)
\item
Se i clienti sono pazienti, la domanda può essere accumulata come backlog, con una penalità $b$, con $b>h$.
Una prima opzione è mantenere una singola variabile di stato $I_t$, rilassandone la non negatività e interpretando valori negativi come backlog.
Una seconda opzione è introdurre due variabili di stato:
\emph{inventario disponibile} $O_t \ge 0$ e \emph{backlog} $B_t \ge 0$.
Il costo diventa
\[
\sum_{t=1}^{T} (h O_t + b B_t),
\]
e le equazioni di transizione sono:
\(
O_{t+1} = \max \{0, O_t - B_t + x_t - d_{t+1}\}, \;
B_{t+1} = \max \{0, -O_t + B_t - x_t + d_{t+1}\}.
\)
\end{enumerate}
\paragraph{Ulteriori problematiche}
In applicazioni reali possono sorgere ulteriori difficoltà: valutazione dello \textbf{stato terminale},
stati \emph{parzialment} osservabili, domanda censurata, 
\textbf{ritardi} dovuti a lead time di consegna e \emph{incertezza} dipendente dallo stato e dalle decisioni.

\subsection{Uno sguardo al principio di programmazione dinamica: il problema del cammino minimo}

\paragraph{Rete diretta aciclica}
Si consideri un \textbf{problema deterministico}: il \textcolor{DarkRed}{cammino minimo su una rete diretta aciclica}.
Una \textbf{rete diretta} è costituita da un insieme di nodi $N=\{0,1,2,\ldots,N\}$ e da un insieme di archi $A$.
Un \textcolor{DarkRed}{\textbf{arco}} è una coppia ordinata $(i,j)\in A$, con $i,j\in N$, ed è associato a un costo $c_{ij}>0$.
Si assume che la rete sia \emph{aciclica}.
\\ \hspace*{1.5em}Per un \textcolor{DarkRed}{\textbf{nodo}} $i\in N$ si definiscono, rispettivamente, 
\[
S_i = \{ j\in N \mid (i,j)\in A \},
\qquad
B_i = \{ j\in N \mid (j,i)\in A \},
\]
il set di \textbf{successori} e \textbf{predecessori} del nodo $i$.
Un \textcolor{DarkRed}{\textbf{cammino diretto}} da $i$ a $j$ è una sequenza di nodi $(k_1,k_2,\ldots,k_m)$
con $k_1=i$, $k_m=j$, e archi consecutivi in $A$. La \textbf{lunghezza del cammino} è la somma dei costi sugli archi.
\\ \hspace*{1.5em}
Ogni \emph{nodo} è uno \emph{stato} e ogni \emph{arco} una \emph{transizione} possibile. In ogni stato occorre scegliere una transizione
in modo da raggiungere lo stato terminale al costo minimo.

\paragraph{Approccio greedy}
Una \textcolor{DarkRed}{\textbf{regola greedy}} consisterebbe nel muoversi verso il nodo più vicino
\[
\min_{j\in S_i} c_{ij}.
\]
Tale approccio non è in generale ottimale. Tuttavia, l’\textbf{idea} di risolvere una sequenza di semplici problemi a singolo
stadio presenta un certo fascino. Può quindi essere utile affinare l’\textbf{obiettivo} introducendo una misura della
bontà dello stato successivo.
Si introduce il valore $V_j$ associato a ciascun nodo successore, risolvendo in $i$ il problema:
\[
\min_{j\in S_i} (c_{ij}+V_j).
\]
\paragraph{Definizione del valore ottimo}
\emph{Come si può introdurre un termine aggiuntivo di questo tipo?} Il punto di partenza consiste nel trovare una caratterizzazione della soluzione ottima. Sia \(V_i\) la \textbf{lunghezza del cammino minimo} dal nodo \(i \in N\) al nodo terminale \(N\), indicato come \(i \xrightarrow{*} N\):
\(
V_i := L(i \xrightarrow{*} N).
\)
\\ \hspace*{1.5em}
Supponiamo inoltre che, dato un cammino ottimo da \(i\) a \(N\), un nodo \(j\) appartenga a tale cammino. Allora è immediato osservare che deve valere la seguente proprietà
\[
j \xrightarrow{*} N \ \text{è un sottocammino di} \ i \xrightarrow{*} N.
\]
Per comprenderne il motivo, consideriamo la decomposizione del cammino \(i \xrightarrow{*} N\) in due sottocammini: \(\mathcal{P}_{i \to j}\) dal nodo \(i\) al nodo \(j\), e \(\mathcal{P}_{j \to N}\) dal nodo \(j\) al nodo \(N\). La lunghezza del cammino \(i \xrightarrow{*} N\) è la somma delle lunghezze dei due sottocammini:
\(
V_i
= L\!\left(i \xrightarrow{*} N\right) 
= L(\mathcal{P}_{i \to j}) + L(\mathcal{P}_{j \to N}).
\)
\\ \hspace*{1.5em}
Si noti che il secondo sottocammino non è influenzato dal modo in cui si va da \(i\) a \(j\). Il sistema è \textbf{\textcolor{DarkRed}{markoviano}}, nel senso che il modo in cui si raggiunge il nodo \(j\) non ha alcuna influenza sulla lunghezza di qualunque cammino \emph{futuro} che parte dal nodo \(j\).
\\ \hspace*{1.5em}
L'affermazione equivale a dire che il sottocammino \(\mathcal{P}_{j \to N}\) nella decomposizione precedente è \textbf{ottimo}, nel senso che
\(
L(\mathcal{P}_{j \to N}) = L\!\left(j \xrightarrow{*} N\right).
\)
Per \textbf{dimostrarlo}, si suppone che \(\mathcal{P}_{j \to N}\) non sia un cammino ottimo da \(j\) a \(N\), cioè che
\(
L(\mathcal{P}_{j \to N}) > L\!\left(j \xrightarrow{*} N\right).
\)
Allora potremmo migliorare il lato destro della decomposizione considerando il cammino costituito dallo stesso cammino iniziale \(\mathcal{P}_{i \to j}\), seguito da un cammino ottimo \(j \xrightarrow{*} N\):
\(
L(\mathcal{P}_{i \to j}) + L\!\left(j \xrightarrow{*} N\right)
< L(\mathcal{P}_{i \to j}) + L(\mathcal{P}_{j \to N}) 
= L\!\left(i \xrightarrow{*} N\right)
= V_i,
\)
che è una contraddizione, poiché abbiamo assunto che \(V_i\) sia la lunghezza di un cammino minimo.
\\ \hspace*{1.5em}
I cammini minimi godono quindi di una sorta di \textbf{\textcolor{DarkRed}{proprietà di annidamento}}, e questo risultato suggerisce una decomposizione ricorsiva del problema complessivo di determinare il cammino minimo \(0 \xrightarrow{*} N\).
\\ \hspace*{1.5em}
Se si conoscessero i valori \(V_j\) per ciascun nodo \(j \in \mathcal{S}_0\), si potrebbe effettuare la prima decisione risolvendo il problema a singolo stadio
\[
V_0 = \min_{j \in \mathcal{S}_0} \bigl( c_{0j} + V_j \bigr).
\]

\paragraph{Ricorsione di Bellman}
L’ultima equazione può essere applicata a qualunque nodo, e non soltanto a quello iniziale. Si ottiene così una \textbf{programmazione dinamica all’indietro}, che parte dalla \textbf{condizione terminale} \(V_N = 0\) e procede risolvendo un’\textbf{equazione funzionale ricorsiva}
\[
V_i = \min_{j\in S_i} \bigl( c_{ij} + V_j \bigr), \qquad \forall i\in N,
\]
con condizione terminale \(V_N=0\).
L’equazione viene risolta etichettando i nodi a partire da quello terminale, seguendo un \textbf{ordinamento topologico}.
L’equazione precedente definisce una \textbf{value function} \(V(\cdot):N\to\mathbb{R}\) che associa a ciascuno stato il suo valore ottimo.

\paragraph{Cammini minimi su reti strutturate}
Il problema del cammino minimo considerato nella sezione precedente riguarda una rete non strutturata. Un caso più strutturato si verifica quando i nodi della rete corrispondono a stati di un sistema dinamico che evolve nel tempo.
Questo tipo di grafo corrisponde a un \textbf{problema decisionale sequenziale deterministico} con un \textbf{insieme finito di stati}, che può derivare da una discretizzazione di uno spazio degli stati continuo.
\\ \hspace*{1.5em}Se lo stato terminale \(s_T\) è fissato, non ha senso associare un valore terminale ad esso. Se lo stato terminale è libero, può invece avere senso associare un contributo terminale \(F_T(s_T)\). In tal caso si aggiunge uno strato temporale al grafo e si introduce un nodo terminale fittizio \(\Omega\).

\subsection{Il principio di decomposizione della programmazione dinamica -- Equazione di Bellman}
\paragraph{Decomposizione in problemi a singolo stadio}
Il problema del cammino minimo suggerisce che un problema decisionale multistadio può essere decomposto in una sequenza di problemi più semplici a singolo stadio.
\\ \hspace*{1.5em}In un \textcolor{DarkRed}{\textbf{problema deterministico}}, si cerca una sequenza di vettori \((x_0,x_1,\ldots,x_{T-1})\) per ottimizzare una misura di prestazione
\(
\operatorname{opt}\ H(x_0,x_1,\ldots,x_{T-1}; s_0,s_1,\ldots,s_T),
\)
soggetta a un insieme di vincoli su stati e decisioni.
\\ \hspace*{1.5em}Nel \textcolor{DarkRed}{\textbf{caso stocastico}}, il problema è
\(
\operatorname{opt}\ \mathbb{E}\!\left[
H(x_0,x_1,\ldots,x_{T-1}; s_0,s_1,\ldots,s_T)
\right],
\)
dove l’attesa è presa rispetto a una sequenza di variabili aleatorie \((\xi_1,\ldots,\xi_T)\).
Questa notazione nasconde la vera natura multistadio del problema, poiché ora si dovrebbero trovare una sequenza di funzioni.
\\ \hspace*{1.5em}Infatti, a parte la decisione iniziale \(x_0\) da prendere \emph{qui e ora}, i
vettori decisionali possono essere funzioni delle decisioni passate e delle realizzazioni osservate dei fattori di rischio
\[
x_t=\mu_t\!\bigl(x_{[t-1]},\xi_{[t]}\bigr),\qquad t=1,\ldots,T-1.
\]
\hspace*{1.5em}
Se si individua un insieme adatto di \textbf{variabili di stato}, si può semplificare la dipendenza dalla storia e cercare funzioni più semplici che mappano lo stato corrente nella decisione ottima, cioè \(x_t=\mu_t(s_t)\).

\paragraph{Additività della funzione obiettivo}
Oltre alla struttura \textbf{\textcolor{DarkRed}{markoviana}} della dinamica, una decomposizione \emph{pulita} è possibile se si assume che la funzione obiettivo abbia forma additiva
\[
\mathbb{E}_0\left[
\sum_{t=0}^{T-1}\gamma^t f_t(s_t,x_t)+\gamma^T F_T(s_T)
\right].
\]
Una \textbf{regola decisionale rapida} e approssimativa è: \emph{quando si è nello stato \(s_t\), risolvere il problema miopico}
\(
\operatorname{opt}_{x_t\in X(s_t)} f_t(s_t,x_t),
\)
dove \(X(s_t)\) è l’insieme delle decisioni ammissibili nello stato \(s_t\).
\\ \hspace*{1.5em}
È già noto che tale approccio \textbf{\textcolor{DarkRed}{greedy}} non è atteso funzionare bene in generale: occorre bilanciare obiettivi di breve e lungo periodo introducendo una \textbf{value function} \(V_t(\cdot)\).
\\ \hspace*{1.5em}
L’\textbf{idea fondamentale} della programmazione dinamica è che si può trovare una value function tale da ottenere la prestazione ottima. Il valore \(V_t(s)\) dovrebbe essere il costo/ricavo atteso ottenuto applicando una politica ottima dal tempo \(t\) in avanti, partendo dallo stato \(s\).

\paragraph{Problema a singolo stadio parametrizzato dallo stato}
Formalmente, nello stato \(s_t\) al tempo \(t\) si deve risolvere
\[
\begin{aligned}
V_t(s_t)=\operatorname{opt}_{x_t\in X(s_t)}
\Bigl\{
f_t(s_t,x_t)
+ \gamma\,\mathbb{E}\!\bigl[
V_{t+1}\!\bigl(g_{t+1}(s_t,x_t,\xi_{t+1})\bigr)\,\bigm|\, s_t,x_t
\bigr]
\Bigr\}.
\end{aligned}
\]
Questa equazione funzionale ricorsiva è detta \textbf{\textcolor{DarkRed}{equazione di Bellman}}.
\\ \hspace*{1.5em}
La decisione ottima \(x_t^\star\) si ottiene risolvendo un problema di ottimizzazione parametrizzato dallo stato corrente \(s_t\), usando la conoscenza della value function \(V_{t+1}(\cdot)\).
Nel caso di uno \emph{spazio degli stati finito} e piccolo, la politica ottima può essere rappresentata in
\textbf{forma tabellare}.
In generale, la politica è implicita nella sequenza delle funzioni valore. In ogni caso, concettualmente si trova una 
politica ottima in forma di feedback:
\(
x_t^\star=\mu_t^\star(s_t)\in X(s_t).
\)
Raccogliendo tutte le funzioni \(\mu_t^\star(\cdot)\) si ottiene la politica complessiva:
\(
\mu^\star\equiv\bigl(\mu_0^\star(\cdot),\mu_1^\star(\cdot),\ldots,\mu_{T-1}^\star(\cdot)\bigr).
\)
\\ \hspace*{1.5em}
Quando si considera una singola funzione \(\mu^\star(\cdot)\) indipendente dal tempo si parla di \textcolor{DarkRed}{\textbf{politica stazionaria}},
adatta ai problemi a orizzonte infinito. Talvolta ci si accontenta di una politica subottima,
ad esempio ottenuta da un’approssimazione della value function ottima.

\paragraph{Teorema: principio di ottimalità della programmazione dinamica}
Si consideri una politica ottima \(\bigl(\mu_0^\star(\cdot),\mu_1^\star(\cdot),\ldots,\mu_{T-1}^\star(\cdot)\bigr)\)
per il \textbf{problema multistadio}
\[
\operatorname{opt}\ \mathbb{E}_0\left[
\sum_{t=0}^{T-1}\gamma^t f_t(s_t,x_t)+\gamma^T F_T(s_T)
\right].
\]
Si assuma che al tempo \(\tau\) ci si trovi nello stato \(s_\tau\) e si consideri il problema di coda
\[
\operatorname{opt}\ \mathbb{E}_\tau\left[
\sum_{t=\tau}^{T-1}\gamma^{t-\tau} f_t(s_t,x_t)+\gamma^{T-\tau}F_T(s_T)
\right].
\]
Allora la politica troncata \(\bigl(\mu_\tau^\star(\cdot),\mu_{\tau+1}^\star(\cdot),\ldots,\mu_{T-1}^\star(\cdot)\bigr)\) è ottima per il problema di coda.
\\ \hspace*{1.5em}
Le \textbf{dimostrazioni} si basano sull’induzione matematica e possono essere piuttosto complicate
quando si considerano questioni matematiche sottili.

\subsection{DP stocastica per orizzonti finiti}

\paragraph{Risoluzione all’indietro}
L’\textbf{equazione di Bellman} è un’\emph{equazione di ottimalità} e richiede di determinare la value function per ogni
istante temporale. Il processo naturale \textcolor{DarkRed}{procede all’indietro nel tempo}, partendo dalla condizione terminale:
\(
V_T(s_T)=F_T(s_T)\ \ \forall s_T.
\)
\\ \hspace*{1.5em}
All’ultimo istante decisionale, \(t=T-1\), si risolve per ogni possibile stato \(s_{T-1}\)
\[
\begin{aligned}
V_{T-1}(s_{T-1})=\operatorname{opt}_{x_{T-1}\in X(s_{T-1})}
\Bigl\{
f_{T-1}(s_{T-1},x_{T-1})
+ \gamma\,\mathbb{E}\!\bigl[
V_T\!\bigl(g_T(s_{T-1},x_{T-1},\xi_T)\bigr)\,\bigm|\, s_{T-1},x_{T-1}
\bigr]
\Bigr\}.
\end{aligned}
\]
Questo è un \emph{problema statico ma non miopico}, poiché \(V_T(\cdot)\) incorpora l’effetto della decisione \(x_{T-1}\) sullo stato terminale.
\\ \hspace*{1.5em}Risolvendo il problema precedente per ogni stato \(s_{T-1}\) si costruisce la value function \(V_{T-1}(\cdot)\). Poi, svolgendo la ricorsione all’indietro nel tempo, si ottengono \(V_{T-2}(s_{T-2})\) e così via, fino a \(V_1(s_1)\).
Infine, dato lo stato iniziale \(s_0\), la prima decisione ottima si trova risolvendo
\[
\begin{aligned}
V_0(s_0)=\operatorname{opt}_{x_0\in X(s_0)}
\Bigl\{
f_0(s_0,x_0)
+ \gamma\,\mathbb{E}\!\bigl[
V_1\!\bigl(g_1(s_0,x_0,\xi_1)\bigr)\,\bigm|\, s_0,x_0
\bigr]
\Bigr\}.
\end{aligned}
\]

\paragraph{Uso delle funzioni valore: caso deterministico}
\emph{Come sfruttare la conoscenza delle funzioni valore \(V_t(\cdot)\)?} In un contesto \textbf{deterministico}, si può trovare la sequenza di decisioni ottime \(x_t^\star\) risolvendo una sequenza di problemi a singolo stadio e aggiornando lo stato secondo le decisioni applicate.
\\ \hspace*{1.5em}
In un contesto \textbf{stocastico}, si può eseguire una simulazione \textbf{\emph{Monte Carlo}} come segue. Dato lo stato iniziale \(s_0\) e la value function \(V_1(\cdot)\), si risolve il problema del primo stadio e si trova \(x_0^\star\); quindi si campiona \(\xi_1\) e si genera lo stato successivo \(s_1=g_1(s_0,x_0^\star,\xi_1)\). Ripetendo lo stesso ragionamento, dato lo stato \(s_t\) e la value function \(V_{t+1}(\cdot)\), si risolve il problema del tempo \(t\) ottenendo \(x_t^\star\), e si prosegue fino a generare l’ultima decisione \(x_{T-1}^\star\) e lo stato terminale \(s_T\).

\paragraph{DP stocastica per orizzonti infiniti}
La forma ricorsiva deve essere adattata per un problema a orizzonte infinito del tipo
\[
\operatorname{opt}\ \mathbb{E}\left[
\sum_{t=0}^{\infty}\gamma^t f(s_t,x_t)
\right],
\]
dove si assume che i costi immediati siano limitati e \(\gamma<1\), così che la serie converga a un valore finito.
\\ \hspace*{1.5em}
Si elimina l’indice \(t\) dal contributo immediato e dalla funzione di transizione:
\(
s_{t+1}=g(s_t,x_t,\xi_{t+1}).
\)
\\ \hspace*{1.5em}
L’equazione funzionale diventa
\[
\begin{aligned}
V(s)=\operatorname{opt}_{x\in X(s)}
\Bigl\{
f(s,x)+\gamma\,\mathbb{E}\!\left[
V\bigl(g(s,x,\xi)\bigr)
\right]
\Bigr\},
\end{aligned}
\]
dove \(X(s)\) è l’insieme delle decisioni ammissibili nello stato \(s\).
\\ \hspace*{1.5em}
Ora la value function è definita come \textbf{\textcolor{DarkRed}{punto fisso}} di un operatore potenzialmente complicato,
poiché \(V(s)\) compare su entrambi i lati dell’equazione. È necessario un \emph{metodo iterativo} per risolvere tale equazione.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF 8 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1cm}
\textcolor{DarkGoldenrod}{
\section{CAP 8 -- Implementazione della programmazione dinamica}
}

\paragraph{Recap}
L’\textcolor{DarkGoldenrod}{\textbf{allocazione di risorse tramite programmazione dinamica}} viene affrontata in diversi contesti, mostrando come il principio DP possa essere implementato operativamente in problemi discreti, continui e stocastici. L’attenzione è posta sulla costruzione e sull’uso della \textbf{value function} come strumento computazionale, mettendo in evidenza il ruolo della variabile di stato e delle decisioni di controllo.
\\ \hspace*{1.5em}
Nel contesto di \textcolor{DarkGoldenrod}{\textbf{allocazione discreta}}, il problema dello \textbf{zaino} fornisce un modello elementare ma rappresentativo. Un problema statico di selezione binaria viene riformulato come processo sequenziale introducendo uno stadio fittizio associato agli oggetti, con il \textbf{budget residuo} come stato. In questo caso la value function è definita su un insieme finito e può essere \textbf{tabulata}, portando a una ricorsione di Bellman semplice e a un’implementazione numerica diretta.
\\ \hspace*{1.5em}
L’\textcolor{DarkGoldenrod}{\textbf{allocazione continua del budget}} estende il problema a decisioni reali e a funzioni di profitto \textbf{crescente e concave}. In presenza di soluzioni interne, il problema ammette una caratterizzazione analitica tramite condizioni di ottimalità. Tuttavia, la stessa struttura può essere descritta in termini di programmazione dinamica, con una value function definita su uno spazio continuo.
\\ \hspace*{1.5em}
Nel caso continuo, la value function è un oggetto \textbf{infinito-dimensionale} e non può essere memorizzata esplicitamente. La risoluzione numerica richiede quindi \textbf{discretizzazione dello stato} e tecniche di \textbf{approssimazione}. L’uso di \textbf{spline cubiche} consente di interpolare i valori fuori griglia evitando le oscillazioni tipiche dell’interpolazione polinomiale globale e preservando, almeno approssimativamente, proprietà qualitative della funzione valore.
\\ \hspace*{1.5em}
La programmazione dinamica viene applicata anche al \textcolor{DarkGoldenrod}{\textbf{controllo stocastico delle scorte}}, in cui la domanda è modellata come variabile aleatoria discreta. In questo contesto il costo immediato dipende dalla realizzazione futura del fattore di rischio e la ricorsione DP include un termine in \textbf{aspettativa}. La modellazione richiede la specificazione della distribuzione della domanda, dello stato iniziale e dell’orizzonte temporale.
\\ \hspace*{1.5em}
Nel \textcolor{DarkGoldenrod}{\textbf{caso deterministico}}, emergono proprietà strutturali forti che restringono l’insieme delle decisioni rilevanti. In particolare, la condizione di Wagner--Whitin consente di riformulare il problema di lot-sizing come un \textbf{cammino minimo} su una rete di dimensioni contenute, riducendo drasticamente la complessità computazionale.
\\ \hspace*{1.5em}
In ambiente \textcolor{DarkGoldenrod}{\textbf{stocastico}}, tali proprietà non valgono più in generale, ma sotto ipotesi di convessità emergono politiche strutturate. In assenza di costi fissi si ottengono politiche di tipo \textbf{base-stock}, mentre l’introduzione di costi fissi conduce a politiche \textbf{\((s,S)\)}, caratterizzate da soglie di riordino e livelli obiettivo.
\\ \hspace*{1.5em}
L’insieme di questi modelli evidenzia anche i \textcolor{DarkGoldenrod}{\textbf{limiti della programmazione dinamica}}, legati alla crescita dello spazio degli stati, alla difficoltà dei sottoproblemi di ottimizzazione, al costo del calcolo delle aspettative e alla complessità della modellazione delle transizioni quando il sistema dipende dalle decisioni di controllo.

\subsection{Allocazione discreta di risorse: il problema dello zaino}

\paragraph{Definizione, formulazione e motivazione DP}
Il \textcolor{DarkGoldenrod}{problema dello \textbf{zaino}} richiede di selezionare un sottoinsieme di oggetti di \textbf{valore totale massimo}, rispettando un vincolo di \textbf{capacità} (budget).  
\\ \hspace*{1.5em}
Per modellare la selezione di ciascun oggetto si introducono variabili decisionali \textbf{binarie}
\[
x_k =
\begin{cases}
1 & \text{se l’oggetto } k \text{ è selezionato},\\
0 & \text{altrimenti}.
\end{cases}
\]
\\ \hspace*{1.5em}
Il problema può essere formulato come un problema di \textbf{programmazione lineare binaria pura}
\[
\begin{aligned}
\max \ & \sum_{k=1}^{n} v_k x_k \\
\text{s.t. } \ & \sum_{k=1}^{n} w_k x_k \le B,\\
 \ &x_k \in \{0,1\} \quad \forall k.
\end{aligned}
\]
\hspace*{1.5em}Si assume un’allocazione discreta del budget, poiché la selezione di un’attività è una decisione \textbf{tutto-o-niente}. Il problema può essere risolto con algoritmi \textbf{branch-and-cut}, ma si adotta un approccio basato sul principio della \textbf{programmazione dinamica}.

\paragraph{Riformulazione sequenziale, stato e transizione}
Il problema non è intrinsecamente dinamico, ma può essere riformulato come un problema di \textbf{allocazione sequenziale di risorse} introducendo un indice discreto fittizio \(k\).  
\\ \hspace*{1.5em}
Allo stadio iniziale si dispone del budget \(B\), mentre allo stadio finale resta da valutare solo l’ultimo oggetto dato il budget residuo. La dipendenza dalle decisioni passate è interamente riassunta dal \textbf{budget residuo}.  
La variabile di stato allo stadio \(k\) è il budget disponibile \(s_k\), mentre la decisione è la variabile binaria \(x_k\). La transizione dello stato è
\[
s_{k+1}=s_k-w_k x_k,
\qquad s_1=B.
\]

\paragraph{value function e ricorsione di Bellman}
Si definisce la value function
\[
V_k(s)=\text{profitto della selezione ottimale degli oggetti } \{k,\dots,n\} \text{ con budget residuo } s.
\]
\hspace*{1.5em}
Nel caso \textbf{discreto} è possibile tabulare tutte le funzioni valore. La \emph{ricorsione DP} è
\[
V_k(s)=
\begin{cases}
V_{k+1}(s) & 0\le s<w_k,\\
\max\{V_{k+1}(s),V_{k+1}(s-w_k)+v_k\} & w_k\le s\le B.
\end{cases}
\]
L’equazione esprime che, allo stadio \(k\), viene considerato l’oggetto \(k\).  
\textbf{Se} il suo peso \(w_k\) non è compatibile con il budget residuo \(s\), cioè se \textbf{\(s < w_k\)}, l’oggetto non può essere selezionato. In questo caso la variabile di stato rimane invariata e la value function nello stato \(s\) coincide con quella dello stadio successivo, ossia \(V_k(s)=V_{k+1}(s)\).  
\\ \hspace*{1.5em}
\textbf{Se invece \(s \ge w_k\)}, è necessario confrontare due alternative. La \emph{prima} consiste nell’includere l’oggetto \(k\) nel sottoinsieme, ottenendo una ricompensa pari al suo valore \(v_k\) e allocando in modo ottimale il budget aggiornato \(s-w_k\) agli oggetti successivi \(k+1,\dots,n\). La \emph{seconda} consiste nel non includere l’oggetto \(k\), allocando l’intero budget residuo \(s\) in modo ottimale agli oggetti \(k+1,\dots,n\).  
Poiché la decisione è binaria, il problema di ottimizzazione a singolo stadio risulta immediato.
Per l’ultimo oggetto, corrispondente allo stadio \(k=n\), la value function è definita come
\[
V_n(s)=
\begin{cases}
0 & \text{se } 0 \le s < w_n,\\
v_n & \text{se } w_n \le s \le B.
\end{cases}
\]


\paragraph{Implementazione MATLAB e complessità}
La funzione \textcolor{DarkGoldenrod}{\texttt{DPKnapsack}} memorizza la value function in una tabella indicizzata per stato e stadio e una tabella delle decisioni ottime.  
\\ \hspace*{1.5em}
La ricorsione è calcolata backward e la soluzione ottima è ricostruita forward. Assumendo dati interi, la complessità è \(O(nB)\), quindi \textbf{pseudo-polinomiale}.

\subsection{Allocazione continua del budget}
\paragraph{Formulazione e soluzione analitica}
Si considera una \textcolor{DarkGoldenrod}{versione \textbf{continua} del problema di allocazione delle risorse}, in cui un budget
complessivo \(B\) deve essere allocato tra un insieme di \(n\) attività.  
\\ \hspace*{1.5em}
L’allocazione all’attività \(k\) è rappresentata da una variabile decisionale continua \(x_k \ge 0\),
per \(k=1,\dots,n\). Il \textbf{contributo al profitto} dell’attività \(k\) dipende dall’allocazione di risorse tramite una funzione \(f_k(\cdot)\) \textbf{crescente} e \textbf{concava}.
\[
\begin{aligned}
\max \ & \sum_{k=1}^{n} f_k(x_k) \\
\text{s.t. } \ & \sum_{k=1}^{n} x_k \le B, \\
& x_k \ge 0 \quad \forall k.
\end{aligned}
\]
\hspace*{1.5em}
Se le funzioni di profitto sono \textbf{concave}, ad esempio \(f_k(x)=\sqrt{x}\) per \(k=1,\dots,n\), il problema risulta relativamente semplice da risolvere.
Poiché le funzioni sono strettamente crescenti, il budget è interamente utilizzato. Assumendo una soluzione interna, la Lagrangiana conduce a un’allocazione uniforme \(x_k^\ast=B/n\).

\paragraph{Soluzione interna e Lagrangiana}
Assumendo una \textcolor{DarkGoldenrod}{\textbf{soluzione interna}} \(x_k^\ast>0\), il problema diventa un \emph{problema
non lineare} con un solo vincolo di uguaglianza.  
Introducendo il moltiplicatore di Lagrange \(\lambda\), la \textbf{lagrangiana} è
\[
\mathcal{L}(x,\lambda)=\sum_{k=1}^{n}\sqrt{x_k}+\lambda\left(\sum_{k=1}^{n}x_k-B\right).
\]
Le condizioni del primo ordine sono:
\(
\frac{1}{2\sqrt{x_k}}+\lambda=0,\quad k=1,\dots,n, \;
\sum_{k=1}^{n}x_k-B=0,
\)
che implicano un’allocazione \textbf{uniforme}
\[
x_k^\ast=\frac{B}{n},\quad k=1,\dots,n.
\]
\hspace*{1.5em}In questo caso, la funzione obiettivo è \textbf{concava} e le condizioni di ottimalità sono \textbf{necessarie e sufficienti}.

\paragraph{Formulazione DP}
Il problema può essere riformulato in un quadro di \textcolor{DarkGoldenrod}{\textbf{programmazione dinamica}}
introducendo un \emph{indice discreto fittizio} \(k=1,\dots,n\) associato alle attività.  
Sia \(V_k(s)\) il \textbf{profitto ottimale} ottenibile allocando un budget residuo \(s\) alle attività \(\{k,k+1,\dots,n\}\).
La \textbf{transizione di stato} è
\(
s_{k+1}=s_k-x_k,
\)
con condizione iniziale \(s_1=B\).
Le \textbf{value funcitons} soddisfano
\[
V_k(s_k)=\max_{0\le x_k\le s_k}\left\{f_k(x_k)+V_{k+1}(s_k-x_k)\right\}, \;\;
V_n(s_n)=\max_{0\le x_n\le s_n} f_n(x_n)=f_n(s_n).
\]
\hspace*{1.5em}Il vincolo \(s_k\ge 0\) è garantito imponendo il vincolo sulla decisione \(0\le x_k\le s_k\), poiché la transizione è deterministica.
\\ \hspace*{1.5em}
Nel \textbf{caso continuo}, la value function \(V_k(s)\) è un oggetto in uno spazio funzionale \textbf{infinito-dimensionale}.  
Poiché la valutazione è possibile solo su un insieme finito di stati, occorre usare \textbf{approssimazione} o \textbf{interpolazione} per stimare i valori fuori dalla griglia.

\paragraph{Interpolazione con spline cubiche in MATLAB}
È noto che l’interpolazione polinomiale può soffrire di \textbf{oscillazioni inaccettabili}.  
Una funzione \textbf{concava} e \textbf{monotona crescente} può essere approssimata da una funzione \textbf{non monotona}, con effetti negativi sulle procedure di ottimizzazione.
\\ \hspace*{1.5em}Un’alternativa standard consiste nell’uso di funzioni polinomiali a tratti di ordine più basso, in cui ogni tratto è associato a un sottointervallo della griglia.  
Una scelta comune sono le \textcolor{DarkGoldenrod}{\textbf{spline cubiche}}, ottenute in MATLAB tramite \texttt{spline} (costruzione) e \texttt{ppval} (valutazione in punti arbitrari, anche fuori griglia).
\\ \hspace*{1.5em}
L’\textbf{errore} può essere significativo con griglie grossolane e migliora con griglie più fitte. Restano aperti temi come: la garanzia che una funzione monotona venga approssimata da una spline monotona, la scelta dei nodi per un buon compromesso tra costo computazionale ed errore, e la generalizzazione a dimensioni superiori.

\paragraph{Risoluzione numerica del problema continuo tramite DP}
Si usa una \textbf{spline cubica} per approssimare la value function del problema continuo.  
Si imposta una griglia uniforme su \([0,B]\), replicata per ciascuno stadio. La griglia contiene \(m+1\) valori di stato,
con \emph{passo di discretizzazione} \(\delta s=B/m\), cioè stati del tipo \(j\,\delta s\), \(j=0,1,\dots,m\).
Per ogni punto della griglia si risolve un sottoproblema del tipo
\[
V_k(s_k)=\max_{0\le x_k\le s_k}\left\{f_k(x_k)+V_{k+1}(s_k-x_k)\right\},
\]
tramite \textbf{ottimizzazione numerica}.  
I valori fuori dalla griglia sono stimati via spline cubiche. La condizione al bordo su \(V_n(\cdot)\) è banale e si arresta il calcolo a \(V_2(\cdot)\) quando si vuole risolvere il problema per un budget specifico \(B\).  
La \textbf{politica ottima} \(x_t^\ast=\mu_t^\ast(s_t)\) non è memorizzata in una tabella esplicita, ma è \textbf{implicita} nella sequenza delle funzioni valore.

\paragraph{Implementazione MATLAB: idea generale}
La funzione \textcolor{DarkGoldenrod}{\texttt{findPolicy}} serve a costruire, stadio per stadio, un’\textbf{approssimazione} delle funzioni valore tramite \textbf{spline cubiche}, usando una griglia uniforme del budget.  
Riceve il budget totale, una lista di funzioni di profitto (una per attività) e il numero di punti della griglia.  
Restituisce una lista di spline che rappresentano le funzioni valore approssimate; la prima funzione non viene usata direttamente quando l’interesse è risolvere il problema per un budget iniziale specifico.
\\ \hspace*{1.5em}
La funzione \textcolor{DarkGoldenrod}{\texttt{applyPolicy}} applica la politica ottima \textbf{in avanti nel tempo} utilizzando le funzioni valore approssimate: a ogni stadio sceglie l’allocazione che massimizza la somma tra contributo immediato e valore futuro, aggiorna il budget residuo e costruisce il vettore delle allocazioni ottime insieme al valore complessivo ottenuto.

\subsection{Controllo stocastico delle scorte}

\paragraph{Impostazione del problema}
Si considera una variazione \textcolor{DarkGoldenrod}{\textbf{stocastica del problema di lot-sizing}}, assumendo una \textbf{domanda aleatoria discreta}. In questo caso è naturale adottare una rappresentazione \textbf{tabellare} della value function.
Occorre specificare la \textbf{dinamica dello stato} in caso di stockout. Qui si assumono \textbf{vendite perse}
\[
I_{t+1}=\max\{0,\; I_t + x_t - d_{t+1}\}
\]
dove \(\big(d_t\big)_{t=1,\dots,T}\) è una sequenza di variabili i.i.d., e \(x_t\) è la quantità ordinata al tempo \(t\) e consegnata immediatamente.
\\ \hspace*{1.5em}
Al tempo \(t\) si osserva l’inventario a mano \(I_t\).  
Poi si decide l’ordine \(x_t\), che viene ricevuto immediatamente e porta la disponibilità a \(I_t+x_t\).  
Durante l’intervallo \(t+1\) si osserva la domanda aleatoria \(d_{t+1}\) e si aggiorna l’inventario secondo la dinamica sopra.

\paragraph{Spazio degli stati e azioni ammissibili}
Per tabulare la value function occorre fissare un limite superiore allo stato. Si assume un \emph{vincolo} \(I_t\le I_{\max}\), che implica:
\(
\mathcal{X}(I_t)=\{0,1,\dots, I_{\max}-I_t\}.
\)
Il \textbf{costo immediato} include un \emph{costo lineare} d’ordine \(c\,x_t\) e una \emph{penalità quadratica} sull’inventario contabile dopo la domanda del periodo successivo:
\(
\beta\,(I_t+x_t-d_{t+1})^2.
\)
L’\textbf{inventario fisico} non può essere negativo ed è determinato dalla dinamica con \(\max\{0,\cdot\}\); un \textbf{inventario contabile} negativo rappresenta domanda non soddisfatta.  
Per semplicità, il \textbf{costo terminale} è nullo:
\(
F_T(I_T)=0.
\)
\\ \hspace*{1.5em}
Si può sostenere che la \textbf{penalità complessiva} non dovrebbe essere simmetrica e che si potrebbe definire una funzione lineare a tratti con pendenze diverse. Tuttavia, questo aspetto non è realmente essenziale ai fini illustrativi.  
Ciò che risulta più rilevante è il fatto che il costo immediato dipende dalla realizzazione del fattore di rischio durante il periodo di tempo \(t+1\), successivo alla decisione \(x_t\).  
\\ \hspace*{1.5em}
Questo implica che, \emph{nella ricorsione di programmazione dinamica, non compare un termine di costo immediato deterministico della forma \(f_t(s_t,x_t)\), bensì un termine stocastico della forma \(h_t(s_t,x_t,\xi_{t+1})\).}

\paragraph{Ricorsione di programmazione dinamica}
La \textcolor{DarkGoldenrod}{\textbf{ricorsione di programmazione dinamica}} risultante è
\[
V_t(I_t)=
\min_{x_t \in \mathcal{X}(I_t)}
\mathbb{E}_{d_{t+1}}
\Big[
c\,x_t
+\beta\,(I_t+x_t-d_{t+1})^2
+V_{t+1}\!\left(\max\{0,\; I_t+x_t-d_{t+1}\}\right)
\Big],
\]
per \(t=0,1,\dots,T-1\), con \(I_t \in \{0,1,2,\dots,I_{\max}\}\).

\paragraph{Modellazione dell’incertezza}
Poiché i \textbf{fattori di rischio} costituiscono semplicemente una sequenza di variabili aleatorie discrete indipendenti
e identicamente distribuite, per modellare l’incertezza è sufficiente specificare una \emph{funzione di massa di probabilità}, ossia un vettore di probabilità \(\pi_k\) associato a ciascun possibile valore della domanda \(k=0,1,2,\dots,d_{\max}\).  
\\ \hspace*{1.5em}
È inoltre necessario specificare lo stato iniziale \(I_0\) e l’orizzonte temporale \(T\) considerato per la pianificazione.


\paragraph{Lot-sizing stocastico: idea delle procedure MATLAB}
La funzione \textcolor{DarkGoldenrod}{\texttt{MakePolicy}} serve ad apprendere le funzioni valore \textbf{a ritroso} (\textbf{backward}) e la \textbf{politica ottima} in forma tabellare, dato un limite massimo di inventario, una distribuzione discreta della domanda e i parametri economici.  
L’output è una tabella dei valori (value function per ogni stato e istante) e una tabella delle azioni (decisione ottima d’ordine per ogni stato e istante).
\\ \hspace*{1.5em}
La funzione \textcolor{DarkGoldenrod}{\texttt{SimulatePolicy}} serve a \textbf{simulare} l’applicazione della politica ottima su molti scenari di domanda generati casualmente, per ottenere una stima statistica del costo totale e confrontarla con il valore previsto dalla value function.

\subsection{Sfruttare la struttura: cammini minimi per il lot-sizing deterministico}

\paragraph{Motivazione}
Nel problema giocattolo di lot-sizing è semplice memorizzare funzioni valore e politica in una tabella, ma ciò è \textbf{inefficiente} quando i valori possibili di domanda sono molti, e diventa impossibile con spazio degli stati \textbf{continuo}.  
Talvolta è possibile semplificare drasticamente il problema sfruttando la \textbf{struttura}.
\\ \hspace*{1.5em}
Si consideri una versione deterministica con soli \textbf{costi fissi d’ordine} \(\phi\) e \textbf{costi di giacenza} \(h\), con inventario iniziale e finale pari a zero, e domanda non nulla nel primo periodo.  
In linea di principio, una \textcolor{DarkGoldenrod}{\textbf{ricorsione DP}} è
\[
V_t(I_t)=\min_{x_t \ge d_{t+1}-I_t}
\left\{
\phi\,\delta(x_t)
+h\,(I_t+x_t-d_{t+1})
+V_{t+1}(I_{t+1})
\right\},
\quad t=0,\dots,T-1
\]
con condizione:
\(
V_T(I_T)\equiv 0.
\)
Il vincolo su \(x_t\) garantisce che la domanda sia sempre soddisfatta e che lo stato non diventi negativo.

\paragraph{Teorema: proprietà di Wagner--Whitin}
Per il \textcolor{DarkGoldenrod}{\textbf{uncapacitated lot-sizing deterministico}} con \textbf{costi fissi} e \textbf{costi lineari di inventario}, esiste una soluzione ottima tale che:
\(
I_t\,x_t=0,\quad t=0,1,\dots,T-1.
\)
Il \textbf{messaggio} è che non è mai ottimale ordinare quando l’inventario è positivo: si ordina solo se l’inventario è \textbf{nullo}.

\paragraph{Bilancio globale dei flussi e nodo fittizio}
Si osserva che, considerando l’intera rete, deve valere una \emph{condizione di equilibrio globale} dei flussi,
espressa dalla relazione
\[
\sum_{t=0}^{T-1} x_t = \sum_{t=1}^{T} d_t.
\]
\\ \hspace*{1.5em}
Tale condizione di equilibrio viene rappresentata introducendo un \textbf{nodo fittizio} \(0\), il cui flusso entrante rappresenta la quantità totale ordinata sull’intero orizzonte di pianificazione.
\\ \hspace*{1.5em}
È inoltre necessario garantire che la \emph{domanda sia soddisfatta in ciascun intervallo temporale}. A tal fine si introduce un insieme di nodi corrispondenti agli istanti temporali \(t=1,\dots,T\), associati all’ultimo istante di ciascun intervallo, il che equivale ad assumere che la domanda possa essere soddisfatta alla fine dell’intervallo temporale.
\\ \hspace*{1.5em}
Il \textbf{bilancio di flusso} al nodo \(t\) corrisponde all’equazione di transizione dello stato
\(
I_t = I_{t-1} + x_{t-1} - d_t.
\)
Si assuma ora, in contrasto con il teorema precedente, che valgano contemporaneamente le condizioni \(I_{t-1}>0\) e \(x_{t-1}>0\). In tal caso, \textbf{è immediato osservare che, reindirizzando il flusso orizzontale \(I_{t-1}\) lungo l’arco di ordinazione associato a \(x_{t-1}\), è possibile ridurre il costo complessivo}.

\paragraph{Conseguenza pratica della condizione di Wagner--Whitin}
La conseguenza pratica della condizione di Wagner--Whitin è che, all’istante temporale \(t\), \textbf{è sufficiente considerare un insieme ristretto di possibili decisioni di ordinazione}. In particolare, l’ordine può coprire esattamente il fabbisogno di uno o più periodi futuri consecutivi, oppure non essere effettuato affatto.
\\ \hspace*{1.5em}
\textcolor{DarkGoldenrod}{\emph{Questo implica che la variabile decisionale può assumere solo valori corrispondenti alla somma delle domande di intervalli temporali consecutivi successivi.}}
\[
x_t \in \left\{
0,\;
d_{t+1},\;
(d_{t+1}+d_{t+2}),\;
(d_{t+1}+d_{t+2}+d_{t+3}),\;
\dots,\;
\sum_{\tau=t+1}^{T} d_{\tau}
\right\}.
\]

\paragraph{Riformulazione come problema di cammino minimo}
Sfruttando questa proprietà strutturale, il problema a singolo item può essere riformulato come un \textcolor{DarkGoldenrod}{problema di \textbf{cammino minimo}} su una rete di dimensioni contenute.
\\ \hspace*{1.5em}
Il nodo iniziale \(0\) rappresenta lo stato iniziale. Per ciascun istante temporale \(t\), esistono archi che collegano tale nodo agli istanti successivi \(t+1,\dots,T\). La soluzione del problema consiste nel determinare un cammino che collega il nodo iniziale al nodo terminale minimizzando il costo complessivo.

\paragraph{Significato economico degli archi}
Gli archi selezionati nel cammino ottimo rappresentano il numero di intervalli temporali coperti dal successivo ordine.
\\ \hspace*{1.5em}
Il costo associato a ciascun arco tiene conto sia del \textcolor{DarkGoldenrod}{\textbf{costo fisso di ordinazione}} sia del \textbf{costo di giacenza} generato dall’inventario accumulato per soddisfare le domande future.
\\ \hspace*{1.5em}
Grazie al numero limitato di nodi della rete, la risoluzione del problema di lot-sizing tramite questa formulazione come cammino minimo conduce a un algoritmo \textbf{molto efficiente}, con \textcolor{DarkGoldenrod}{complessità \textbf{polinomiale}}.

\subsection{Lot-sizing stocastico: politiche \(S\) e \((s,S)\)}

\paragraph{Backlog e penalità convessa}
La \textbf{proprietà di Wagner--Whitin} non vale nel caso stocastico, ma in alcuni casi esistono risultati strutturali.  
Si assume che non vi siano \textbf{vendite perse} e che sia consentito backlog, con costo totale che include:
\[
q(s)=h\max\{0,s\}+b\max\{0,-s\},
\]
dove \(s\) può essere positivo (inventario) o negativo (backlog), \(h\) è il costo di giacenza e \(b>h\) è il costo di backlog.  
La funzione \(q(\cdot)\) è \textbf{convessa} e tende a \(+\infty\) quando \(s\to \pm\infty\).
\emph{Trascurando i costi fissi} e includendo un costo variabile lineare unitario \(c\), l’obiettivo è \textbf{minimizzare}
\[
\mathbb{E}_0\left[
\sum_{t=0}^{T-1}
\left(
c\,x_t + q(I_t+x_t-d_{t+1})
\right)
\right].
\]
\hspace*{1.5em}
La \textbf{ricorsione DP} è
\[
V_t(I_t)=\min_{x_t\ge 0}
\left\{
c\,x_t + H(I_t+x_t) + \mathbb{E}\!\left[V_{t+1}(I_t+x_t-d_{t+1})\right]
\right\},
\]
dove
\[
H(y_t):=\mathbb{E}\!\left[q(y_t-d_{t+1})\right]
= h\,\mathbb{E}\!\left[\max\{0,y_t-d_{t+1}\}\right]
+b\,\mathbb{E}\!\left[\max\{0,d_{t+1}-y_t\}\right].
\]
Si introduce \(y_t\) come inventario disponibile dopo l’ordine (lead time nullo):
\(
y_t := I_t + x_t,
\)
con condizione terminale:
\(
V_T(I_T)=0.
\)
Si assume distribuzione della domanda costante nel tempo.
\hspace*{1.5em}La \textbf{ricorsione} si riscrive come
\[
V_t(I_t)=\min_{y_t \ge I_t} G_t(y_t) - c\,I_t,
\]
dove
\[
G_t(y_t)=c\,y_t + H(y_t) + \mathbb{E}\!\left[V_{t+1}(y_t-d_{t+1})\right].
\]

\paragraph{Convessità e livello obiettivo \(S_t\)}
Si assume (senza prova) che \(V_t(\cdot)\) e \(G_t(\cdot)\) siano \textbf{convesse} per ogni \(t\), e che \(G_t(\cdot)\to +\infty\) quando \(y\to \pm\infty\).  
Ne segue che \(G_t(\cdot)\) ha un \textcolor{DarkGoldenrod}{minimizzatore non vincolato finito}
\[
S_t = \arg\min_{y_t\in\mathbb{R}} G_t(y_t).
\]
\hspace*{1.5em}L'\textbf{esempio} illustra il posizionamento relativo dei minimi \textbf{non vincolati} e \textbf{vincolati} in un problema di lot-sizing stocastico.
\textcolor{DarkGoldenrod}{\textbf{Nel caso (a)}}, il livello \(S_t\) soddisfa il vincolo \(S_t \ge I_t\) e coincide con il minimo non vincolato, cioè \(S_t = y_t^\ast\). In questa situazione, il minimizzatore non vincolato appartiene all’insieme ammissibile e, di conseguenza, il minimo vincolato e quello non vincolato coincidono.
\textcolor{DarkGoldenrod}{\textbf{Nel caso (b)}}, il livello \(S_t\) non soddisfa il vincolo \(S_t < I_t\) e risulta \(I_t = y_t^\ast\). In questo caso, il minimo vincolato è localizzato sul \textbf{bordo dell’insieme ammissibile}, poiché il minimo non vincolato non è accessibile a causa del vincolo.\\
\hspace*{1.5em}
La politica ottima è una politica \textcolor{DarkGoldenrod}{\textbf{base-stock}}
\[
x_t^\ast=\mu_t^\ast(I_t)=
\begin{cases}
S_t-I_t, & \text{se } I_t<S_t,\\
0, & \text{se } I_t\ge S_t.
\end{cases}
\]
I valori \(S_t\) sono livelli obiettivo: si ordina quanto serve per raggiungere il target ottimo. In orizzonte finito, il problema è determinare la sequenza ottima di livelli \(S_t\).

\paragraph{Politiche \((s,S)\) con costi fissi}
Se si introducono costi fissi, si perde convessità. Tuttavia una proprietà correlata (K-convessità) porta a una politica ottima
\[
\mu_t^\ast(I_t)=
\begin{cases}
S_t-I_t, & \text{se } I_t<s_t,\\
0, & \text{se } I_t\ge s_t,
\end{cases}
\]
dipendente da due sequenze \(s_t\) e \(S_t\), con \(s_t\le S_t\). In \emph{ambiente stazionario} è ottima una politica stazionaria \((s,S)\).  
Si ordina solo quando l’inventario è sotto \textbf{small \(s\)}, riportandolo a \textbf{big \(S\)}; \(S-s\) è una quantità minima d’ordine che aiuta a controllare i costi fissi.

\paragraph{Le maledizioni della programmazione dinamica}
La \textbf{programmazione dinamica} è un principio potente e flessibile, ma presenta alcune limitazioni rilevanti.  
\\ \hspace*{1.5em}
La prima è la \textcolor{DarkGoldenrod}{\textbf{maledizione della dimensionalità dello stato}}. È necessario disporre della funzione valore per ogni elemento dello spazio degli stati. Quando tale spazio è finito e di dimensioni contenute, le funzioni valore possono essere memorizzate in forma tabellare; tuttavia, questo approccio diventa impraticabile in presenza di spazi degli stati molto grandi.  
\\ \hspace*{1.5em}
Una seconda limitazione è la \textbf{maledizione dell’ottimizzazione}. La programmazione dinamica viene utilizzata per decomporre un problema multistadio intrattabile in una sequenza di sottoproblemi a singolo stadio. Tuttavia, anche questi sottoproblemi possono risultare complessi e difficili da risolvere.  
\\ \hspace*{1.5em}
Si incontra inoltre la \textbf{maledizione dell’aspettativa}. Quando i fattori di rischio \(\xi_t\) sono rappresentati da variabili aleatorie continue, il calcolo dell’aspettativa richiede la valutazione di integrali multidimensionali complessi; di conseguenza, è necessario adottare qualche strategia di discretizzazione.  
\\ \hspace*{1.5em}
Infine, si presenta la \textbf{maledizione della modellazione}. Il sistema può essere talmente complesso da rendere impossibile la definizione di un modello esplicito delle transizioni di stato. Questo aspetto è ulteriormente aggravato nel contesto della programmazione dinamica, poiché le transizioni dipendono almeno in parte dalle decisioni di controllo.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF 9 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1cm}
\textcolor{DeepPink}{\section{CAP 9 -- Modelli per la programmazione dinamica}}

\paragraph{Recap} MANCA

\subsection{Principio di modellazione per la programmazione dinamica}
\paragraph{Idea generale della programmazione dinamica}
Il principio della \textcolor{DeepPink}{\textbf{programmazione dinamica}} è un concetto \textbf{flessibile} per la \textcolor{DeepPink}{\textbf{decomposizione}} di un problema decisionale multistadio in una sequenza di \textcolor{DeepPink}{\textbf{problemi a singolo stadio}}, bilanciando il contributo immediato e i contributi attesi delle decisioni future.
Il principio si fonda sulla \textcolor{DeepPink}{\textbf{value function}}, definita dalla ricorsione di DP
\[
V_t(s_t) = \operatorname{opt}_{x_t \in X(s_t)}
\left\{
f_t(s_t, x_t) + \gamma\, \mathbb{E}\!\left[\,V_{t+1}(s_{t+1}) \mid s_t, x_t \right]
\right\}.
\]
\hspace*{1.5em}L'equazione precedente è solo una possibile forma della ricorsione di DP. Si possono adottare forme più specifiche, ad esempio quando i \textcolor{DeepPink}{\textbf{fattori di rischio}} sono \textbf{variabili aleatorie discrete} e l'attesa si riduce a una somma.

\paragraph{Scambio tra attesa e ottimizzazione}
In alcuni casi si adotta una riformulazione più radicale, in cui si \textcolor{DeepPink}{\textbf{scambiano}} gli operatori di attesa e ottimizzazione. Ciò può dipendere dalla struttura informativa del problema oppure essere il risultato di una manipolazione volta a evitare la soluzione di un sottoproblema stocastico difficile.
\\ \hspace*{1.5em}
Un caso ben noto è il \textcolor{DeepPink}{\textbf{Q-learning}}, una forma di reinforcement learning in cui $V(s)$ è sostituita da \textcolor{DeepPink}{\textbf{Q-factors}} $Q(s,x)$, che rappresentano il valore delle coppie stato–azione. Più in generale, lo scambio tra ottimizzazione e attesa può essere ottenuto introducendo il concetto di \textcolor{DeepPink}{\textbf{stato post-decisione}}.
\\ \hspace*{1.5em}
Trovare una descrizione adeguata dello stato del sistema può richiedere un'attenta modellazione: può essere necessaria una \textbf{ridefinizione dello spazio degli stati} per eliminare dipendenze dal percorso e ottenere un modello \textbf{markoviano aumentato ed equivalente}.
\\ \hspace*{1.5em}
Sebbene sia naturale pensare a stati e decisioni come scalari o vettori, essi possono consistere in oggetti diversi, ad esempio insiemi.
\\ \hspace*{1.5em}
Le \emph{capacità di modellazione} procedono di pari passo con la conoscenza algoritmica nell'affrontare un problema decisionale tramite DP; tali capacità si affinano solo con l'esperienza su una vasta gamma di problemi.

\paragraph{Processi decisionali di Markov finiti}
Il termine \textcolor{DeepPink}{\textbf{processo decisionale di Markov (MDP)}} è riservato a problemi con spazi di
stato e spazi delle azioni discreti. Il termine \textbf{azione} è spesso adottato per riferirsi alle decisioni di controllo. Stati e azioni discreti possono essere enumerati e associati a numeri interi.
\\ \hspace*{1.5em}
Si considerano \textbf{MDP finiti}. Anche se gli stati possono corrispondere a vettori in uno spazio multidimensionale, si usa una notazione del tipo $i, j \in S = \{1, \dots, N\}$, dove $N$ è la dimensione dello spazio degli stati.
\\ \hspace*{1.5em}
Si usa $a$ o $a_t$ per indicare le azioni; $A_t(i)$ o $A(i)$ denota l'insieme delle \textcolor{DeepPink}{\textbf{azioni ammissibili}} nello stato $i$ al tempo $t$. L'insieme di tutte le azioni possibili è $A$.
\\ \hspace*{1.5em}
Il sistema sottostante è una catena di Markov finita a tempo discreto. Nei MDP le \textbf{probabilità di transizione} dipendono dall'azione selezionata: $\pi_{t+1}(i,a,j)$ è la probabilità di transizione dallo stato $i$ allo stato $j$ durante l'intervallo $t+1$, dopo aver scelto l'azione $a$ al tempo $t$.

\paragraph{Esempio: controllo stocastico di inventario}
Dall'esempio si evince come, con spazio di stato finito e azioni ammissibili dipendenti dallo stato, le \textbf{matrici di transizione} dipendano dall'azione e possano essere rappresentate in forma tabellare.

\paragraph{Ricorsioni DP per MDP}
Nel contesto MDP, la value function al tempo $t$ è un vettore finito-dimensionale con componenti $V_t(i)$ e soddisfa
\[
V_t(i) = \operatorname{opt}_{a \in A(i)}
\left\{
f_t(i,a) + \gamma \sum_{j \in S}
\pi_{t+1}(i,a,j)\, V_{t+1}(j)
\right\}, \quad i \in S.
\]
Nel caso a \textcolor{DeepPink}{\textbf{orizzonte infinito scontato}}
\[
V(i) = \operatorname{opt}_{a \in A(i)}
\left\{
f(i,a) + \gamma \sum_{j \in S}
\pi(i,a,j)\, V(j)
\right\}, \quad i \in S.
\]
\hspace*{1.5em}
Se il \textbf{contributo immediato è stocastico} e dipende dallo stato successivo, lo si denota $h(i,a,j)$ e si riscrive
\[
V(i) = \operatorname{opt}_{a \in A(i)}
\sum_{j \in S}
\pi(i,a,j)\,
\left\{
h(i,a,j) + \gamma V(j)
\right\}, \quad i \in S.
\]
\hspace*{1.5em}
In principio, $f(i,a) = \sum_{j \in S} \pi(i,a,j)\, h(i,a,j)$, ma ciò può essere impraticabile quando $N$ è finito ma enorme oppure quando le probabilità di transizione non sono note.

\paragraph{Esempio: arresto ottimo ricorrente}
Dall'esempio si evince il \textcolor{DeepPink}{\textbf{trade-off}} tra ricompensa immediata e attesa di opportunità migliori, e come la struttura della catena consenta una formulazione DP esplicita sui valori $V(k)$.

\subsection{Valutazione delle politiche e Q-factors}
\paragraph{Valutazione delle politiche e Q-factors}
L’esempio precedente potrebbe suggerire che gli \textbf{MDP finiti} siano piuttosto semplici da trattare.
La \textbf{value function} è semplicemente un vettore e, nel caso di un \textbf{orizzonte finito}, esiste una relazione esplicita tra le funzioni valore a stadi diversi. Se l’insieme delle \textbf{azioni ammissibili} non è troppo grande, il passo di ottimizzazione si risolve banalmente per enumerazione.
\\ \hspace*{1.5em}Nel caso a \textbf{orizzonte infinito}, è necessario determinare i valori degli stati risolvendo un \textbf{sistema di equazioni}, che non è lineare (è infatti \textbf{a tratti lineare}). Tuttavia, tale sistema può essere risolto tramite metodi iterativi piuttosto semplici.
In realtà, il problema degli MDP potrebbe non essere così semplice, a causa della \textbf{dimensione dello spazio degli stati}. Inoltre, anche solo determinare l’intero insieme delle \textbf{probabilità di transizione} \(\pi(i,a,j)\) può risultare molto difficile, se non impossibile, quando si manifesta la \textbf{curse of modeling}.
\\ \hspace*{1.5em}Un primo ingrediente per aggirare queste difficoltà è il \textbf{campionamento Monte Carlo}. Un secondo ingrediente consiste nel riscrivere la \textbf{ricorsione di DP} in una forma diversa, basata sui \textbf{Q-factors}. Questo viene ora fatto nel caso a \textbf{orizzonte infinito}.
In modo informale, un \textbf{Q-factor} \(Q(i,a)\) misura il valore di intraprendere l’azione \(a\) quando il sistema si trova nello stato \(i\). Più precisamente, tale quantità deve essere definita con riferimento alla \textbf{politica} che verrà applicata negli stadi successivi.

\paragraph{Policy evaluation e policy iteration}
Una \textcolor{DeepPink}{\textbf{politica stazionaria ammissibile}} $\mu$ associa a ciascuno stato $i$ un'azione $a=\mu(i)\in A(i)$. La value function associata a $\mu$ si ottiene risolvendo un sistema lineare
\[
V^\mu(i)= f\bigl(i,\mu(i)\bigr)+\gamma \sum_{j\in S}\pi\bigl(i,\mu(i),j\bigr)\,V^\mu(j),
\quad i\in S.
\]
Questa relazione è fondamentale nei metodi basati su \textcolor{DeepPink}{\textbf{policy iteration}}: si valuta una politica candidata e poi si tenta di migliorarla.

\paragraph{Definizione di Q-factor per una politica}
Il \textcolor{DeepPink}{\textbf{Q-factor}} associato a $\mu$ è la mappa $S\times A \to \mathbb{R}$:
\[
Q^\mu(i,a) := f(i,a)+\gamma \sum_{j\in S}\pi(i,a,j)\,V^\mu(j).
\]
L'idea è applicare $a$ nello stato $i$ e poi seguire la politica $\mu$.
Sostituendo la value function ottima nella relazione precedente si ottengono i \textcolor{DeepPink}{\textbf{Q-factors ottimi}}
\[
Q(i,a) = f(i,a) + \gamma \sum_{j \in S} \pi(i,a,j)\, V(j),
\qquad i \in S,\; a \in A(i).
\]
Vale $V(j)\equiv \operatorname{opt}_{a\in A(j)} Q(j,a)$, $j\in S$.
La \textbf{ricorsione di DP} può essere riscritta come
\[
Q(i,a)= f(i,a)+\gamma \sum_{j\in S}\pi(i,a,j)\left[\operatorname{opt}_{\tilde a\in A(j)} Q(j,\tilde a)\right],
\quad i\in S,\; a\in A(i).
\]
\paragraph{Vantaggi e svantaggi}
Confrontando le equazioni, emergono sia aspetti negativi sia aspetti positivi. Lo \textbf{svantaggio principale} è che,
al posto della funzione valore sugli stati \(V(i)\), si introducono ora funzioni valore \textbf{stato--azione}
\(Q(i,a)\); salvo casi di dimensione molto ridotta, ciò sembra aggravare ulteriormente la \textbf{curse of dimensionality}.
Il \textbf{vantaggio fondamentale} è però lo \textbf{scambio tra aspettazione e ottimizzazione}. Questo aspetto risulta particolarmente rilevante nei problemi continui, nei quali è spesso più semplice risolvere molti problemi di ottimizzazione deterministici di piccole dimensioni piuttosto che un unico grande problema stocastico. Inoltre, i Q-factors possono essere \textbf{appresi tramite campionamento statistico}, caratteristica cruciale sia nei problemi di grande scala sia quando la dinamica sottostante è sconosciuta; ciò apre la strada alla \textbf{programmazione dinamica model-free}. Infine, quando la dimensionalità rende impossibile determinare esplicitamente tutti i Q-factors per ogni coppia stato--azione, è possibile ricorrere a una \textbf{rappresentazione compatta} basata su un’\textbf{architettura di approssimazione}, che può spaziare da una regressione lineare relativamente semplice fino alle \textbf{reti neurali profonde}.

\subsection{Diverse forme di DP stocastica}

\paragraph{Ricorsione DP di base}
Si consideri nuovamente la \emph{ricorsione di DP}
\[
V_t(s_t) = \operatorname{opt}_{x_t \in X(s_t)}
\left\{
f_t(s_t, x_t) + \gamma \,\mathbb{E}\!\left[\,V_{t+1}(s_{t+1}) \mid s_t, x_t \right]
\right\}.
\]

\paragraph{Ipotesi sottostanti}
Si osserva lo stato $s_t$ all'istante $t$; si prende una decisione ammissibile $x_t\in X(s_t)$; si osserva un contributo immediato $f_t(s_t,x_t)$; si passa a un nuovo stato $s_{t+1}$ che dipende dai fattori di rischio realizzati $\xi_{t+1}$, secondo una distribuzione che può dipendere da $s_t$ e $x_t$.
\\ \hspace*{1.5em}
Per determinare $V_t(\cdot)$ a partire da $V_{t+1}(\cdot)$ si dovrebbe risolvere un problema di \textcolor{DeepPink}{\textbf{ottimizzazione stocastica}} che può includere un'attesa impegnativa; nei MDP i Q-factors consentono di scambiare ottimizzazione e attesa, e talvolta lo scambio è richiesto dalla struttura informativa.

\paragraph{Esempio: lot-sizing con lookahead limitato}
Dall’esempio si evince che, quando la domanda del periodo successivo è nota al momento della decisione, il contributo immediato diventa deterministico e la ricorsione di programmazione dinamica assume una forma in cui l’ottimizzazione è interna all’aspettazione. La struttura della ricorsione riflette quindi direttamente l’ordine temporale tra informazione, decisione e dinamica del sistema.

\subsection{Variabili di stato post-decisione}

\paragraph{Ricorsione in forma scambiata}
Una forma alternativa è:
\[
V_t(s_t)= \mathbb{E}_t\!\left[\operatorname{opt}_{x_t\in X(s_t)}\left\{f_t(x_t,s_t)+\gamma V_{t+1}(s_{t+1})\right\}\right].
\]

\paragraph{Vantaggi potenziali}
Il problema di ottimizzazione interno è deterministico e l'attesa esterna può essere stimata tramite campionamento statistico.

\paragraph{Introduzione dello stato post-decisione}
Si introduce uno stato intermedio osservato dopo la decisione $x_t$ ma prima della realizzazione di $\xi_{t+1}$, detto \textcolor{DeepPink}{\textbf{stato post-decisione}} $s_t^x$. Si scinde la transizione in due passi: $s_t^x = g_t^{(1)}(s_t,x_t)$, $s_{t+1}= g_{t+1}^{(2)}(s_t^x,\xi_{t+1})$.

\paragraph{Relazione tra funzioni valore}
Si definisce il valore del post-decision state $V_t^x(s_t^x)$ e vale $V_t^x(s_t^x)= \mathbb{E}\!\left[\,V_{t+1}(s_{t+1}) \mid s_t^x\right]$. Quindi la ricorsione standard può essere riscritta come ottimizzazione deterministica:
\[
V_t(s_t)= \operatorname{opt}_{x_t\in X(s_t)}\left\{f_t(s_t,x_t)+\gamma V_t^x(s_t^x)\right\}.
\]

\paragraph{Passo all'indietro e nuovo scambio}
Scrivendo la relazione un passo indietro e sostituendo l'espressione precedente, si ottiene una ricorsione che presenta nuovamente lo scambio tra attesa e ottimizzazione:
\[
V_{t-1}^x(s_{t-1}^x)
=
\mathbb{E}\!\left[
\operatorname{opt}_{x_t\in X(s_t)}
\left(
f_t(s_t,x_t)+\gamma V_t^x(s_t^x)
\right)
\Bigm| s_{t-1}^x
\right].
\]

\paragraph{Connessione con i Q-factors}
La coppia stato–azione $(i,a)$ può essere interpretata come stato post-decisione prima di osservare la transizione casuale al nuovo stato; la formulazione in termini di Q-factors si inserisce naturalmente in questo quadro.

\subsection{Aumento dello stato nella gestione delle scorte}

\paragraph{Inventario on-hand vs inventario disponibile}
L'\textcolor{DeepPink}{\textbf{inventario on-hand}} appare naturale come variabile di stato, ma le decisioni d'ordine dovrebbero basarsi sull'\textcolor{DeepPink}{\textbf{inventario disponibile}}, che include on-order e backlog.

\paragraph{Limite dell'equazione di stato standard}
L'equazione $I_{t+1}= I_t + x_t - d_{t+1}$ assume disponibilità immediata di quanto ordinato al tempo $t$.

\paragraph{Lead time e variabili di pipeline}
Se il \textcolor{DeepPink}{\textbf{lead time}} è un intero $LT\ge 1$, si introducono variabili $z_{t,\tau}$ (pipeline), con $\tau=0,1,\dots,LT-1$. Vale $I_{t+1}= I_t + z_{t,0} - d_{t+1}$ (trascurando l'incertezza) e la decisione $x_t$ entra come $z_{t+1,LT-1}= x_t$. Per $\tau<LT-1$, le transizioni sono uno shift temporale: $z_{t+1,\tau}= z_{t,\tau+1}$, $\tau=0,1,\dots,LT-2$.

\paragraph{Items deperibili e stato per età}
Per prodotti deperibili si introduce un array $I_{t,\tau}$ che descrive l'inventario per età; le transizioni dipendono dalla politica di issuing FIFO o LIFO.

\subsection{Revenue management}

\paragraph{Definizione e obiettivo}
Il \textcolor{DeepPink}{\textbf{revenue management}} consiste in modelli e tecniche per massimizzare il ricavo ottenuto vendendo risorse deperibili; l'idea nasce (come \textcolor{DeepPink}{\textbf{yield management}}) nell'industria aerea.

\paragraph{Approcci quantity-based e price-based}
Esistono due approcci: \textcolor{DeepPink}{\textbf{quantity-based}} (si limita la disponibilità della risorsa per massimizzare il ricavo) e \textcolor{DeepPink}{\textbf{price-based}} (si aggiustano dinamicamente i prezzi, dynamic pricing). Si considerano modelli DP quantity-based assumendo costo marginale nullo o trascurabile, così che massimizzare il profitto equivale a massimizzare il ricavo.

\paragraph{Classi tariffarie e prezzi}
Si considerino $C$ unità di risorsa da allocare a $n$ classi $j=1,\dots,n$, vendute a prezzi $p_j$ con $p_1 > p_2 > \cdots > p_n$. I posti sono fisicamente identici, ma differenziati tramite ancillaries (cancellazione, vincoli, pasti, ecc.).

\paragraph{Caratteristiche essenziali}
La domanda $D_j$ è casuale e può realizzarsi con schemi diversi. Due aspetti sono essenziali: \textcolor{DeepPink}{\textbf{comportamento dei clienti}} (segmentazione perfetta vs preferenze da modellare con un choice model) e \textcolor{DeepPink}{\textbf{timing della domanda}} (sequentialità auspicabile; caso peggiore quando arrivano prima i clienti low-budget).

\paragraph{Modelli statici e dinamici; protection levels e bid-prices}
Un modello con intervalli disgiunti, ciascuno dedicato a una sola classe, è statico; se le richieste sono interleaved nel tempo è dinamico. La policy si esprime tramite \textcolor{DeepPink}{\textbf{protection levels}} oppure \textcolor{DeepPink}{\textbf{bid-prices}} (prezzo minimo accettabile per vendere un posto).

\subsection{Modello statico con segmentazione perfetta della domanda}

\paragraph{Assunzioni, stadi e stato}
Le domande sono indipendenti e avvengono sequenzialmente (prima $D_n$); nessun cliente cambia classe. Gli stadi decisionali sono $j=n,n-1,\dots,1$. La variabile di stato è la capacità residua $s_j$, con stato iniziale $s_n=C$. Condizione al contorno: $V_0(s_0)=0$, $s_0=0,1,\dots,C$.

\paragraph{Ricorsione DP in forma scambiata}
La ricorsione assume la forma scambiata:
\[
V_j(s_j)=
\mathbb{E}\!\left[
\max_{0\le x_j \le \min\{s_j,D_j\}}
\left(
p_j x_j + V_{j-1}(s_j-x_j)
\right)
\right].
\]
Con uno shift di indice e notazione compatta:
\[
V_{j+1}(s)=
\mathbb{E}\!\left[
\max_{0\le x \le \min\{s,D_{j+1}\}}
\left(
p_{j+1}x + V_j(s-x)
\right)
\right].
\]

\subsection{Valore marginale atteso e livelli di protezione}

\paragraph{Valore marginale atteso della capacità}
Si definisce $\Delta V_j(s):= V_j(s)-V_j(s-1)$, che misura il \textcolor{DeepPink}{\textbf{costo opportunità}} di un posto dato $s$.

\paragraph{Somma telescopica e riscrittura}
Si ottiene:
\[
V_{j+1}(s)
=
V_j(s)
+
\mathbb{E}\!\left[
\max_{0\le x \le \min\{s,D_{j+1}\}}
\left(
\sum_{z=1}^{x}\bigl(p_{j+1}-\Delta V_j(s+1-z)\bigr)
\right)
\right].
\]
La riscrittura usa una somma telescopica: $V_j(s-x)= V_j(s)-\sum_{z=1}^{x}\Delta V_j(s+1-z)$.

\paragraph{Proprietà dei valori marginali}
Si possono provare: $\Delta V_j(s+1)\le \Delta V_j(s)$, $\Delta V_{j+1}(s)\ge \Delta V_j(s)$, $\forall s,j$.

\paragraph{Regola di accettazione e protezione annidata}
I termini $p_{j+1}-\Delta V_j(s+1-z)$ sono decrescenti in $z$: si aumenta $x$ finché compare il primo termine negativo o si raggiunge $\min\{s,D_{j+1}\}$. Si ottiene un livello di protezione \textcolor{DeepPink}{\textbf{annidato}}:
\[
y_j^\ast:= \max\{\,y:\; p_{j+1}<\Delta V_j(y)\,\},
\qquad j=1,\dots,n-1.
\]
La decisione ottima allo stadio $j+1$ è:
\[
x_{j+1}^\ast(s_{j+1},D_{j+1})
=
\min\{(s_{j+1}-y_j^\ast)^+,\; D_{j+1}\}.
\]
Lo scambio tra attesa e ottimizzazione è giustificato dal fatto che non serve conoscere $D_{j+1}$ in anticipo: si accetta finché si raggiunge la protezione o si esaurisce la capacità o termina lo stadio.

\subsection{Modello dinamico con segmentazione perfetta della domanda}

\paragraph{Assunzioni e probabilità di arrivo}
Si rilassa la sequenzialità della domanda, mantenendo una segmentazione rigida. Il tempo è discretizzato in $t=1,\dots,T$ assumendo al più un arrivo per intervallo. Sia $\lambda_j(t)$ la probabilità di arrivo della classe $j$ nell'intervallo $t$, con vincolo $\sum_{j=1}^{n}\lambda_j(t)\le 1$, $\forall t$.

\paragraph{value function e condizioni al contorno}
Si determinano $V_t(s)$ (capacità residua $s$) con $V_t(0)=0$, $t=1,\dots,T$, $V_{T+1}(s)=0$, $s=0,1,\dots,C$.

\paragraph{Ricavo disponibile e decisione binaria}
Sia $R(t)$ il ricavo disponibile al tempo $t$: vale $p_j$ se arriva un cliente di classe $j$, $0$ altrimenti. Alla richiesta si decide con $x\in\{0,1\}$ se accettare o meno.

\paragraph{Ricorsione DP in forma scambiata}
La ricorsione assume la forma scambiata:
\[
V_t(s)=
\mathbb{E}\!\left[
\max_{x\in\{0,1\}}
\left\{
R(t)\,x+V_{t+1}(s-x)
\right\}
\right].
\]
Usando i valori marginali attesi si ottiene una politica in termini di protection levels, qui potenzialmente time-varying.

\subsection{Modello dinamico con scelta del cliente}

\paragraph{Modello di scelta}
Se la segmentazione perfetta viene rilassata, occorre un \textcolor{DeepPink}{\textbf{choice model}}. La decisione di controllo al tempo $t$ è il sottoinsieme $S_t\subseteq N=\{1,\dots,n\}$ delle classi offerte.

\paragraph{Ricavi e probabilità di scelta}
Si introduca $p_0=0$ per il caso di non-acquisto. Con probabilità $\lambda$ arriva un potenziale passeggero; dato $S_t$, la probabilità di scelta della classe $j$ è $P_j(S_t)$, includendo $P_0(S_t)$, con vincoli: $P_j(S)\ge 0$, $S\subseteq N$, $j\in S\cup\{0\}$, $\sum_{j\in S}P_j(S)+P_0(S)=1$, $S\subseteq N$.

\paragraph{Probabilità di acquisto dipendenti dalla decisione}
Le probabilità dipendenti dalla decisione sono $\lambda P_j(S_t)$ per $j=1,\dots,n$, e $(1-\lambda)+\lambda P_0(S_t)$ per $j=0$.

\paragraph{Ricorsione DP}
In questo caso la decisione deve essere dichiarata prima della scelta del passeggero, quindi la ricorsione è nella forma standard:
\[
V_t(s)
=
\max_{S_t\subseteq N}
\left\{
\sum_{j\in S_t}\lambda P_j(S_t)\bigl(p_j+V_{t+1}(s-1)\bigr)
+
\bigl(\lambda P_0(S_t)+1-\lambda\bigr)\,V_{t+1}(s)
\right\}.
\]

\paragraph{Osservazione conclusiva}
La forma della ricorsione riflette la \textcolor{DeepPink}{\textbf{struttura informativa}}: qui la massimizzazione è sull'attesa perché la decisione precede la scelta del cliente.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF 10 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1cm}
\textcolor{MediumVioletRed}{\section{CAP 10 -- Programmazione dinamica numerica per stati discreti}}
\subsection{Catene di Markov a tempo discreto}

\paragraph{Definizione di catena di Markov a tempo discreto}
Una \textbf{catena di Markov a tempo discreto} è un processo stocastico con variabile di stato $s_t$, $t = 0,1,2,\dots$, che assume valori in un insieme \textbf{discreto}.  
Se lo spazio degli stati è numerabile, è possibile associare ciascuno stato a un numero intero e rappresentare il processo mediante una \textbf{rete}.

\paragraph{Probabilità di transizione}
Le transizioni tra stati sono rappresentate da archi diretti etichettati con le \textbf{probabilità di transizione}.  
Le probabilità di transizione sono \textbf{probabilità condizionate} che dipendono solo dallo stato corrente:
\[
\pi(i,j) := \mathbb{P}\{ s_{t+1} = j \mid s_t = i \}.
\]

\paragraph{Catene omogenee}
Nel caso di problemi a \textbf{orizzonte infinito} si utilizzano catene \textbf{omogenee} (o \textbf{time-invariant}), in cui le probabilità di transizione non dipendono dal tempo.

\paragraph{Matrice di transizione}
Una catena di Markov a tempo discreto può essere descritta raccogliendo le probabilità di transizione in una \textbf{matrice di transizione a un passo} $\Pi$, in cui l’elemento $\pi_{ij}$ rappresenta la probabilità di transizione dallo stato $i$ allo stato $j$.

\paragraph{Proprietà di normalizzazione}
Poiché dopo una transizione il processo deve necessariamente trovarsi in uno stato dello spazio degli stati, ogni riga della matrice di transizione somma a uno:
\[
\sum_{j=1}^N \pi(i,j) = 1, \quad \forall i.
\]

\subsection{Catene di Markov controllate e MDP}

\paragraph{Azioni e transizioni controllate}
Nei \textbf{processi decisionali markoviani} (MDP), le transizioni sono parzialmente controllate tramite la selezione di azioni.  
In ciascuno stato $i$ è disponibile un insieme finito di azioni ammissibili $A(i)$.  
Per ogni azione $a \in A(i)$ sono definite probabilità di transizione:
\[
\pi(i,a,j).
\]

\paragraph{Distribuzione stazionaria}
Per valutare le prestazioni di una politica di controllo stazionaria, si può considerare la \textbf{probabilità di lungo periodo} di trovarsi in ciascuno stato, indicata con $q(i)$ e raccolta nel vettore $q$.  
Tali probabilità potrebbero non esistere se la catena non soddisfa opportune proprietà strutturali.

\paragraph{Proprietà strutturali delle catene}
Una catena può presentare stati \textbf{transienti}, \textbf{ricorrenti}, \textbf{assorbenti} e \textbf{periodicità}.  
Si assume una \textbf{unichain}, tale che ogni stato possa essere visitato infinitamente spesso nel lungo periodo e raggiunto da ogni altro stato in tempo finito con probabilità positiva.  
Nel seguito si assume che il processo di Markov sia \textbf{ben comportato} per ogni politica di controllo.

\subsection{MDP a orizzonte temporale finito}

\paragraph{Ricorsione di programmazione dinamica}
Per un MDP a \textbf{orizzonte finito}, la ricorsione di DP è:
\[
V_t(i) = \operatorname{opt}_{a \in A(i)}
\left\{
f(i,a) + \sum_j \pi(i,a,j)\, V_{t+1}(j)
\right\}, \quad i \in S.
\]

\paragraph{Cosa insegna l’esempio (arresto ottimo su random walk)}
L’esempio mostra che, in un problema di arresto ottimo a orizzonte finito, la politica risultante può essere \textbf{non stazionaria} e dipendere dal tempo residuo (confronto tra \texttt{wait} e \texttt{stop} tramite DP).

\subsection{Processi decisionali markoviani a orizzonte infinito}

\paragraph{Equazioni di Bellman a orizzonte infinito}
Nel caso di un \textbf{MDP a orizzonte infinito}, la value function è definita implicitamente da equazioni del tipo:
\[
V(i) = \operatorname{opt}_{a \in A(i)}
\left\{
f(i,a) + \sum_j \pi(i,a,j)\, V(j)
\right\}, \quad i \in S,
\tag{2}
\]
oppure, quando il contributo immediato dipende anche dallo stato successivo,
\[
V(i) = \operatorname{opt}_{a \in A(i)}
\sum_{j \in S} \pi(i,a,j)
\left\{
h(i,a,j) + V(j)
\right\}, \quad i \in S.
\tag{3}
\]

\paragraph{Rappresentazione vettoriale}
Per MDP finiti, lo spazio degli stati è:
\[
S = \{1,\dots,n\},
\]
e la value function $V : S \to \mathbb{R}$ è rappresentabile come vettore $V \in \mathbb{R}^n$ con componenti $V(i)$.

\paragraph{Strategie di soluzione}
Esistono due strategie fondamentali:
\begin{itemize}[label=-]
\item \textbf{Value iteration}: iterazioni economiche, convergenza al valore ottimo solo nel limite;
\item \textbf{Policy iteration}: iterazioni più costose, ma convergenza in tempo finito per MDP finiti (numero finito di politiche).
\end{itemize}

\subsection{Operatori di Bellman}

\paragraph{Operatore ottimo}
Data una value function generica $\tilde V$, si definisce l’operatore $T$:
\[
[T\tilde V](i) =
\operatorname{opt}_{a \in A(i)}
\sum_{j \in S} \pi(i,a,j)
\left\{
h(i,a,j) + \tilde V(j)
\right\}, \quad i \in S.
\tag{4}
\]

\paragraph{Operatore associato a una politica}
Data una politica stazionaria generica $\mu$, si definisce l’operatore $T_\mu$:
\[
[T_\mu \tilde V](i) =
\sum_{j \in S} \pi(i,\mu(i),j)
\left\{
h(i,\mu(i),j) + \tilde V(j)
\right\}, \quad i \in S.
\tag{5}
\]

\paragraph{Ruolo degli operatori e punti fissi}
L’operatore $T$ è centrale per la \textbf{value iteration}, mentre $T_\mu$ è centrale per la \textbf{policy iteration}.  
Il vettore valore ottimo $V$ è un \textbf{punto fisso} di $T$:
\[
V = TV.
\tag{6}
\]
La value function $V^\mu$ di una politica stazionaria $\mu$ è il punto fisso di $T_\mu$.

\paragraph{Condizioni di esistenza}
Per un MDP finito con \textbf{sconto stretto} ($\gamma < 1$) e contributi per stadio \textbf{limitati} (esiste $M$ con $|h(i,a,j)| \le M$), si assume che $T$ e $T_\mu$ siano \textbf{operatori di contrazione}, con punto fisso unico.

\paragraph{Miglioramento delle politiche}
L’operatore $T$ fornisce una caratterizzazione della politica stazionaria ottima e consente di \textbf{migliorare} una politica non ottima usando il valore $V^\mu$, che viene ottenuto tramite $T_\mu$.

\subsection{Value iteration}

\paragraph{Idea di base}
Con sconto stretto ($\gamma<1$), si cerca un punto fisso di $T$ tramite iterazione:
\[
V^{(k+1)} = T V^{(k)}.
\]
Se $V^{(k)} \to V$, allora $V$ è un punto fisso di $T$ e rappresenta la soluzione.

\subsection{Algoritmo di Value Iteration}

\begin{algorithm}[H]
\caption{Value Iteration per MDP a orizzonte infinito}
\begin{algorithmic}[1]
\State Scegliere una value function iniziale $V^{(0)}$ (ad esempio $V^{(0)}(i)=0$ per ogni $i \in S$)
\State Scegliere una tolleranza $\varepsilon$
\State Inizializzare $k=0$ e \texttt{stop}=\texttt{false}
\While{\texttt{stop} $\neq$ \texttt{true}}
    \For{ogni stato $i \in S$}
        \State
        \[
        V^{(k+1)}(i) =
        \operatorname{opt}_{a \in A(i)}
        \left\{
        f(i,a) + \sum_{j \in S} \pi(i,a,j)\, V^{(k)}(j)
        \right\}
        \]
    \EndFor
    \If{$\|V^{(k+1)} - V^{(k)}\|_1 < \varepsilon$}
        \State \texttt{stop}=\texttt{true}
    \Else
        \State $k = k + 1$
    \EndIf
\EndWhile
\State Porre $\hat V = V^{(k+1)}$
\State Per ogni $i \in S$, determinare una politica stimata ottima:
\[
\hat\mu(i) \in
\arg\operatorname{opt}_{a \in A(i)}
\left\{
f(i,a) + \sum_j \pi(i,a,j)\, \hat V(j)
\right\}
\]
\State Restituire $\hat V$ e $\hat\mu$
\end{algorithmic}
\end{algorithm}

\paragraph{Cosa insegna l’esempio numerico (value iteration)}
L’esempio illustra che la politica ottima ottenuta da value iteration può dipendere in modo sensibile da probabilità di transizione e fattore di sconto, e che la convergenza in iterazioni può variare significativamente con tali parametri.

\subsection{Policy iteration}

\paragraph{Motivazione}
Con value iteration può accadere di individuare presto la politica ottima (da valori approssimati) ma senza valutarne ancora con precisione il valore.  
Policy iteration mira invece alla valutazione (più) diretta del valore di una politica.

\paragraph{Policy evaluation come sistema lineare}
Per una politica stazionaria $\mu$, $T_\mu$ non include ottimizzazione:
\[
[T_\mu \tilde V](i)= f\bigl(i,\mu(i)\bigr)+\sum_{j\in S}\pi\bigl(i,\mu(i),j\bigr)\tilde V(j),\quad i\in S.
\]
Il valore $V^\mu$ soddisfa:
\[
V^\mu(i)= f\bigl(i,\mu(i)\bigr)+\sum_{j\in S}\pi\bigl(i,\mu(i),j\bigr)V^\mu(j),\quad i\in S.
\]
Definendo la matrice di transizione indotta $\Pi^\mu$ e il vettore dei contributi $f^\mu$:
\[
f^\mu :=
\begin{bmatrix}
f\bigl(1,\mu(1)\bigr)\\
f\bigl(2,\mu(2)\bigr)\\
\vdots\\
f\bigl(n,\mu(n)\bigr)
\end{bmatrix},
\tag{9}
\]
si ottiene il sistema:
\[
V^\mu = f^\mu + \gamma \Pi^\mu V^\mu,
\qquad
\bigl(I-\gamma \Pi^\mu\bigr)V^\mu=f^\mu.
\]
Formalmente:
\[
V^\mu = \bigl(I-\gamma \Pi^\mu\bigr)^{-1}f^\mu.
\]
Per matrici grandi e sparse, metodi diretti possono essere proibitivi; si adottano quindi metodi iterativi.

\paragraph{Policy improvement}
Data $V^\mu$, una politica migliorata $\tilde\mu$ è definita da:
\[
\tilde\mu(i)\in
\arg\operatorname{opt}_{a\in A(i)}
\left\{
f(i,a)+\sum_j \pi(i,a,j)\,V^\mu(j)
\right\},\quad i\in S.
\]
Per un problema di massimizzazione:
\[
V^\mu(i)\le V^{\tilde\mu}(i),\quad i\in S,
\]
(con disuguaglianza invertita in minimizzazione). Se $\mu$ non è ottima, la disuguaglianza è stretta per almeno uno stato.  
Poiché il numero di politiche stazionarie deterministiche è finito, una sequenza di miglioramenti porta a una politica ottima.

\subsection{Algoritmo di Policy Iteration}

\begin{algorithm}[H]
\caption{Policy Iteration per MDP finito a orizzonte infinito}
\begin{algorithmic}[1]
\State Definire una politica stazionaria iniziale arbitraria $\mu^{(0)}$
\State Inizializzare $k=0$ e \texttt{stop}=\texttt{false}
\While{\texttt{stop} $\neq$ \texttt{true}}
    \State \textbf{Policy evaluation:} risolvere
    \[
    \bigl(I-\gamma \Pi^{\mu^{(k)}}\bigr)V^{\mu^{(k)}} = f^{\mu^{(k)}}
    \]
    \State \textbf{Policy improvement:} porre, per ogni $i\in S$,
    \[
    \mu^{(k+1)}(i)\in
    \arg\operatorname{opt}_{a\in A(i)}
    \left\{
    f(i,a)+\sum_j \pi(i,a,j)\,V^{\mu^{(k)}}(j)
    \right\}
    \]
    \If{$\mu^{(k+1)}=\mu^{(k)}$}
        \State \texttt{stop}=\texttt{true}
    \Else
        \State $k=k+1$
    \EndIf
\EndWhile
\State Restituire la value function ottima e la politica stazionaria ottima
\end{algorithmic}
\end{algorithm}

\paragraph{Cosa insegna l’esempio numerico (policy iteration)}
L’esempio evidenzia che policy iteration può raggiungere la politica ottima in un numero finito (spesso piccolo) di passi di miglioramento, grazie alla valutazione esplicita della politica corrente.

\subsection{Value iteration vs. policy iteration}

\paragraph{Confronto e collegamento}
Value iteration impiega molte iterazioni economiche e converge nel limite; policy iteration usa poche iterazioni potenzialmente costose e converge in tempo finito per MDP finiti.  
La valutazione di una politica può anche essere eseguita iterativamente tramite:
\[
V^{(k+1)}_\mu(i)= f\bigl(i,\mu(i)\bigr)+\sum_{j\in S}\pi\bigl(i,\mu(i),j\bigr)V^{(k)}_\mu(j),\quad i\in S,
\tag{10}
\]
e interrompendo in modo prematuro tale procedura si ottiene una stima $\widehat V_\mu$ da usare nel miglioramento, dando luogo a \textbf{optimistic policy iteration} e, più in generale, a metodi di \textbf{generalized policy iteration}.  
Value iteration e policy iteration possono quindi essere visti come estremi di un continuum di approcci.

\subsection{Collegamento con Reinforcement Learning}

\paragraph{DP model-free e Q-factors}
Quando le probabilità di transizione (e possibilmente i contributi immediati) non sono note, si passa a una DP \textbf{model-free}. In tale contesto, la value function è tipicamente sostituita dai \textbf{Q-factors} $Q(s,a)$ dipendenti da stati e azioni.

\paragraph{Q-learning e SARSA}
Il \textbf{Q-learning} è il corrispettivo RL della value iteration ed è un metodo \textbf{off-policy}.  
I corrispettivi RL della policy iteration sono \textbf{on-policy}; un approccio noto è \textbf{SARSA}.

\paragraph{Osservazione operativa in RL}
In RL non è in generale possibile valutare esattamente il valore di una politica se ciò richiede simulazioni Monte Carlo costose o esperimenti online; è quindi necessario effettuare un passo di miglioramento prima o poi, generando diverse varianti di strategie.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF 11 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1cm}
\textcolor{DarkViolet}{\section{CAP 11 -- Programmazione dinamica approssimativa e apprendimento per rinforzo per stati discreti}}


\subsection{Approximate Dynamic Programming e DP model-free}

\paragraph{Approximate Dynamic Programming}
L’\textbf{Approximate Dynamic Programming (ADP)} è il nome collettivo di un’ampia classe di \textbf{metodi numerici di programmazione dinamica} che, a differenza dei metodi standard, \textbf{non garantiscono} di trovare una \textbf{politica ottima}.  
In cambio della natura approssimata, i metodi ADP mirano ad attenuare le \textbf{maledizioni della DP}.  
Nel seguito si considerano \textbf{metodi approssimati} per \textbf{MDP finiti} nel caso \textbf{a orizzonte infinito}.

\paragraph{Reinforcement Learning}
Le versioni \textbf{model-free} di \textbf{value iteration} e \textbf{policy iteration} sono raccolte sotto il termine \textbf{Reinforcement Learning (RL)}.  
Lo schema di base è il seguente: un \textbf{agente} interagisce con un sistema per apprendere una \textbf{politica di controllo} senza disporre di un \textbf{modello} del sistema; al tempo $t$ osserva lo \textbf{stato} $s_t$, prova un’\textbf{azione} $a_t$, e osserva il \textbf{contributo immediato} $h_{t+1}$ e lo \textbf{stato successivo} $s_{t+1}$.  
Una buona politica deve bilanciare \textbf{obiettivi di breve e lungo periodo}, misurati dal contributo immediato e dal valore dello stato successivo.

\subsection{Valori di stato, Q-factors e necessità di rappresentazioni compatte}

\paragraph{Perché $V(s)$ non basta in RL model-free}
Un punto chiave nel RL model-free è che, anche con una conoscenza perfetta della \textbf{value function} $V(s)$, non è possibile valutare correttamente la bontà di un’azione se mancano informazioni sulle \textbf{transizioni} verso lo stato successivo.

\paragraph{Q-factors}
Un rimedio consiste nell’introdurre i \textbf{valori stato--azione}, cioè i \textbf{Q-factors}:
\[
Q(s,a).
\]
Sostituendo i valori di stato con i valori stato--azione, diventa possibile adottare \textbf{strategie di campionamento} per apprendere una politica decisionale.  
Si assume che lo spazio \textbf{stato--azione} non sia troppo grande e che una rappresentazione \textbf{tabellare} dei Q-factors sia fattibile; spesso, però, ciò non vale, e risulta necessaria una \textbf{rappresentazione compatta}.

\subsection{Campionamento e stima in contesti non stazionari}

\paragraph{Monte Carlo}
In teoria della probabilità si è interessati al valore atteso di una funzione $h(\cdot)$ di una variabile aleatoria $X$:
\[
\theta=\mathbb{E}\!\left[h(X)\right]=\int_{\mathcal{X}} h(x)f_X(x)\,dx.
\]
Poiché l’integrazione ad alta dimensionalità è difficile, si ricorre al \textbf{campionamento Monte Carlo}:
\[
\hat{\theta}=\frac{1}{m}\sum_{k=1}^{m} h\!\left(X^{(k)}\right),
\]
che fornisce una stima \textbf{non distorta} di $\theta$.

\paragraph{Complicazioni negli MDP}
Negli MDP si vogliono stimare \textbf{valori di stato} $V(i)$ o \textbf{Q-factors} $Q(i,a)$. In questo contesto l’apprendimento statistico deve affrontare:
\begin{itemize}[label=-]
\item il bilanciamento tra \textbf{esplorazione} ed \textbf{exploitation};
\item la difficoltà di stimare un \textbf{target non stazionario}.
\end{itemize}

\subsection{Il tradeoff esplorazione--exploitation}

\paragraph{Scelta dell’azione durante l’apprendimento}
Se si sta imparando $Q(i,a)$ e ci si trova nello stato $i$, una strategia di pura \textbf{exploitation} sceglierebbe l’azione che massimizza le stime correnti.  
Questa scelta è sensata solo quando le stime dei valori delle azioni sono sufficientemente accurate, condizione che tipicamente non vale all’inizio. Un’azione può apparire non buona perché:
\begin{itemize}[label=-]
\item il contributo immediato è stato stimato male;
\item conduce a uno stato molto buono che non è ancora stato esplorato.
\end{itemize}
Da qui nasce il problema fondamentale di bilanciare \textbf{exploitation} ed \textbf{esplorazione}.

\paragraph{Problema statico semplificato}
Si consideri un insieme finito $A$ di azioni, ciascuna associata a una ricompensa casuale $R(a)$. Se fossero noti i valori attesi $\nu(a)$, un decisore neutrale al rischio sceglierebbe l’azione con valore atteso massimo. Se invece si hanno solo stime rumorose $\hat{\nu}(a)$, la pura exploitation (massimizzare $\hat{\nu}$) può essere una strategia inadeguata.

\subsection{Multi-armed bandits}

\paragraph{Idea}
Un classico esempio è il \textbf{multi-armed bandit}: una slot machine con bracci diversi, associati a differenti distribuzioni di payoff.  
Con pochi campioni è facile mancare l’azione ottima, soprattutto quando l’azione con valore atteso più alto presenta anche una varianza elevata.

\subsection{Politiche randomizzate per l’esplorazione}

\paragraph{Compromesso tra estremi}
All’estremo opposto della pura exploitation, la pura esplorazione seleziona azioni a caso con probabilità uniformi. Occorre un compromesso, che porta a \textbf{politiche randomizzate}.

\paragraph{Approccio $\varepsilon$-greedy}
\textbf{Versione statica}: fissata una probabilità $\varepsilon$,
con probabilità $1-\varepsilon$ si seleziona l’azione più promettente, e con probabilità $\varepsilon$ si seleziona un’azione casuale.

\textbf{Versione dinamica}: una possibile raffinazione è rendere $\varepsilon$ dipendente dal tempo, ad esempio:
\[
\varepsilon(k)=\frac{c}{d+k},
\qquad \text{oppure} \qquad
\varepsilon(k)=d+\frac{c}{k}.
\]

\paragraph{Esplorazione di Boltzmann}
L’esplorazione può essere realizzata tramite una funzione soft-max che definisce la probabilità di scegliere l’azione $a\in A$:
\[
\pi(a)=\frac{\exp\!\big(\rho\,\hat{\nu}(a)\big)}{\sum_{a'\in A}\exp\!\big(\rho\,\hat{\nu}(a')\big)}.
\]
Quando $\rho=0$ si ha \textbf{pura esplorazione}, mentre per $\rho\to +\infty$ si tende alla \textbf{pura exploitation}.

\subsection{Non stazionarietà e smoothing esponenziale}

\paragraph{Media campionaria e aggiornamento incrementale}
Per stimare $\theta=\mathbb{E}[X]$ si usa la media campionaria, che soddisfa:
\[
\hat{\theta}(m)
=
\hat{\theta}(m-1)+\frac{1}{m}\Big(X(m)-\hat{\theta}(m-1)\Big).
\]
La correzione è smussata da $1/m$, che tende a zero all’aumentare di $m$.

\paragraph{Passo costante in contesti non stazionari}
In un contesto non stazionario si può mantenere costante la correzione:
\[
\boxed{
\hat{\theta}(m)=\hat{\theta}(m-1)+\alpha\Big(X(m)-\hat{\theta}(m-1)\Big)
=\alpha X(m)+(1-\alpha)\hat{\theta}(m-1)
}
\]
dove $\alpha\in(0,1)$ pesa nuova informazione e informazione passata.

\paragraph{Smoothing esponenziale e ruolo di $\alpha$}
Questa procedura è nota come \textbf{smoothing esponenziale}: le osservazioni più vecchie ricevono pesi \textbf{esponenzialmente decrescenti}.  
Il coefficiente $\alpha$ è detto \textbf{learning rate}, \textbf{smoothing coefficient} o \textbf{forgetting factor}.  
Un $\alpha$ grande produce aggiornamenti più reattivi e ``nervosi''; un $\alpha$ piccolo introduce più inerzia e filtraggio del rumore.

\paragraph{Learning rate decrescente}
Se la non stazionarietà è dovuta solo all’apprendimento (e non a variazioni della dinamica del sistema), si può usare una sequenza decrescente $\alpha(k)$ che soddisfa condizioni del tipo:
\[
\boxed{
\sum_{k=1}^{\infty}\alpha(k)=\infty,
\qquad
\sum_{k=1}^{\infty}\alpha(k)^2<\infty.
}
\]

\subsection{Apprendimento tramite differenze temporali e SARSA}

\paragraph{Operatore $T_\mu$ e punto fisso}
Si introduce l’operatore $T_\mu$:
\[
[T_\mu \tilde V](i)
=
\sum_{j\in S}\pi(i;\mu(i);j)\Big(h(i;\mu(i);j)+\gamma \tilde V(j)\Big),
\qquad i\in S,
\]
dove $\mu$ è una \textbf{politica stazionaria} e $\tilde V$ è una value function.  
Il valore $V_\mu$ della politica $\mu$ si ottiene trovando il \textbf{punto fisso} di $T_\mu$:
\[
T_\mu V_\mu = V_\mu.
\]

\paragraph{Punto fisso in termini di Q-factors della politica}
La stessa idea può essere riscritta in termini dei \textbf{Q-factors} associati alla politica stazionaria $\mu$:
\[
Q_\mu\big(i;\mu(i)\big)
=
\mathbb{E}\Big[
h\big(i;\mu(i);j\big)+\gamma Q_\mu\big(j;\mu(j)\big)
\Big],
\]
dove l’aspettativa è rispetto allo \textbf{stato successivo} $j$.

\subsection{Schema di punto fisso, smoothing e differenze temporali}

\paragraph{Riformulazione astratta}
Si consideri un’equazione di punto fisso:
\[
y = Hy,
\]
dove $y$ è un vettore e $H$ è un operatore.  
L’iterazione diretta $y^{(k)}=Hy^{(k-1)}$ non garantisce convergenza in generale e, inoltre, in pratica $H$ non è valutabile esattamente perché richiede aspettative non note.

\paragraph{Schema incrementale con learning rate}
L’equazione può essere riscritta come:
\[
y = y + \alpha(Hy-y),
\]
da cui lo schema iterativo:
\[
y^{(k)}=y^{(k-1)}+\alpha\big(Hy^{(k-1)}-y^{(k-1)}\big).
\]

\paragraph{Sostituzione dell’aspettativa con osservazioni}
Applicando \textbf{smoothing esponenziale} e sostituendo l’aspettativa con osservazioni campionate, si ottiene lo schema tipico dell’apprendimento statistico.

\paragraph{Differenza temporale e aggiornamento (SARSA)}
Quando si osserva una transizione da $i=s(k)$ a $j=s(k+1)$ applicando l’azione prescritta dalla politica $\mu$, la \textbf{differenza temporale} è:
\[
\boxed{
\delta(k)=\Big[h\big(i;\mu(i);j\big)+\gamma \hat Q_\mu^{(k-1)}\big(j;\mu(j)\big)\Big]
-\hat Q_\mu^{(k-1)}\big(i;\mu(i)\big)
}
\]
e l’aggiornamento dei Q-factors è:
\[
\boxed{
\hat Q_\mu^{(k)}\big(i;\mu(i)\big)=\hat Q_\mu^{(k-1)}\big(i;\mu(i)\big)+\alpha\,\delta(k).
}
\]
La differenza temporale svolge il ruolo della correzione $Hy-y$.  
Poiché si usano stime per ottenere nuove stime, il termine \textbf{bootstrapping} descrive questo tipo di procedura.

\paragraph{Significato di SARSA e natura on-policy}
L’algoritmo è noto come \textbf{SARSA} perché utilizza la sequenza
\textbf{(State, Action, Reward, State, Action)}: si osserva lo stato $i$, si seleziona l’azione $\mu(i)$, si osservano il reward e il nuovo stato $j$, e si considera l’azione $\mu(j)$ nel nuovo stato.  
SARSA è un approccio \textbf{on-policy}: si agisce secondo una politica e si apprende il valore di quella stessa politica, come richiesto nella \textbf{policy iteration}.

\paragraph{Esplorazione e policy iteration generalizzata (ottimistica)}
Servono due ingredienti:
\begin{itemize}[label=-]
\item una quantità adeguata di \textbf{esplorazione}, ad esempio tramite \textbf{$\varepsilon$-greedy} (con probabilità $\varepsilon(k)$ si devia dalla politica e si seleziona un’azione casuale);
\item l’interruzione dell’apprendimento prima della valutazione esatta della politica, seguita da un passo di \textbf{miglioramento} della politica.
\end{itemize}
Questo schema è detto \textbf{generalized} o \textbf{optimistic policy iteration}: si assume ottimisticamente che la politica corrente sia stata valutata con accuratezza sufficiente. Ciò può accelerare l’apprendimento, ma può essere dannoso per la convergenza.

\paragraph{Miglioramento greedy della politica}
Quando si dispone di un’approssimazione $\hat Q_\mu(i;a)$, si tenta di migliorare la politica con una scelta greedy:
\[
\boxed{
\tilde\mu(i)\in \arg\opt_{a\in A(i)} \hat Q_\mu(i;a),
\qquad i\in S.
}
\]
A differenza dell’iterazione esatta, non vi è garanzia che $\tilde\mu$ sia effettivamente migliorativa.

\subsection{Q-learning per MDP finiti}

\paragraph{Equazioni per i Q-factors ottimali}
Nel Q-learning si considera direttamente la forma DP basata sui \textbf{Q-factors ottimali}:
\[
Q(i;a)
=
\sum_{j\in S}\pi(i;a;j)\Big[h(i;a;j)+\gamma \opt_{a'\in A(j)}Q(j;a')\Big],
\qquad i\in S,\ a\in A(i),
\]
\[
V(i)=\opt_{a\in A(i)}Q(i;a).
\]
A differenza di SARSA, qui non si valuta una politica fissata $\mu$, ma si mira ai fattori ottimali.

\paragraph{Logica off-policy e scelta dell’azione}
In Q-learning la politica è \textbf{implicita} nei Q-factors: aggiornandoli continuamente, si continua a cambiare politica; in un certo senso, si usa una politica per impararne un’altra.  
Al passo $k$, nello stato $s(k)=i$, la scelta greedy è:
\[
a(k)\in \arg\opt_{a\in A(i)}\hat Q^{(k-1)}(i;a).
\]

\paragraph{Target campionato e aggiornamento con smoothing}
Dopo l’azione si osservano lo stato successivo $j=s(k+1)$ e il contributo immediato.  
Si costruisce un target campionato che bilancia breve e lungo periodo:
\[
\tilde q
=
h\big(i;a(k);j\big)+\gamma \opt_{a'\in A(j)}\hat Q^{(k-1)}(j;a').
\]
L’aggiornamento tramite smoothing esponenziale è:
\[
\boxed{
\hat Q^{(k)}\big(s(k);a(k)\big)
=
\alpha \tilde q + (1-\alpha)\hat Q^{(k-1)}\big(s(k);a(k)\big).
}
\]

\paragraph{Forma a differenze temporali}
La correzione può essere scritta come differenza temporale:
\[
\delta(k)
=
\Big[h\big(s(k);a(k);j\big)+\gamma \opt_{a'\in A(j)}\hat Q^{(k-1)}(j;a')\Big]
-\hat Q^{(k-1)}\big(s(k);a(k)\big),
\]
e l’aggiornamento riguarda solo la coppia stato--azione osservata.

\paragraph{Confronto SARSA vs Q-learning}
La differenza si coglie confrontando le correzioni:
\begin{itemize}[label=-]
\item \textbf{SARSA} è \textbf{on-policy} e riferito a una politica stazionaria incumbente $\mu$;
\item \textbf{Q-learning} è \textbf{off-policy} e la politica cambia mentre si aggiornano i Q-factors.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF 12 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1cm}
\textcolor{Indigo}{\section{CAP 12 -- Simulazione}}
\subsection{Simulazione coda M/M/1 e intervalli di confidenza (code 1)}
\paragraph{Contesto del problema}
Lo script simula una \textbf{coda M/M/1} (arrivi Poisson con tasso $\lambda$, tempi di servizio esponenziali con tasso $\mu$, un solo server) per valutare la bontà degli \textbf{intervalli di confidenza} costruiti sui tempi di attesa tramite \texttt{normfit}.\\
\hspace*{1.5em}Il sistema è \textbf{stabile} quando $\rho=\lambda/\mu<1$. Nel codice si imposta $\lambda=1$ e $\mu=1.1$, e si stampa il \textcolor{Indigo}{\textbf{valore teorico}} del tempo di attesa atteso, da usare come riferimento per confrontare gli intervalli ottenuti in simulazione.\\
\hspace*{1.5em}L’esperimento viene ripetuto \textbf{10 volte} per osservare la variabilità degli intervalli e verificare se risultano \textcolor{Indigo}{\textbf{coerenti}} tra loro (intervalli che si sovrappongono) e se includono il \textcolor{Indigo}{\textbf{vero valore}}.

\begin{lstlisting}[style=matlabStyle]
%% 1 - SIMULAZIONE CODA M/M/1
% Scopo: verificare la qualità degli intervalli di confidenza stimati via simulazione
% per il tempo di attesa medio in una coda M/M/1, usando normfit sui tempi di attesa.

rng 'default' % Inizializza il generatore di numeri casuali (riproducibilità)

inRate = 1;       % Tasso di arrivo: lambda
servRate = 1.1;   % Tasso di servizio: mu (stabilità se lambda/mu < 1)
                  % Nota: aumentando mu (es. 2.1) il sistema ha meno "memoria",
                  % la correlazione si riduce e gli intervalli risultano più affidabili.

% Valore teorico del tempo di attesa atteso (da confrontare con la simulazione)
fprintf(1,"Tempo di attesa atteso (teorico): %.4f\n\n",(inRate/servRate)/(servRate-inRate));

sampleSize = 600000; % Numero di clienti simulati per ciascuna replica

fprintf(1,"Intervalli di confidenza stimati:\n");
for k = 1:10 % Ripeto l'esperimento 10 volte per ottenere 10 intervalli di confidenza
    ww = MM1_Queue(inRate, servRate, sampleSize); % Tempi di attesa simulati (per tutti i clienti)

    % Stima di media e intervallo di confidenza assumendo (implicitamente) normalità e i.i.d.
    [~,~,ci] = normfit(ww);
    fprintf(1,"(%.4f, %.4f)\n",ci(1),ci(2));
end

% Funzione (annidata) per simulare la sequenza dei tempi di attesa in una M/M/1
function [waitTimes] = MM1_Queue(lambda, mu, howmany)
waitTimes = zeros(howmany,1); % Per convenzione, il primo cliente ha tempo di attesa 0

for j = 2:howmany
    % Tempi tra arrivi e tempi di servizio esponenziali
    intTime  = exprnd(1/lambda); % Inter-arrival time
    servTime = exprnd(1/mu);     % Service time

    % Ricorsione di Lindley:
    % il tempo di attesa del cliente j dipende dal tempo di attesa del cliente j-1
    waitTimes(j) = max(0, waitTimes(j-1) + servTime - intTime);
end
end

% Analisi diagnostiche sulla decima replica (ww dell'ultimo ciclo)

% Autocorrelazione: evidenzia dipendenza seriale nei tempi di attesa
figure;
autocorr(ww)

% Istogramma: controlla se la distribuzione empirica è compatibile con una normale
figure;
hist(ww)

% Media cumulata: verifica se/quanto la simulazione ha raggiunto il regime stazionario
figure;
t = (1:sampleSize)';
plot(t, cumsum(ww(t))./t)

% Osservazione: se gli intervalli non si sovrappongono, l'inferenza è instabile.
% Possibili cause principali:
% 1) I tempi di attesa non sono indipendenti: la correlazione porta a sottostimare la varianza.
% 2) La normalità non è una buona approssimazione per ww.
% 3) La stima può essere influenzata dal transitorio (warm-up) e non riflettere pienamente il regime.
\end{lstlisting}

\paragraph{Conclusioni}
Dalla simulazione emergono tre criticità legate all’uso diretto di \texttt{normfit(ww)} sui tempi di attesa, e i tre plot le rendono \textcolor{Indigo}{\textbf{molto evidenti}}.\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(i) Dipendenza seriale.}} Il grafico di autocorrelazione mostra una \textbf{correlazione molto alta} anche a lag elevati: i tempi di attesa non sono i.i.d., perché derivano dalla ricorsione di Lindley. Questo significa che \texttt{normfit} lavora con un’ipotesi strutturalmente sbagliata e tende a produrre intervalli \textbf{troppo ottimisti} (varianza sottostimata rispetto alla varianza “effettiva” con correlazione).\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(ii) Ipotesi di normalità non adeguata.}} L’istogramma dei tempi di attesa è fortemente asimmetrico (massa concentrata su valori piccoli e \textbf{coda lunga}): è difficile giustificare un fit normale per la distribuzione di \texttt{ww}. Di conseguenza, anche quando gli intervalli numericamente sembrano ragionevoli, l’inferenza “tipo normale” può risultare fuorviante.\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(iii) Effetti di transitorio.}} La media cumulata cresce rapidamente all’inizio e poi si assesta lentamente verso un valore stabile: questo indica un \textbf{warm-up non trascurabile}. Se non si gestisce il transitorio (ad es.\ eliminando i primi campioni), la stima della media e gli intervalli risultano influenzati dalla fase iniziale e possono non riflettere correttamente il regime stazionario.\\
\hspace*{1.5em}In sintesi, i plot mostrano che \textcolor{Indigo}{\textbf{correlazione elevata}}, \textcolor{Indigo}{\textbf{non-normalità}} e \textcolor{Indigo}{\textbf{transitorio}} sono presenti contemporaneamente: per questo gli intervalli prodotti da \texttt{normfit(ww)} non sono un indicatore affidabile della precisione della stima, anche quando il valore teorico appare “vicino” ai risultati osservati.

% Tre plot in una riga (sostituisci i nomi file con i tuoi)
\begin{figure}[H]
\centering
\begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[height=2.8cm]{Figures/1.pdf}
    \caption{Istogramma dei tempi di attesa}
\end{subfigure}\hfill
\begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[height=2.8cm]{Figures/2.pdf}
    \caption{Autocorrelazione dei tempi di attesa}
\end{subfigure}\hfill
\begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[height=2.8cm]{Figures/3.pdf}
    \caption{Media cumulata dei tempi di attesa}
\end{subfigure}

\caption{Diagnostiche sulla simulazione della coda M/M/1: distribuzione empirica, dipendenza seriale e convergenza al regime stazionario.}
\label{fig:mm1_diagnostics}
\end{figure}

\subsection{Simulazione (s,S) con revisione periodica: script \emph{extremely naive} (code 2)}

\paragraph{Contesto del problema}
Lo script implementa una simulazione \textbf{molto semplificata} di una politica di controllo scorte \textbf{(s,S) a revisione periodica}.\\
\hspace*{1.5em}L’obiettivo è capire se la politica sia \textcolor{Indigo}{\textbf{“buona”}} oppure no; tuttavia, in un problema di inventory, il concetto di bontà non è automatico: serve scegliere un \emph{criterio di valutazione} (ad esempio livello di servizio, vendite perse, costo di mantenimento, costo totale, ecc.).\\
\hspace*{1.5em}Nel modello si osserva il sistema su un orizzonte discreto $T$ e, a ogni periodo, si aggiorna il livello di scorta disponibile, si soddisfa (se possibile) la domanda e si decide se emettere un ordine: se la scorta finale scende sotto la soglia $s$, si ordina quanto basta per riportarsi a $S$.

\begin{lstlisting}[style=matlabStyle]
%% 2 - EXTREMELY NAIVE: simulazione politica (s,S) a revisione periodica
% Scopo: simulare una politica (s,S) e valutare (in modo qualitativo) se la scelta
% dei parametri è soddisfacente. Per giudicare "buona/cattiva" una policy serve però
% fissare una metrica (es. vendite perse, stockout, livello di servizio, costi).

% Parametri della policy (s,S)
smallS = 50;   % soglia di riordino: se l'inventario finale <= s allora riordino
bigS   = 100;  % livello target: dopo il riordino voglio tornare a S

% Parametri della domanda (assunzione: domanda ~ Normale)
mu    = 60;
sigma = 10;

% Orizzonte di simulazione e vettori di output
T = 50;                  % orizzonte temporale discreto
onHand    = zeros(T,1);  % scorta disponibile a inizio periodo (dopo arrivi)
aveOnHand = zeros(T,1);  % scorta media nel periodo (proxy per holding cost)
lostSales = zeros(T,1);  % vendite perse (assunzione: no backorder)
stockOut  = NaN(T,1);    % indicatore stockout (true/false)
ordered   = zeros(T,1);  % quantità ordinata a fine periodo

% Scenario di domanda (caso semplice per debug)
demand = repmat(10,T,1); % domanda costante = 10 per tutti i periodi

% Alternativa: domanda casuale normale (attenzione a domande negative se sigma è grande)
% demand = normrnd(mu,sigma,T,1);

% Stato iniziale del sistema
endInv  = bigS;  % inventario finale del periodo precedente (inizializzato a S)
onOrder = 0;     % ordine in arrivo dal periodo precedente (lead time = 1 periodo)

% Simulazione a tempo discreto (revisione periodica)
for t = 1:T
    % Arrivo dell'ordine emesso nel periodo precedente
    onHand(t) = endInv + onOrder;

    % Soddisfazione della domanda nel periodo t
    if demand(t) <= onHand(t)
        % nessuno stockout: domanda interamente soddisfatta
        stockOut(t)  = false;
        lostSales(t) = 0;
        endInv       = onHand(t) - demand(t);
    else
        % stockout: non posso soddisfare tutta la domanda
        stockOut(t)  = true;
        lostSales(t) = demand(t) - onHand(t);
        endInv       = 0;
    end

    % Scorta media nel periodo (utile per stimare un costo di holding)
    aveOnHand(t) = (onHand(t) + endInv) / 2;

    % Decisione di riordino a fine periodo secondo politica (s,S)
    if endInv <= smallS
        % sottosoglia: ordino per riportarmi a S
        onOrder    = bigS - endInv;
        ordered(t) = onOrder;
    else
        % sopra soglia: non ordino
        onOrder    = 0;
        ordered(t) = 0;
    end
end
\end{lstlisting}

\paragraph{Conclusioni}
Lo script produce una \textbf{traiettoria campionaria completa} del sistema di inventario sotto una politica \textbf{(s,S) a revisione periodica}, ma il suo output rimane \textcolor{Indigo}{\textbf{puramente descrittivo}} e non immediatamente interpretabile in termini di performance.\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(i) Esistenza di un output, ma non di una valutazione.}} La simulazione genera diverse serie temporali di interesse: livello di scorta disponibile (\texttt{onHand}), scorta media per periodo (\texttt{aveOnHand}), vendite perse (\texttt{lostSales}), indicatori di stockout (\texttt{stockOut}) e quantità ordinate (\texttt{ordered}). Tuttavia, nessuna di queste variabili viene aggregata in un indicatore sintetico che permetta di giudicare la bontà della policy.\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(ii) Interpretazione dei risultati con domanda costante.}} Nel caso mostrato (\texttt{demand = 10}), la domanda è molto inferiore al livello target $S=100$, per cui il sistema tende rapidamente a stabilizzarsi: non si osservano stockout e le vendite perse sono nulle. Questo comportamento è coerente, ma poco informativo, perché non mette sotto stress la politica (s,S).\\
\hspace*{1.5em}In sintesi, lo script ha valore didattico perché chiarisce la \textbf{meccanica della politica (s,S)} e la sua implementazione in tempo discreto, ma per diventare uno strumento decisionale deve essere esteso introducendo una \textcolor{Indigo}{\textbf{funzione obiettivo}} (ad esempio costo totale o livello di servizio medio) e scenari di domanda più realistici.


\subsection{Politica (s,S) a revisione periodica: simulazione su scenari e ottimizzazione dei parametri (code 3a, 3b, 3c)}

\paragraph{Contesto del problema}
Questa parte unifica tre blocchi coerenti: \textbf{(3a)} una funzione che simula una politica \textbf{(s,S)} su uno o più scenari di domanda, \textbf{(3b)} uno script di \textcolor{Indigo}{\textbf{ottimizzazione}} dei parametri $(s,S)$ minimizzando il \textbf{costo medio} stimato via simulazione, e \textbf{(3c)} un test su domanda costante per confrontare i risultati con un riferimento tipo \textbf{EOQ}.\\
\hspace*{1.5em}L’impostazione è quella tipica della \textcolor{Indigo}{\textbf{simulation--optimization}}: la funzione obiettivo non è disponibile in forma chiusa, ma viene \textbf{stimata} (costo totale) tramite simulazione; in seguito, la stima viene minimizzata con algoritmi di ricerca che non richiedono derivate (qui \texttt{patternsearch} e \texttt{surrogateopt}).\\
\hspace*{1.5em}Gli scenari di domanda vengono generati \textbf{all’esterno} della funzione per garantire modularità e confronti ripetibili tra soluzioni. Inoltre si distingue tra \textbf{in-sample} (usato per ottimizzare) e \textbf{out-of-sample} (utile per controllare la robustezza, anche se qui non impiegato direttamente).\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{Nota operativa.}} \texttt{SS\_Simulation} è una \textbf{funzione}: se viene eseguita direttamente dal Command Window senza argomenti (es.\ \texttt{>> SS\_Simulation}), MATLAB genera l’errore \emph{Not enough input arguments}. La funzione è pensata per essere chiamata da (3b--3c) tramite \texttt{objfun}, passando esplicitamente tutti gli input.

\begin{lstlisting}[style=matlabStyle]
%% 3a - Simulation of (s,S) periodic review policy (funzione)
% Scopo: simulare una policy (s,S) su più scenari di domanda, calcolando:
% - totalCost(k): costo totale della replica k (da usare in ottimizzazione)
% - freqStockout(k): frequenza di periodi con vendite perse (proxy servizio)

function [totalCost, freqStockout] = SS_Simulation(smallS, bigS, demandScenarios, ...
    initialStock, onHandCost, lostPenalty, unitCost, fixedCharge)

% Parametri della policy
% smallS, bigS    : soglia s e livello target S (con S >= s)
% demandScenarios : matrice scenari (righe = repliche, colonne = periodi)
% initialStock    : scorta iniziale (può influenzare il transitorio)

% Parametri economici
% onHandCost   : costo di giacenza per unità media in stock
% lostPenalty  : penalità per unità di domanda persa (vendite perse)
% unitCost     : costo unitario per unità ordinata
% fixedCharge  : costo fisso ogni volta che si effettua un ordine (setup)

% Nota: ogni riga di demandScenarios è una replica di durata T
[numScenarios, T] = size(demandScenarios);

% Output: un valore per ciascuno scenario
totalCost    = zeros(numScenarios,1);
freqStockout = zeros(numScenarios,1);

% Vettori di monitoraggio per una singola replica
onHand    = zeros(T,1);    % scorta disponibile a inizio periodo
aveOnHand = zeros(T,1);    % scorta media nel periodo
lostSales = zeros(T,1);    % vendite perse
stockOut  = NaN(T,1);      % true/false stockout
ordered   = zeros(T,1);    % quantità ordinata a fine periodo

% Ciclo sulle repliche (scenari)
for k = 1:numScenarios

    % Stato iniziale della replica k
    endInv  = initialStock;  % scorta finale iniziale
    onOrder = 0;             % nulla in transito (lead time = 1 periodo)
    demand  = demandScenarios(k,:);

    % Evoluzione temporale (revisione periodica)
    for t = 1:T
        % Arrivo dell'ordine emesso nel periodo precedente
        onHand(t) = endInv + onOrder;

        % Soddisfazione domanda (assunzione: vendite perse, no backorder)
        if demand(t) <= onHand(t)
            stockOut(t)  = false;
            lostSales(t) = 0;
            endInv       = onHand(t) - demand(t);
        else
            stockOut(t)  = true;
            lostSales(t) = demand(t) - onHand(t);
            endInv       = 0;
        end

        % Scorta media del periodo (per holding cost)
        aveOnHand(t) = (onHand(t) + endInv)/2;

        % Decisione di riordino secondo (s,S)
        if endInv <= smallS
            onOrder    = bigS - endInv;  % riordino per tornare a S
            ordered(t) = onOrder;
        else
            onOrder    = 0;
            ordered(t) = 0;
        end
    end

    % Costo totale della replica k
    totalCost(k) = onHandCost*sum(aveOnHand) + lostPenalty*sum(lostSales) + ...
        unitCost*sum(ordered) + fixedCharge*sum(ordered>0);

    % Frequenza stockout: frazione di periodi con lostSales > 0
    freqStockout(k) = sum(lostSales>0)/T;
end
\end{lstlisting}

\begin{lstlisting}[style=matlabStyle]
%% 3b - Optimization: Script to optimize (s,S) parameters

% Economics
unitCost = 10;
onHandCost = 0.1*unitCost;
lostPenalty = 2*unitCost; % penalità elevata per vendite perse
fixedCharge = 1000;

% Demand distribution (oggetto distribuzione)
pd = makedist('NegativeBinomial','R',20,'P',.3);

% Make inSample scenario
rng default
horizonInSample = 30000; % una replica molto lunga
inSampleDemand = random(pd,1,horizonInSample);

% Initial stock (scelta per ridurre l'effetto del transitorio)
initialStock = mean(pd)+2*std(pd);

% Objective function (minimizza costo medio simulato)
% Parametrizzazione: x(1)=s, x(2)=S-s così impongo S>=s
objfun = @(x) mean(SS_Simulation(x(1), x(1)+x(2), inSampleDemand, ...
    initialStock, onHandCost, lostPenalty, unitCost, fixedCharge));

upperBounds = [mean(pd)+10*std(pd);50*mean(pd)];

% Use pattern search
xStar = patternsearch(objfun,[2*mean(pd);5*mean(pd)],[],[],[],[],zeros(2,1),upperBounds);
smallS_Pattern = xStar(1);
bigS_Pattern = xStar(1)+xStar(2);
fprintf(1,'Pattern Search: smallS = %.2f, bigS = %.2f\n', smallS_Pattern, bigS_Pattern);

% Use surrogate opt
xStar = surrogateopt(objfun,zeros(2,1),upperBounds);
smallS_Surrogate = xStar(1);
bigS_Surrogate = xStar(1)+xStar(2);
fprintf(1,'Surrogate Opt: smallS = %.2f, bigS = %.2f\n', smallS_Surrogate, bigS_Surrogate);
\end{lstlisting}

\begin{lstlisting}[style=matlabStyle]
%% 3c - Test EOQ Script to optimize (s,S) parameters (domanda costante)

% Economics
unitCost = 10;
onHandCost = 0.1*unitCost;
lostPenalty = 2*unitCost;
fixedCharge = 10000;

% Make inSample scenario
rng default
horizonInSample = 80000;
D = 5; % domanda costante
inSampleDemand = D*ones(1,horizonInSample);

EOQ = sqrt(2*D*fixedCharge/onHandCost)
initialStock = EOQ;

% Create objfun
objfun = @(x) mean(SS_Simulation(x(1), x(1)+x(2), inSampleDemand, ...
    initialStock, onHandCost, lostPenalty, unitCost, fixedCharge));
upperBounds = [100*initialStock;100*initialStock];

% Use pattern search
xStar = patternsearch(objfun,upperBounds,[],[],[],[],zeros(2,1),upperBounds);

smallS_Pattern = xStar(1);
bigS_Pattern = xStar(1)+xStar(2);
fprintf(1,'Pattern Search: smallS = %.2f, bigS = %.2f\n', smallS_Pattern, bigS_Pattern);
\end{lstlisting}

\paragraph{Conclusioni}
Il blocco (3a--3c) realizza un flusso completo di \textcolor{Indigo}{\textbf{simulation--optimization}} per una policy (s,S).\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(i) Output della simulazione.}} La funzione \texttt{SS\_Simulation} produce due output utili: \textbf{costo totale} (vettore per replica) e \textbf{frequenza di stockout}. In (3b--3c) viene ottimizzata la \textbf{media} del costo totale, quindi l’output è direttamente trasformato in una funzione obiettivo minimizzabile.\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(ii) Interpretazione della figura di (3b).}} Il grafico di \texttt{patternsearch} mostra la riduzione del \textbf{best function value} lungo le iterazioni fino a stabilizzarsi: è l’evidenza che l’algoritmo sta esplorando lo spazio dei parametri e convergendo verso un valore di costo più basso. Coerentemente, nel Command Window vengono stampate le coppie $(s,S)$ trovate (Pattern Search e Surrogate Opt).\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(iii) Interpretazione del test (3c).}} Nel caso a domanda costante, \texttt{EOQ} fornisce una scala di riferimento per l’ordine “ragionevole”. L’ottimizzazione via \texttt{patternsearch} porta infatti a un $S$ dell’ordine di grandezza dell’EOQ (come si vede dall’output stampato), mentre $s$ può risultare molto piccolo: con domanda deterministica e costo fisso elevato, la logica è \textbf{riordinare raramente} ma in lotti grandi, riducendo il numero di ordini.\\
\hspace*{1.5em}In sintesi, (3a) definisce una metrica quantitativa, (3b) la minimizza su uno scenario stocastico in-sample e (3c) controlla che la soluzione sia coerente in un caso deterministico benchmark.

\begin{figure}[H]
\centering
\includegraphics[height=4.5cm]{Figures/4.pdf}
\caption{Andamento del \emph{best function value} durante l’ottimizzazione dei parametri $(s,S)$ tramite \texttt{patternsearch} nello scenario stocastico in-sample.}
\label{fig:ss_3b}
\end{figure}


\subsection{Coda M/M/1 con balking (FIFO): simulazione percentuale di clienti che rinunciano (code 4a, 4b)}

\paragraph{Contesto del problema}
Si considera una \textbf{coda M/M/1} (interarrivi e tempi di servizio esponenziali, tassi assegnati) con disciplina \textbf{FIFO} e un meccanismo di \textbf{balking}: quando un cliente arriva, decide se entrare o rinunciare in funzione della lunghezza della coda.\\
\hspace*{1.5em}La regola di balking è la seguente: il cliente \textcolor{Indigo}{\textbf{entra sicuramente}} se la coda è lunga meno di 10 clienti, entra con \textcolor{Indigo}{\textbf{probabilità 50\%}} se la lunghezza è tra 10 e 15 (estremi inclusi), e \textcolor{Indigo}{\textbf{rinuncia sicuramente}} se la coda è maggiore di 15.\\
\hspace*{1.5em}L’obiettivo della simulazione è stimare, su un numero fissato di clienti serviti, la \textcolor{Indigo}{\textbf{percentuale di clienti che rinunciano}} (ossia che non si mettono in coda).\\
\hspace*{1.5em}Il codice propone due implementazioni: \textbf{(4a)} una versione script “diretta” basata sulla simulazione a eventi discreti (arrivo/completamento), e \textbf{(4b)} una versione più pulita in forma di funzione, che incapsula lo \textcolor{Indigo}{\textbf{stato del sistema}} e separa la logica degli eventi tramite funzioni annidate.

\begin{lstlisting}[style=matlabStyle]
%% 4a - ESAME BA 03/07/2024
% Coda M/M/1 FIFO con balking: stimo quanti clienti rinunciano a entrare.
% Regola di balking:
% - se la lunghezza della coda e' < 10: il cliente entra sempre
% - se la lunghezza e' tra 10 e 15 (inclusi): entra con probabilita' 50%
% - se la lunghezza e' > 15: rinuncia sicuramente

%% VERSIONE 1 - Script M/M/1 con balking

% Tassi
arrivalRate = 1;
serviceRate = 1.4;

% Stato del sistema
numInQueue = 0; % numero nel sistema (incluso l'eventuale cliente in servizio)
count = 0;      % numero di completamenti di servizio
lost  = 0;      % numero di clienti persi per balking

% Tempi del prossimo evento (simulazione a eventi discreti)
nextCompletion = inf;                 % sistema inizialmente vuoto
nextArrival    = exprnd(1/arrivalRate);
toServe = 10000;                      % voglio servire questo numero di clienti

while count <= toServe

    % Evento di completamento: avviene se e' il prossimo tra i due eventi
    if nextCompletion <= nextArrival
        clock = nextCompletion;       % avanzo l'orologio
        count = count + 1;            % ho completato un servizio

        if numInQueue == 0
            % nessuno in coda: il server diventa idle == il server è libero, non sta servendo nessuno
            nextCompletion = inf;
        else
            % qualcuno in coda: parte subito il servizio del prossimo cliente
            numInQueue = numInQueue - 1;
            nextCompletion = clock + exprnd(1/serviceRate);
        end

    % Evento di arrivo
    else
        clock = nextArrival;          % avanzo l'orologio

        if numInQueue == 0
            % server idle: il cliente entra e inizia subito il servizio
            numInQueue = 1;
            nextCompletion = clock + exprnd(1/serviceRate);

        elseif numInQueue < 10
            % entra sicuramente
            numInQueue = numInQueue + 1;

        elseif numInQueue <= 15
            % entra con probabilita' 50% (altrimenti balking)
            if rand > 0.5
                lost = lost + 1;
            else
                numInQueue = numInQueue + 1;
            end

        else
            % coda troppo lunga: balking certo
            lost = lost + 1;
        end

        % Pianifico il prossimo arrivo
        nextArrival = clock + exprnd(1/arrivalRate);
    end
end

fprintf(1,'%d customers balked\n', lost);
\end{lstlisting}

\begin{lstlisting}[style=matlabStyle]
%% 4b - VERSIONE 2: funzione M/M/1 con balking (piu' modulare)
% Idea: incapsulo lo stato in una struct e separo la logica dei due eventi
% (completamento e arrivo) tramite funzioni annidate.

function lost = MM1_Balking(arrivalRate,serviceRate,toServe)

% Stato del sistema (incapsulato)
state.numInQueue = 0;   % numero nel sistema (include il cliente in servizio)
state.count      = 0;   % numero di completamenti
state.lost       = 0;   % clienti persi per balking
state.clock      = 0;   % tempo corrente

% Tempi del prossimo evento
state.nextCompletion = inf;
state.nextArrival    = exprnd(1/arrivalRate);

% Simulazione a eventi: ad ogni passo scelgo l'evento piu' vicino
while state.count <= toServe
    if state.nextCompletion <= state.nextArrival
        state.clock = state.nextCompletion;
        state = manageService(state);
    else
        state.clock = state.nextArrival;
        state = manageArrival(state);
    end
end

lost = state.lost;

% --- Funzioni annidate: aggiornano lo stato dato un evento ---

    function state = manageService(state)
        % Gestione completamento di servizio
        state.count = state.count + 1;

        if state.numInQueue == 0
            % nessuno in coda: server idle
            state.nextCompletion = inf;
        else
            % prendo il prossimo cliente in coda e schedulo il completamento
            state.numInQueue = state.numInQueue - 1;
            state.nextCompletion = state.clock + exprnd(1/serviceRate);
        end
    end

    function state = manageArrival(state)
        % Gestione arrivo con regola di balking
        if state.numInQueue == 0
            % server idle: servizio immediato
            state.numInQueue = 1;
            state.nextCompletion = state.clock + exprnd(1/serviceRate);

        elseif state.numInQueue < 10
            % entra sicuramente
            state.numInQueue = state.numInQueue + 1;

        elseif state.numInQueue <= 15
            % entra con probabilita' 50%
            if rand > 0.5
                state.lost = state.lost + 1;
            else
                state.numInQueue = state.numInQueue + 1;
            end

        else
            % balking certo
            state.lost = state.lost + 1;
        end

        % Pianifico il prossimo arrivo
        state.nextArrival = state.clock + exprnd(1/arrivalRate);
    end

end
\end{lstlisting}

\paragraph{Conclusioni}
Il codice implementa correttamente una simulazione a \textbf{eventi discreti} con due soli eventi possibili (arrivo/completamento), includendo una regola di balking dipendente dalla lunghezza della coda.\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(i) Output di interesse.}} La quantità \texttt{lost} rappresenta il numero di clienti che rinunciano; per ottenere la \textbf{percentuale} richiesta basta normalizzare rispetto agli arrivi totali simulati (oppure rispetto a \texttt{lost + served}, a seconda della definizione adottata).\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(ii) Struttura a eventi e stato.}} La versione (4b) è più pulita perché incapsula lo stato e separa la logica dei due eventi, rendendo più facile estendere il modello (ad esempio aggiungendo nuove regole o nuove metriche).\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(iii) Interpretazione del parametro di soglia.}} Le soglie 10 e 15 regolano direttamente il trade-off tra congestione e perdita di domanda: soglie più basse riducono la congestione ma aumentano i clienti persi, soglie più alte fanno l’opposto.\\
\hspace*{1.5em}In sintesi, la versione (4b) è preferibile per riuso e manutenzione, mentre la versione (4a) è utile come implementazione immediata e trasparente della logica di simulazione.

\subsection{Two-stage ATP/ATO: modello ai valori attesi vs modello stocastico (code 5a,5b)}

\paragraph{Contesto del problema}
Questi script confrontano due approcci per un problema di pianificazione in presenza di domanda incerta, formulato come \textbf{problema a due stadi}.\\
\hspace*{1.5em}Nel \textcolor{Indigo}{\textbf{primo stadio}} si decide quante componenti produrre (\texttt{make}), soggette a vincoli di capacità sulle risorse. Nel \textcolor{Indigo}{\textbf{secondo stadio}} si decide quante unità di prodotto finito vendere (\texttt{sell}) una volta osservata la domanda (che varia per scenario).\\
\hspace*{1.5em}Il codice implementa due strategie: (i) un \textbf{modello deterministico} basato sulla \textcolor{Indigo}{\textbf{domanda media}} (Expected Value, EV), e (ii) un \textbf{modello stocastico} (Recourse Problem, RP) che ottimizza il profitto atteso su più scenari.\\
\hspace*{1.5em}Nel blocco 5b, oltre alla soluzione EV e RP, viene calcolata la metrica \textcolor{Indigo}{\textbf{VSS}} (\emph{Value of the Stochastic Solution}), che quantifica il guadagno di usare il modello stocastico rispetto a “fissare” le decisioni di primo stadio ottenute dal modello ai valori attesi.

\begin{lstlisting}[style=matlabStyle]
%% 5a - Two Stages ATP (con intlinprog): modello ai valori attesi vs modello stocastico

% DATI DEL PROBLEMA
numItems     = 3;
numComp      = 5;
numScenarios = 3;
numResources = 3;

compCost = [20; 30; 10; 10; 10];   % costo componenti (primo stadio)
gozinto  = [ 1 1 1 0 0
             1 1 0 1 0
             1 1 0 0 1 ];          % matrice di distinta base (prodotti -> componenti)

needRes  = [ 1 2 1
             1 2 2
             2 2 0
             1 2 0
             3 2 0 ];              % consumo risorse per ogni componente

availRes = [800; 700; 600];        % disponibilita' risorse
endPrice = [80; 70; 90];           % prezzo di vendita dei prodotti

demand = [ 100  50 120
            50  25  60
           100 110  60 ];          % domanda per scenario (righe=prodotti, colonne=scenari)

probs = ones(numScenarios,1)/numScenarios;
demandAverage = mean(demand,2);

%% 1) MODELLO DETERMINISTICO AI VALORI ATTESI (EV) - variabili intere
% Variabili di decisione: [make; sell]
% Obiettivo: massimizzare profitto = ricavi - costi
% In intlinprog si minimizza, quindi cambio segno ai ricavi ( -endPrice )

objCoeff = [compCost; -endPrice];

% Bounds: componenti senza upper bound, vendite limitate dalla domanda media
lb = zeros(numComp+numItems,1);
ub = [repmat(Inf,numComp,1); demandAverage];

% Vincoli componenti: gozinto' * sell <= make
A1 = [-eye(numComp), gozinto'];
b1 = zeros(numComp,1);

% Vincoli risorse: needRes' * make <= availRes
A2 = [needRes', zeros(numResources,numItems)];
b2 = availRes;

A = [A1; A2];
b = [b1; b2];

% Risoluzione
[xVar, aux] = intlinprog(objCoeff,1:length(objCoeff),A,b,[],[],lb,ub);
profit  = -aux;
display(xVar);
display(profit);

%% 2) MODELLO STOCASTICO (RP) - variabili continue (secondo stadio per scenario)
% Variabili: [make; sell_1; ...; sell_S]
% Obiettivo: costi primo stadio + ricavi attesi sui vari scenari

objCoeff = [compCost];
for s = 1:numScenarios
    objCoeff = [objCoeff; -probs(s)*endPrice];
end

lb = zeros(numComp+numScenarios*numItems,1);
ub = [repmat(Inf,numComp,1); reshape(demand,numItems*numScenarios,1)];

% Matrice vincoli con struttura a blocchi (due stadi)
A = zeros(numResources+numScenarios*numComp, numComp+numScenarios*numItems);

% Vincoli risorse (uguali per tutti gli scenari)
A(1:numResources, 1:numComp) = needRes';

% Vincoli componenti per ciascuno scenario: gozinto' * sell_s <= make
baseRow = numResources;
baseCol = numComp;
for s = 1:numScenarios
    A(baseRow+(1:numComp), 1:numComp) = -eye(numComp);
    A(baseRow+(1:numComp), baseCol+(1:numItems)) = gozinto';
    baseRow = baseRow + numComp;
    baseCol = baseCol + numItems;
end

b = [availRes; zeros(numScenarios*numComp,1)];

% Risoluzione
[xVarS, aux] = intlinprog(objCoeff,1:length(objCoeff),A,b,[],[],lb,ub);
profitS = -aux;
display(xVarS);
display(profitS);
\end{lstlisting}

\begin{lstlisting}[style=matlabStyle]
%% 5b - In Two Stage ATO (con optimproblem): EV vs RP e calcolo VSS

format bank
typeVar = 'integer';      % soluzione con variabili intere
% typeVar = 'continuous'; % alternativa: rilassamento continuo

% DATI DEL PROBLEMA (stessi dati di 5a)
numItems     = 3;
numComp      = 5;
numScenarios = 3;
numResources = 3;

compCost = [20; 30; 10; 10; 10];
gozinto  = [ 1 1 1 0 0
             1 1 0 1 0
             1 1 0 0 1 ];
needRes  = [ 1 2 1
             1 2 2
             2 2 0
             1 2 0
             3 2 0 ];
availRes = [800; 700; 600];
endPrice = [80; 70; 90];

demand = [ 100  50 120
            50  25  60
           100 110  60 ];
probs = ones(numScenarios,1)/numScenarios;

%% 1) PROBLEMA AI VALORI ATTESI (EV)
demandAverage = mean(demand,2);

probEV = optimproblem('ObjectiveSense','max');
x = optimvar('x', numComp, 1, 'LowerBound', 0, 'Type', typeVar);                 % componenti
y = optimvar('y', numItems, 1, 'LowerBound', 0, 'UpperBound', demandAverage, ...
             'Type', typeVar);                                                  % prodotti finiti (media)

probEV.Objective = -dot(compCost,x) + dot(endPrice,y);
probEV.Constraints.cons1 = needRes' * x <= availRes;     % vincoli capacita'
probEV.Constraints.cons2 = gozinto' * y <= x;            % vincolo distinta base

showproblem(probEV)
[solEV, profitEV] = solve(probEV);

solEV.x
solEV.y
profitEV

%% 2) PROBLEMA STOCASTICO (RP) con ricorso
probRP = optimproblem('ObjectiveSense','max');

x = optimvar('x', numComp, 1, 'LowerBound', 0, 'Type', typeVar);                 % primo stadio
y = optimvar('y', numItems, numScenarios, 'LowerBound', 0, 'UpperBound', demand, ...
             'Type', typeVar);                                                  % secondo stadio per scenario

% Ricavo atteso: per ogni scenario calcolo endPrice'*y(:,s) e faccio media pesata
probRP.Objective = -dot(compCost,x) + dot(endPrice'*y, probs);

probRP.Constraints.cons1 = needRes' * x <= availRes;     % vincoli primo stadio

% Vincoli di componente per ogni scenario: gozinto' * y(:,s) <= x
compConstr = optimconstr(numScenarios,numComp);
for s = 1:numScenarios
    compConstr(s,:) = gozinto' * y(:,s) <= x;
end
probRP.Constraints.cons2 = compConstr;

showproblem(probRP)
[solRP, profitRP] = solve(probRP);

solRP.x
solRP.y
profitRP

%% 3) VALUTAZIONE DELLA SOLUZIONE EV NEL MODELLO STOCASTICO (calcolo VSS)
% Fisso la decisione di primo stadio alla soluzione EV e ricalcolo il profitto
probRP.Constraints.fixFirstStage = x == solEV.x;
[solRP2, profitRP2] = solve(probRP);

solRP2.y
profitRP2

% VSS: valore aggiunto della soluzione stocastica rispetto alla EV "valutata" su scenari
VSS = profitRP - profitRP2
format
\end{lstlisting}

\paragraph{Conclusioni}
I blocchi (5a--5b) mostrano chiaramente la differenza tra un modello ai \textbf{valori attesi} (EV) e un modello \textbf{stocastico a due stadi} (RP), e quantificano il beneficio di considerare l’incertezza tramite la metrica \textcolor{Indigo}{\textbf{VSS}}.\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(i) Soluzione EV (domanda media).}} Nel caso EV, il solver trova una soluzione ottima con profitto pari a \textbf{3220}. La decisione di primo stadio risulta \texttt{x} = (116,116,26,0,90) e la decisione di vendita sulla domanda media è \texttt{y} = (26,0,90). Questo conferma che il modello EV ottimizza rispetto a una domanda “aggregata”, ignorando esplicitamente la variabilità tra scenari.\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(ii) Soluzione stocastica RP.}} Nel modello a due stadi (RP), la soluzione ottima produce un profitto atteso pari a \textbf{2883.33}, con decisione di primo stadio \texttt{x} = (115,115,55,0,65). Al secondo stadio, le vendite \texttt{y} dipendono dallo scenario e si adattano ai diversi vincoli di domanda: questo è esattamente l’effetto del \textbf{ricorso} (si decide cosa vendere dopo aver osservato lo scenario).\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(iii) Valutazione della soluzione EV nel modello stocastico e VSS.}} Fissando nel problema stocastico la decisione di primo stadio uguale a quella EV (\texttt{x == solEV.x}), il profitto ottenuto scende a \textbf{2320}. La differenza tra la soluzione pienamente stocastica e la soluzione EV “trapiantata” nel modello a scenari è la \textcolor{Indigo}{\textbf{VSS}}, che nei risultati vale:
\(
\text{VSS} = 2883.33 - 2320 = \textbf{563.33}.
\)
Questo valore misura il guadagno ottenibile scegliendo \textbf{direttamente} una soluzione ottimizzata per scenari, anziché ottimizzare su domanda media e poi subire le conseguenze della variabilità.\\
\hspace*{1.5em}In sintesi, anche se il profitto EV (\textbf{3220}) è più alto nel suo modello deterministico “ideale”, quando si passa a valutare su scenari la soluzione EV perde robustezza (profitto \textbf{2320}), mentre la soluzione RP è costruita per gestire l’incertezza e garantisce un profitto atteso maggiore in quel contesto (\textbf{2883.33}), con un vantaggio quantificato da \textbf{VSS = 563.33}.


\subsection{Capacity control e tariffe aeree: protezione della classe alta e simulazione degli arrivi (code 6)}

\paragraph{Contesto del problema}
Lo script modella un problema di \textcolor{Indigo}{\textbf{revenue management}} per un volo con capacità limitata: esistono due classi tariffarie con prezzi diversi (classe 1 più costosa, classe 2 più economica) e le richieste arrivano nel tempo secondo processi di Poisson.\\
\hspace*{1.5em}L’obiettivo è applicare una regola semplice di \textcolor{Indigo}{\textbf{protezione}} (booking limit): si riserva un certo numero di posti alla classe più remunerativa, rifiutando parte delle richieste della classe 2 quando il livello di protezione è stato raggiunto.\\
\hspace*{1.5em}Il punto chiave è che la decisione viene presa usando tassi di arrivo \textbf{stimati} ($\hat{\lambda}_1,\hat{\lambda}_2$), mentre la simulazione degli arrivi avviene usando i tassi \textbf{veri} ($\lambda_1,\lambda_2$): in questo modo si evidenzia l’impatto dell’errore di stima sul profitto.\\
\hspace*{1.5em}Il \textbf{livello di protezione} viene calcolato tramite un quantile Poisson: si usa \texttt{poissinv} sul numero atteso di arrivi della classe 1 nell’intero orizzonte $T$.

\begin{lstlisting}[style=matlabStyle]
%% 6 - Aircraft capacity and tariffs
% Scopo: gestire la capacita' (posti) di un volo con due classi tariffarie.
% Applico una regola di protezione per la classe alta: accetto la classe 2
% solo finche' non "invado" i posti riservati alla classe 1.

% Dati del problema
cap    = 100;   % capacita' totale (posti)
price1 = 400;   % prezzo classe 1 (alta)
price2 = 200;   % prezzo classe 2 (bassa)

% Orizzonte temporale (giorni fino alla partenza)
T = 60;

% Tassi di arrivo (per giorno): veri e stimati
trueArrivalRate1 = 0.4;  % tasso vero classe 1
trueArrivalRate2 = 2.2;  % tasso vero classe 2

hatArrivalRate1  = 0.5;  % tasso stimato classe 1 (usato per decidere)
hatArrivalRate2  = 2;    % tasso stimato classe 2 (qui non entra nella regola)

% Calcolo del livello di protezione (booking limit)
% Nota: il numero di richieste classe 1 sull'orizzonte T e' Poisson con media hatArrivalRate1*T.
% Uso un quantile: protezioneLevel = quantile_{1 - price2/price1}(Poisson(hatArrivalRate1*T))
protectionLevel = poissinv(1 - price2/price1, hatArrivalRate1*T);

% Stato del sistema
count1 = 0;   % numero di richieste accettate per classe 1
count2 = 0;   % numero di richieste accettate per classe 2
full   = false;

% Tempi del prossimo arrivo per le due classi (simulati sui tassi veri)
nextArrival1 = exprnd(1/trueArrivalRate1);
nextArrival2 = exprnd(1/trueArrivalRate2);

% Condizione di stop: nessun arrivo prima della partenza
stop = min(nextArrival1, nextArrival2) > T;

% Simulazione a eventi discreti: arrivi di classe 1 e classe 2
% Il loop termina quando (i) non arrivano piu' richieste prima della partenza oppure (ii) l'aereo e' pieno
while ~stop && ~full

    if nextArrival1 < nextArrival2
        % Prossimo evento: arrivo classe 1
        clock = nextArrival1;

        % Accetto sempre classe 1 (scelta coerente con la regola di protezione)
        count1 = count1 + 1;

        % Pianifico il prossimo arrivo classe 1
        nextArrival1 = clock + exprnd(1/trueArrivalRate1);

    else
        % Prossimo evento: arrivo classe 2
        clock = nextArrival2;

        % Accetto classe 2 solo se non supero il booking limit
        % In pratica: lascio almeno protectionLevel posti "liberi" per la classe 1
        if count2 < cap - protectionLevel
            count2 = count2 + 1;
        end

        % Pianifico il prossimo arrivo classe 2
        nextArrival2 = clock + exprnd(1/trueArrivalRate2);
    end

    % Controllo capacita'
    if count1 + count2 >= cap
        full = true;
    end

    % Aggiorno stop: la partenza avviene al tempo T
    stop = min(nextArrival1, nextArrival2) > T;
end

% Profitto totale realizzato
fprintf(1,'Total profit %d\n', count1*price1 + count2*price2);

% POSSIBILI MIGLIORAMENTI
% 1) Una volta raggiunto il booking limit per la classe 2, potrei ignorare i suoi arrivi
%    e campionare direttamente il numero di arrivi classe 1 con una Poisson.
% 2) Incapsulare in una funzione (con rate generici) per fare molte repliche e stimare media/varianza del profitto.
% 3) Confrontare i risultati anche con un modello statico "ideale" (senza simulazione).
% 4) La scelta del quantile Poisson puo' essere discutibile: quale quantile e' piu' corretto e perche'?
\end{lstlisting}

\paragraph{Conclusioni}
Lo script implementa una regola di controllo capacità basata su un \textbf{livello di protezione} per la classe alta e stima il profitto realizzato tramite simulazione a eventi discreti.\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(i) Protezione e trade-off ricavo.}} La protezione riduce il rischio di “sprecare” capacità vendendola troppo presto alla classe economica, ma può anche generare rifiuti e posti vuoti se la domanda di classe 1 non si materializza.\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(ii) Errore di stima.}} Il livello di protezione viene deciso con tassi stimati: se $\hat{\lambda}_1$ è troppo alto si rischia di rifiutare troppa classe 2; se è troppo basso si rischia di saturare con classe 2 e poi non avere spazio per classe 1.\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(iii) Struttura del simulatore.}} Il modello è un simulatore a eventi con due processi di arrivo indipendenti; termina quando l’aereo è pieno oppure quando si supera l’orizzonte $T$.\\
\hspace*{1.5em}In sintesi, il codice è un primo prototipo utile per studiare l’effetto del booking limit sul profitto, ma per un’analisi affidabile servono più repliche, confronto con benchmark (statico/dinamico) e una riflessione sulla scelta del quantile usato per la protezione.


\subsection{Roll queue: simulazione a eventi discreti con buffer e domanda batch (code 7)}

\paragraph{Contesto del problema}
Lo script simula un sistema di tipo \textbf{queueing/inventory} in cui si preparano “roll” (panini) uno alla volta e i clienti arrivano nel tempo richiedendo un numero intero di roll.\\
\hspace*{1.5em}La produzione dei roll avviene con tempi di preparazione \textbf{uniformi} tra \texttt{lowTime} e \texttt{highTime}. I roll pronti vengono accumulati in un \textbf{buffer} con capacità massima \texttt{maxPlaces}: quando il buffer è pieno, la produzione viene \textcolor{Indigo}{\textbf{bloccata}} (non si preparano ulteriori roll finché non si libera spazio).\\
\hspace*{1.5em}I clienti arrivano con tempi tra arrivi \textbf{esponenziali} (media \texttt{meanInterTime}) e ogni cliente richiede una quantità casuale di roll (\texttt{demand}) distribuita \textbf{uniformemente discreta} tra 1 e \texttt{maxDemand}.\\
\hspace*{1.5em}Se al momento dell’arrivo c’è già una coda (clienti in attesa) si entra in coda; se non c’è coda, il cliente prova a soddisfare immediatamente la domanda con il buffer disponibile. La performance misurata è il \textcolor{Indigo}{\textbf{tempo medio di attesa}} dei clienti serviti.

\begin{lstlisting}[style=matlabStyle]
%% 7 - Roll queue (Es. 3 tema 03/2024)
% Unita' di tempo: 1 minuto
% Idea: i roll vengono preparati (uno alla volta) e accumulati in un buffer finito.
% I clienti arrivano e richiedono una quantita' intera di roll (domanda batch).
% Se il buffer e' pieno, la produzione si blocca finche' non si libera spazio.

% Parametri di preparazione roll (Uniforme)
lowTime  = 1.5;
highTime = 2;

% Buffer
maxPlaces = 6;    % capacita' massima del buffer (roll pronti)

% Arrivi clienti e domanda
meanInterTime = 4;  % media dei tempi tra arrivi (Esponenziale)
maxDemand = 3;      % domanda massima per cliente (Uniforme discreta 1..maxDemand)

% Contatori
count   = 0;      % clienti serviti
toServe = 1000;   % numero di clienti da servire

% Stato del sistema
clock = 0;

% Prossimi eventi (due processi indipendenti)
nextArrival = exprnd(meanInterTime);         % prossimo arrivo cliente
nextRoll    = unifrnd(lowTime,highTime);     % prossimo completamento roll

buffer  = 3;       % stato iniziale: roll gia' pronti
blocked = false;   % se true: produzione bloccata per buffer pieno

% Coda clienti (traccio: tempo di ingresso e domanda residua)
joinTime       = [];   % istante di ingresso in coda di ciascun cliente
residualDemand = [];   % domanda residua per ciascun cliente in coda

% Statistica: tempo totale di attesa (solo per clienti che effettivamente attendono)
totalWaitingTime = 0;

while count <= toServe

    % Evento 1: completamento di un roll (se avviene prima del prossimo arrivo)
    if nextRoll <= nextArrival
        clock = nextRoll;

        if length(residualDemand) > 0
            % C'e' almeno un cliente in coda: uso il roll appena pronto per servire il primo
            residualDemand(1) = residualDemand(1) - 1;

            if residualDemand(1) == 0
                % Cliente completato: aggiorno statistiche e rimuovo dalla coda
                totalWaitingTime = totalWaitingTime + (clock - joinTime(1));
                count = count + 1;

                joinTime(1) = [];
                residualDemand(1) = [];
            end

        else
            % Nessun cliente in attesa: metto il roll nel buffer
            buffer = buffer + 1;
        end

        % Gestione del blocco: se buffer pieno, fermo la produzione
        if buffer == maxPlaces
            blocked  = true;
            nextRoll = inf;   % nessun altro completamento finche' non si sblocca
        else
            nextRoll = clock + unifrnd(lowTime,highTime);
        end

    % Evento 2: arrivo di un cliente
    else
        clock = nextArrival;
        nextArrival = clock + exprnd(meanInterTime);

        % Domanda del cliente (Uniforme discreta)
        demand = unidrnd(maxDemand);

        if length(residualDemand) > 0
            % Se c'e' gia' coda, il cliente entra in fondo
            residualDemand(end+1) = demand;
            joinTime(end+1) = clock;

        else
            % Se non c'e' coda, provo a servire subito con il buffer
            if buffer >= demand
                buffer = buffer - demand;
                count = count + 1;  % attesa nulla
            else
                % Buffer insufficiente: consumo tutto il buffer e il resto diventa attesa
                demand = demand - buffer;
                buffer = 0;

                residualDemand(1) = demand;
                joinTime(1) = clock;
            end

            % Se la produzione era bloccata e ho liberato spazio, la riattivo
            if blocked
                blocked  = false;
                nextRoll = clock + unifrnd(lowTime,highTime);
            end
        end
    end
end

fprintf(1,'average waiting time = %.2f\n', totalWaitingTime/count);
\end{lstlisting}

\paragraph{Conclusioni}
Lo script costruisce un simulatore a \textbf{eventi discreti} con due processi concorrenti (arrivi clienti e completamenti di roll) e un \textbf{buffer finito} che può bloccare la produzione.\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(i) Interazione tra coda e buffer.}} Se non ci sono clienti in attesa, i roll vengono accumulati; se arrivano clienti con domanda batch, il buffer può azzerarsi rapidamente e generare attesa (domanda residua).\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(ii) Meccanismo di blocco.}} Quando il buffer è pieno, la produzione viene fermata mettendo \texttt{nextRoll = inf}; la produzione riparte solo quando un cliente consuma roll e libera spazio (\texttt{blocked} torna \texttt{false}).\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(iii) Misura di performance.}} Il tempo medio di attesa viene stimato tramite la somma dei tempi in coda (\texttt{clock - joinTime}) per i clienti che attendono. I clienti serviti immediatamente contribuiscono con attesa nulla.\\
\hspace*{1.5em}In sintesi, il modello è utile per studiare l’effetto combinato di \textbf{capacità del buffer}, \textbf{variabilità di produzione} e \textbf{domanda batch} sul congestionamento e sui tempi di attesa medi.

\subsection{Cutting stock: formulazione simmetrica e generazione di colonne (code 8a, 8b, 8c)}

\paragraph{Contesto del problema}
Il \textbf{cutting stock problem} consiste nel tagliare rotoli (o tronchi) di lunghezza fissa
in pezzi di lunghezze predefinite per soddisfare una domanda assegnata, minimizzando
il numero di rotoli utilizzati (o, in modo equivalente, lo spreco totale).\\
\hspace*{1.5em}Il problema è noto per la sua elevata complessità combinatoria e per la presenza
di numerose simmetrie, che rendono rapidamente impraticabili formulazioni dirette
quando la dimensione dell’istanza cresce.\\
\hspace*{1.5em}Vengono confrontati tre approcci:
\textbf{(8a)} una formulazione compatta e simmetrica di tipo Kantorovich,
\textbf{(8b)} una risoluzione tramite \textcolor{Indigo}{\textbf{column generation}}
con approccio \emph{problem-based},
e \textbf{(8c)} la stessa tecnica di column generation implementata in versione
\emph{solver-based}.  
Le versioni (8b) e (8c) risolvono lo \textbf{medesimo modello matematico},
ma differiscono per livello di astrazione e controllo sul solver.

\begin{lstlisting}[style=matlabStyle]
%% 8a - Cutting stock: formulazione simmetrica (Kantorovich) + symmetry breaking

% Generazione casuale di un'istanza
rng 'default'
rollLength = 1000;
numLenghts = 8;
lenghts = randsample(10:800, numLenghts);
demand  = 20 + unidrnd(150, numLenghts, 1);

% Stima pessimistica del numero massimo di rotoli
fudgeFactor = 2;
numRolls = ceil(fudgeFactor*(dot(lenghts,demand)/rollLength));

% Formulazione simmetrica
cutStock = optimproblem("ObjectiveSense","minimize");

% delta(j)=1 se il rotolo j viene usato
delta  = optimvar("delta", numRolls, 1, "Type","integer", ...
                  "LowerBound",0, "UpperBound",1);

% numCut(i,j)=pezzi di tipo i tagliati dal rotolo j
numCut = optimvar("numCut", numLenghts, numRolls, ...
                  "Type","integer", "LowerBound",0);

cutStock.Objective = sum(delta,'all');
cutStock.Constraints.meetDemand = sum(numCut,2) >= demand;
cutStock.Constraints.useLog = lenghts*numCut <= rollLength*delta';

fprintf(1,"Running symmetric model formulation\n")
solve(cutStock);

% Vincoli di symmetry breaking: uso ordinato dei rotoli
breaking = optimconstr(numRolls-1,1);
for k = 1:(numRolls-1)
    breaking(k) = delta(k+1) <= delta(k);
end
cutStock.Constraints.breaking = breaking;

fprintf(1,"Running symmetry breaking model formulation\n")
solve(cutStock);
\end{lstlisting}

\paragraph{Osservazioni su (8a)}
La formulazione simmetrica è intuitiva e aderente alla descrizione del problema,
ma introduce un numero molto elevato di variabili e soffre di \textcolor{Indigo}{\textbf{simmetria}}:
rotoli indistinguibili generano molte soluzioni equivalenti.  
I vincoli di symmetry breaking migliorano le prestazioni, ma la scalabilità rimane limitata,
come evidenziato dai tempi di risoluzione e dal numero di nodi esplorati.

\begin{lstlisting}[style=matlabStyle]
%% 8b - Cutting stock: column generation (problem-based)

logLength  = 40;
lengthlist = [8; 12; 16; 20];
quantity   = [90; 111; 55; 30];
nLengths   = length(lengthlist);

% Pattern iniziali (uno per ciascuna lunghezza)
patterns  = diag(floor(logLength ./ lengthlist));
nPatterns = size(patterns,2);

% Sottoproblema di generazione pattern
subproblem = optimproblem();
cuts = optimvar('cuts', nLengths, 1, 'Type','integer', ...
                'LowerBound', zeros(nLengths,1));
subproblem.Constraints = dot(lengthlist,cuts) <= logLength;

lpopts = optimoptions('linprog','Display','off');
ipopts = optimoptions('intlinprog',lpopts);

reducedCost = -inf;
reducedCostTolerance = -0.0001;
exitflag = 1;

while reducedCost < reducedCostTolerance && exitflag > 0
    logprob = optimproblem('Description','Cut Logs');
    x = optimvar('x', nPatterns, 1, 'LowerBound', 0);

    logprob.Objective.logsUsed = sum(x);
    logprob.Constraints.Demand = patterns*x >= quantity;

    [values,nLogs,exitflag,~,lambda] = solve(logprob,'options',lpopts);

    if exitflag > 0
        fprintf('Using %g logs\n',nLogs);
        subproblem.Objective = 1 - dot(lambda.Constraints.Demand,cuts);
        [values,reducedCost,pexitflag] = solve(subproblem,'options',ipopts);
        newpattern = round(values.cuts);

        if pexitflag > 0 && reducedCost < reducedCostTolerance
            patterns = [patterns newpattern];
            nPatterns = nPatterns + 1;
        end
    end
end

x.Type = 'integer';
[values,logsUsed] = solve(logprob,'options',ipopts);
values.x = round(values.x);
fprintf('Optimal solution uses %g logs\n',sum(values.x));
\end{lstlisting}

\begin{lstlisting}[style=matlabStyle]
%% 8c - Cutting stock: column generation (solver-based)

% Dati del problema: tronchi/rotoli tutti della stessa lunghezza
logLength  = 40;
lengthlist = [8; 12; 16; 20];     % lunghezze dei pezzi richiesti
quantity   = [90; 111; 55; 30];   % domanda per ciascuna lunghezza
nLengths   = length(lengthlist);

% Pattern iniziali "banali": per ogni lunghezza taglio il massimo numero possibile
% (matrice diagonale: ogni colonna = un pattern diverso)
patterns  = diag(floor(logLength ./ lengthlist));
nPatterns = size(patterns,2);

% Sottoproblema (pricing): genera un nuovo pattern risolvendo un ILP
% variabili = numero di pezzi di ciascuna lunghezza nel nuovo pattern
lb2 = zeros(nLengths,1);
A2  = lengthlist';  % vincolo di capacità: somma(length_i * cuts_i) <= logLength
b2  = logLength;

% Opzioni solver (silenzia output)
lpopts = optimoptions('linprog','Display','off');
ipopts = optimoptions('intlinprog',lpopts);

% Inizializzazione criterio di arresto della column generation
reducedCost = -Inf;
reducedCostTolerance = -0.0001; % aggiungo una colonna solo se costo ridotto < 0 (con tolleranza)
exitflag = 1;

% Column generation: alterno
% (1) master LP (con pattern correnti) e (2) pricing ILP (nuovo pattern)
while reducedCost < reducedCostTolerance && exitflag > 0

    % Master LP: min sum(x) s.t. patterns*x >= quantity, x >= 0
    lb = zeros(nPatterns,1);
    f  = lb + 1;          % costo unitario di ogni pattern: 1 (minimizza numero di tronchi)
    A  = -patterns;       % trasformo >= in <= per linprog
    b  = -quantity;

    [values,nLogs,exitflag,~,lambda] = linprog(f,A,b,[],[],lb,[],lpopts);

    if exitflag > 0
        fprintf('Using %g logs\n',nLogs);

        % Pricing problem: cerco un pattern con costo ridotto negativo
        % Nel solver-based uso direttamente i moltiplicatori duali del master LP:
        % f2 = -lambda, così il sottoproblema cerca di minimizzare il costo ridotto
        f2 = -lambda.ineqlin;

        [values,reducedCost,pexitflag] = intlinprog(f2,1:nLengths,A2,b2,[],[],lb2,[],ipopts);

        % Il costo ridotto effettivo è: 1 + reducedCost (perché nel master ogni colonna ha costo 1)
        reducedCost = 1 + reducedCost;
        newpattern = round(values); % pattern trovato (intero)

        % Se esiste un pattern migliorativo (costo ridotto < 0), lo aggiungo come nuova colonna
        if pexitflag > 0 && reducedCost < reducedCostTolerance
            patterns  = [patterns newpattern];
            nPatterns = nPatterns + 1;
        end
    end
end
\end{lstlisting}

\paragraph{Conclusioni}
Il blocco (8a--8c) evidenzia tre livelli di modellazione dello stesso problema.\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(i) Formulazione simmetrica (8a).}}
È concettualmente semplice e utile per comprendere il problema,
ma soffre di forti simmetrie e di scarsa scalabilità.\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(ii) Column generation (8b--8c).}}
Evita l’enumerazione esplicita dei pattern e costruisce la soluzione
in modo incrementale tramite un master LP e un sottoproblema ILP,
seguendo un criterio di arresto analogo a quello del metodo del simplesso.\\
\hspace*{1.5em}\textcolor{Indigo}{\textbf{(iii) Problem-based vs solver-based.}}
Le versioni (8b) e (8c) implementano lo stesso algoritmo di column generation:
la prima privilegia chiarezza e modularità, la seconda offre maggiore controllo
sulle strutture matriciali e sul solver. I risultati coincidono,
confermando l’equivalenza matematica delle due implementazioni.\\
\hspace*{1.5em}In sintesi, (8a) è utile a fini didattici,
mentre (8b--8c) rappresentano l’approccio standard ed efficiente
per istanze realistiche di cutting stock.




\end{document}