\documentclass[a4paper,12pt]{article}

% Preambolo
\usepackage[utf8]{inputenc}  % Supporto per caratteri UTF-8
\usepackage[margin=1cm,includefoot]{geometry}  % Margini ridotti (1 cm su tutti i lati)
\usepackage{titlesec}  % Personalizzazione dei titoli
\usepackage{setspace}  % Controllo della spaziatura
\usepackage{parskip}   % Evita indentazioni, aggiunge spazio tra i paragrafi
\usepackage{enumitem}  % Per personalizzare gli elenchi
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{amsmath,amssymb}
\usepackage{amsmath}

% Operatore "opt" (max o min a seconda del contesto)
\DeclareMathOperator*{\opt}{opt}

\usepackage{graphicx}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[dvipsnames,svgnames]{xcolor}

% ------------------------------
% INDICE: solo section, senza titolo
% ------------------------------
\setcounter{tocdepth}{1}      % mostra solo \section
\renewcommand{\contentsname}{} % rimuove "Contents"

% Personalizzazione del titolo in alto
\makeatletter
\renewcommand{\maketitle}{
    \begin{center}
        \vspace{-2cm}
        {\LARGE \textbf{\@title}} \\[-0.2cm]
    \end{center}
}


% Impostazioni per i numeri di pagina
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[R]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

% Dettagli del documento
\title{Business Analytics}
\date{}

\begin{document}

\maketitle
\vspace{-1.2cm}
\tableofcontents

\vspace{-1cm}
\textcolor{NavyBlue}{\section{CAP 1 -- Introduzione alle decisioni in condizioni di incertezza}}

\paragraph{Recap}
Le decisioni di business si collocano spesso in contesti caratterizzati da \textcolor{NavyBlue}{\textbf{incertezza}}, rendendo insufficiente una semplice previsione puntuale delle variabili rilevanti. Le tre componenti della \textcolor{NavyBlue}{\textbf{Business Analytics}} chiariscono ruoli diversi ma complementari: la descrittiva analizza ciò che è accaduto, la predittiva costruisce modelli per descrivere il futuro, mentre la prescrittiva individua \textbf{azioni ottimali} tenendo conto delle conseguenze economiche.

La qualità di una previsione dipende dalla \textcolor{NavyBlue}{\textbf{funzione di perdita}} associata all’errore. Penalità diverse conducono a previsioni ottimali diverse: il valore atteso è appropriato solo in presenza di costi simmetrici, mentre penalità asimmetriche portano naturalmente a soluzioni basate su \textbf{quantili}. Non esiste quindi una previsione universalmente ottima, ma solo scelte coerenti con il criterio decisionale adottato.

Nei problemi decisionali l’obiettivo diventa la \textcolor{NavyBlue}{\textbf{scelta di un vettore di decisioni}} sotto incertezza. Quando i fattori di rischio non sono probabilisticamente modellabili si adotta un approccio \textcolor{NavyBlue}{\textbf{robusto}}, basato su scenari peggiori; quando invece è disponibile una distribuzione di probabilità si ottiene un \textcolor{NavyBlue}{\textbf{problema di ottimizzazione stocastica}}. In generale, sostituire l’incertezza con valori medi non preserva l’ottimalità della decisione.

Il modello del \textcolor{NavyBlue}{\textbf{newsvendor}} evidenzia come la decisione ottimale derivi dal bilanciamento tra \textbf{costi di underage e overage}, mostrando che la domanda media non è un criterio corretto. Questo esempio chiarisce il legame diretto tra struttura dei costi e decisioni sotto incertezza.

I \textcolor{NavyBlue}{\textbf{vincoli probabilistici}} consentono di imporre il rispetto dei vincoli solo con una certa probabilità, ma possono generare insiemi ammissibili non convessi e problemi difficili da trattare. Gli \textcolor{NavyBlue}{\textbf{alberi decisionali}} permettono invece di rappresentare decisioni \textbf{adattive}, distinguendo tra nodi decisionali e nodi casuali e risolvendo il problema tramite \textcolor{NavyBlue}{\textbf{backward induction}} sulla base del valore atteso.

Il valore dell’informazione viene quantificato attraverso \textcolor{NavyBlue}{\textbf{EVPI}} e \textcolor{NavyBlue}{\textbf{VSS}}, che misurano rispettivamente il beneficio teorico dell’informazione perfetta e il guadagno ottenuto risolvendo il problema stocastico rispetto a una soluzione deterministica a valore atteso.

L’introduzione ai \textcolor{NavyBlue}{\textbf{modelli a due stadi}} chiarisce la distinzione tra decisioni \textcolor{NavyBlue}{\textbf{here-and-now}} e decisioni \textcolor{NavyBlue}{\textbf{wait-and-see}}, mostrando il ruolo delle \textbf{decisioni di ricorso}. Questa struttura si estende ai modelli multistadio, nei quali le decisioni dipendono dall’intera storia delle realizzazioni aleatorie e richiedono il rispetto della \textbf{non-anticipatività}.

Nei modelli multistadio emerge il problema della \textcolor{NavyBlue}{\textbf{generazione degli scenari}} e della crescita dell’albero, che rende necessarie tecniche di campionamento, riduzione e progettazione controllata degli scenari. La qualità di una soluzione va quindi valutata anche in termini di \textcolor{NavyBlue}{\textbf{stabilità in-sample e out-of-sample}}, verificando che le prestazioni non dipendano in modo critico dall’albero utilizzato.

Si analizzano le \textcolor{NavyBlue}{\textbf{proprietà di convessità}} dei modelli stocastici: i vincoli probabilistici non preservano la convessità in generale, mentre nei modelli a due stadi con ricorso lineare la funzione di ricorso risulta convessa, proprietà fondamentale per lo sviluppo di metodi di soluzione basati su decomposizione.

\paragraph{Introduzione} Si distinguono tre principali tipologie di \emph{Business Analytics}
\begin{itemize}[label=-]
    \item \textcolor{NavyBlue}{\textbf{Business Analytics descrittiva}}: ha l’obiettivo di \textbf{analizzare} e sintetizzare i \textbf{dati storici} al fine di comprendere che cosa è accaduto in passato (e.g. overbooking nel trasporto aereo).
    \item \textcolor{NavyBlue}{\textbf{Business Analytics predittiva}}: mira a \textbf{stimare eventi o variabili future} sulla base di modelli (e.g. distinzione tra \emph{forecasting}--previsione non sotto il mio controllo-- e \emph{prediction}).
    \item \textcolor{NavyBlue}{\textbf{Business Analytics prescrittiva}}: si concentra sul \textbf{supporto alle decisioni}, indicando quale azione dovrebbe essere intrapresa per ottimizzare una misura (e.g. \emph{operations management}).
\end{itemize}

\subsection{Motivazione: perché considerare l’incertezza}

\paragraph{Previsioni puntuali e decisioni}
Un punto di partenza naturale nello studio delle decisioni in condizioni di incertezza è la \textbf{costruzione di una previsione puntuale} di una variabile aleatoria. Sia $X$ una v.a. reale, con distribuzione di probabilità nota. Una \textbf{previsione puntuale} consiste nella scelta di un valore $x \in \mathbb{R}$, che rappresenta una \textbf{stima ex ante} della realizzazione futura di $X$.
La \textbf{qualità di una previsione} deve essere misurata in funzione del \textbf{costo associato all’errore} di previsione, che dipende dalla discrepanza tra il valore scelto $x$ e la realizzazione effettiva di $X$.

\paragraph{Errore quadratico medio (MSE)}
Una scelta naturale per misurare il \textbf{costo dell’errore di previsione è la perdita quadratica}. In questo caso, il costo associato alla previsione puntuale $x$ è
\(
(X - x)^2.
\)
Il \emph{problema decisionale} consiste quindi nel \textbf{minimizzare \textcolor{NavyBlue}{l’errore quadratico medio}}:
\(
\mathbb{E}\big[(X - x)^2\big].
\)
Sviluppando il valore atteso, si ottiene
\[
MSE(x) \dot{=} \mathbb{E}\big[(X - x)^2\big]
= \mathbb{E}[X^2] - 2x\,\mathbb{E}[X] + x^2.
\]
La condizione di ottimalità del primo ordine implica che la \emph{previsione ottima coincide con il valore atteso della variabile aleatoria},
\(
x^\ast = \mathbb{E}[X].
\)
Questo risultato mostra che \textcolor{NavyBlue}{l’uso del valore atteso come previsione puntuale è ottimale solo sotto l’ipotesi di una perdita quadratica} (simmetrica).

\paragraph{Perdita assoluta e penalità asimmetriche}
Un criterio alternativo consiste nel misurare l’errore di previsione tramite la \textcolor{NavyBlue}{\textbf{deviazione assoluta}},
\(
\mathbb{E}\big[|X - x|\big].
\)
In questo caso, la soluzione ottima è data dalla \textbf{mediana della distribuzione} di $X$, anziché dal valore atteso.\\
Nelle applicazioni economiche, errori di previsione positivi e negativi possono avere impatti diversi. È quindi naturale introdurre una \textcolor{NavyBlue}{\textbf{funzione di perdita asimmetrica}} del tipo
\[
\mathbb{E}\big[c_u (X - x)^+ + c_o (X-x)^-\big],
\]
dove $(\cdot)^+ = \max\{0,\cdot\}$ e $(\cdot)^- = \max\{0,-\cdot\}$, mentre $c_u$ e $c_o$ rappresentano rispettivamente il \textbf{costo di sottostima e di sovrastima}.
La soluzione ottima è un \emph{quantile della distribuzione di $X$}, il cui livello dipende dal rapporto tra $c_u$ e $c_o$.\\
\hspace*{1.5em} Il \textcolor{NavyBlue}{\textbf{messaggio essenziale}} è che una \textcolor{NavyBlue}{previsione puntuale, tipicamente basata sul valore atteso, non è sufficiente per prendere decisioni ottimali in condizioni di incertezza}. 
La previsione ottimale dipende dalla funzione di perdita adottata e quindi dall’impatto economico dell’errore di previsione. 
In generale, \emph{non esiste una previsione “migliore” in senso assoluto, ma solo previsioni coerenti con uno specifico criterio decisionale.}

\paragraph{Modelli decisionali in ambito business}
L’obiettivo non è la previsione di una variabile aleatoria, ma la \textbf{selezione di un vettore di decisioni} $x \in X$ che ottimizzi una funzione economica in presenza di \emph{fattori di rischio}. 
Se $\xi$ denota un vettore di variabili aleatorie che influenzano il risultato economico, il problema può essere formulato come un \textcolor{NavyBlue}{\textbf{problema di ottimizzazione sotto incertezza}}
\[
\min_{x \in S} f(x,\xi).
\]
\hspace*{1.5em} Se non si dispone di informazioni probabilistiche sui fattori di rischio e si conosce solo un insieme di incertezza $U$, 
e il set di ammissibilità $S$, il problema assume la forma di un \textcolor{NavyBlue}{\textbf{worst-case robust optimization problem}}
\[
\min_{x \in S} \; \max_{\xi \in U} f(x,\xi).
\]
\hspace*{1.5em} Se invece i fattori di rischio sono modellati come variabili casuali con distribuzione nota, si ottiene un \textcolor{NavyBlue}{\textbf{problema di ottimizzazione stocastica}}
\[
\min_{x \in S} \; \mathbb{E}_\mathbb{P}\big[f(x,\tilde{\xi})\big].
\]
Questa formulazione mette in evidenza che, in generale,
\(
\mathbb{E}_\mathbb{P}\big[f(x,\tilde{\xi})\big] \neq f\big(x,\mathbb{E}_\mathbb{P}[\tilde{\xi}]\big),
\)
e giustifica la necessità di \emph{modelli decisionali che tengano esplicitamente conto dell’incertezza}.
La formulazione di questo  modello  suggerisce un problema decisionale statico, in base al quale prendiamo una
decisione $x$ prima della realizzazione della variabile casuale $\tilde{\xi}$, ma non c'è modo di
adattare la decisione dopo il verificarsi dell'evento casuale. 

\paragraph{Differenza tra rischio e incertezza} Il \textcolor{NavyBlue}{\textbf{rischio}} è definito come la presenza di variabili aleatorie con distribuzione di probabilità nota, mentre l'\textcolor{NavyBlue}{\textbf{incertezza}} si riferisce a situazioni in cui tali distribuzioni non sono note o non possono essere stimate con precisione. 

\subsection{Modelli decisionali statistici}
\paragraph{Il modello classico del newsvendor}
Il \textcolor{NavyBlue}{\textbf{modello classico del \emph{newsvendor}}} rappresenta un problema di decisione sotto condizione di incertezza in cui \emph{una decisione deve essere presa prima dell’osservazione di una variabile aleatoria}. 
Sia $D$ una variabile casuale non negativa che rappresenta la \textbf{domanda}, con distribuzione di probabilità nota. 
Il decisore sceglie una \textbf{quantità $q \ge 0$}, a \textbf{prezzo $c$} prima di osservare la realizzazione di $D$.
Ogni pezzo viene venduto a un \textbf{prezzo $p>c$} durante la finestra di vendita, e ad un \textbf{prezzo ridotto} successivamente $p_u<c$.\\
L'intuizione potrebbe suggerire di porre semplicemente $q=\mathbb{E}\big[D\big]$ per massimizzare il valore atteso del profitto. Ma questo risulta essere sbagliato. In realtà, il risultato non
sorprende se introduciamo due tipi di costo:
\begin{itemize}[label=-]
    \item Se $q<D$, avremo un \textcolor{NavyBlue}{\textbf{costo opportunità}} $m=p-c$, cioè il margine di profitto per la parte di domanda non soddisfatta (\textbf{shortfall}).
    \item Se $q>D$, avremo un \textcolor{NavyBlue}{\textbf{costo dell'invenduto}} $c_u=c-p_u$, legata alla svendita della giacenza residue (\textbf{surplus}).
\end{itemize}

\paragraph{Espressione generale del profitto e riscrittura con costi di underage/overage}
La \textcolor{NavyBlue}{\textbf{formulazione tramite funzione di perdita}} può essere affiancata da una scrittura esplicita del profitto, utile per evidenziare la decomposizione in termini di \emph{underage} e \emph{overage}. L’espressione generale del profitto è
\[
\pi(q,d) = -cq + p\min(q,d) + p_u\max(q-d,0) = -cq +p\min(q,d)+p_u(q-d)^+.
\]
Usando l’identità
\(
q = \min(q,d) + (q-d)^+,
\)
il profitto può essere riscritto come
\(
\pi(q,d) = c_u \min(q,d) - c_o (q-d)^+,
\)
dove definiamo il costo di underage $c_u = p-c$ e il costo di overage $c_o = c-p_u$.\\ 
Se l'\textbf{incertezza} sulla \textbf{domanda} è modellata da una v.a. continua con densità
$f_D(x)$, il \textbf{profitto atteso} è
\[
\mathbb{E}[\pi(q,D)]
= c_u\left(\int_0^q x f_D(x)\,dx + \int_q^{+\infty} q f_D(x)\,dx\right)
- c_o \int_0^q (q-x) f_D(x)\,dx.
\]

\paragraph{Teorema -- Regola di Leibniz}
Consideriamo una funzione di due variabili $g(q,x)$ e definiamo una funzione della sola $q$ come
\[
G(q)=\int_{h_1(q)}^{h_2(q)} g(q,x)\,dx.
\]
Notiamo che anche gli estremi di integrazione sono funzioni di $q$. Sotto opportune
ipotesi di continuità, la \textcolor{NavyBlue}{\textbf{regola di Leibniz}} permette di scrivere
\[
\frac{dG}{dq}(q)=\int_{h_1(q)}^{h_2(q)} \frac{\partial g}{\partial q}(q,x)\,dx
+ g\big(q,h_2(q)\big)\,h_2'(q) - g\big(q,h_1(q)\big)\,h_1'(q).
\]
Nel caso del \textbf{profitto atteso del newsvendor}, tramite la regola di Leibniz otteniamo
\[
\frac{d\,\mathbb{E}[\pi(q,D)]}{dq}
= c_u\left(q f_D(q) + \int_{q}^{+\infty} f_D(x)\,dx - q f_D(q)\right)
- c_o \int_{0}^{q} f_D(x)\,dx
\]
\[
= c_u\int_{q}^{+\infty} f_D(x)\,dx - c_o\int_{0}^{q} f_D(x)\,dx
= c_u\big(1-F_D(q)\big) - c_o F_D(q)=0,
\]
dove
\(
F_D(x)\dot{=}\mathbb{P}\{D\le x\}
\)
è la \emph{funzione di distribuzione cumulativa} della domanda.
Ponendo a zero la derivata prima, ricaviamo immediatamente
\[
F_D(q^\ast)=\frac{c_u}{c_u+c_o}.
\]
Come verifica, risulta utile controllare la \textbf{derivata seconda}
\[
\frac{d^2 \mathbb{E}[\pi(q,D)]}{dq^2}
= -c_u f_D(q) - c_o f_D(q) < 0 \qquad \forall q,
\]
in quanto la funzione di densità non può assumere valori negativi.
Ciò dimostra la \textcolor{NavyBlue}{\textbf{concavità del profitto atteso rispetto a $q$}}.

\paragraph{Modelli con vincoli probabilistici (chance-constrained models)}
L’idea alla base dei \emph{chance-constrained models} è che un \textcolor{NavyBlue}{\textbf{vincolo stocastico non debba necessariamente essere soddisfatto in ogni realizzazione dell’incertezza}}, ma solo con una probabilità sufficientemente elevata. In particolare, un vincolo del tipo
\(
g(x,\tilde{\xi}) \le 0
\)
è considerato \emph{accettabile} se risulta soddisfatto con probabilità almeno pari a un livello prefissato. 
Sono considerati \textbf{vincoli probabilistici individuali} o \textbf{congiunto}
\[
\mathbb{P}\{g_j(x,\tilde{\xi}) \le 0\} \ge 1-\alpha_j, \; j \in [m], \qquad
\mathbb{P}\{\mathbf{g}(\mathbf{x},\tilde{\boldsymbol{\xi}}) \le \mathbf{0}_m\} \ge 1-\alpha,
\]
dove $\mathbf{g}$ è una \textbf{funzione vettoriale}.
L’intuizione potrebbe suggerire che, \emph{se le funzioni $g_j$ sono convesse rispetto a $x$ per ogni realizzazione di
$\tilde{\xi}$, allora anche il vincolo probabilistico dovrebbe preservare la convessità}. Tuttavia, \textcolor{NavyBlue}{\textbf{questo non è vero in
generale}}: \emph{l’insieme ammissibile di un problema con vincoli probabilistici può risultare non convesso},
in quanto definito come unione di insiemi ammissibili associati a diversi scenari. 
Per questo motivo, i modelli chance-constrained possono risultare difficili da trattare dal punto di vista computazionale 
e spesso vengono approssimati tramite formulazioni di tipo robusto.
 Nonostante ciò, essi rappresentano uno strumento naturale per modellare problemi statici in cui non è possibile 
 adattare le decisioni dopo la realizzazione dell’incertezza.

\subsection{Alberi decisionali}
\paragraph{Dalle decisioni statiche a quelle adattive}
Un modo naturale per introdurre \textbf{decisioni adattive in condizioni di incertezza} è l’utilizzo degli \textbf{alberi di decisione}.
Gli alberi di decisione \emph{consentono di rappresentare in modo esplicito la sequenza temporale delle decisioni
e delle realizzazioni aleatorie}, evidenziando come le decisioni possano essere adattate sulla base delle informazioni
che si rendono disponibili nel tempo. Un albero di decisione è costituito da due tipi fondamentali di nodi
\begin{itemize}[label=-]
    \item \textcolor{NavyBlue}{\textbf{Nodi decisionali}}, rappresentati da quadrati, che corrispondono a \textbf{scelte discrete tra alternative mutuamente esclusive}. In questi nodi il decisore deve selezionare una sola azione tra quelle disponibili.
    \item \textcolor{NavyBlue}{\textbf{Nodi casuali}}, rappresentati da cerchi, che descrivono la \textbf{realizzazione di esiti aleatori}. A ciascun esito $i$ è associata una probabilità $\pi_i$, e la somma delle probabilità degli esiti che partono da uno stesso nodo casuale è pari a 1.
\end{itemize}
\hspace*{1.5em} Un albero di decisione è composto da una \emph{sequenza di nodi decisionali e nodi casuali}, 
che in genere si \textbf{alternano}. L’albero termina con \textbf{nodi terminali}, rappresentati da punti, ai quali sono
associati i \textbf{risultati finali}, tipicamente espressi in termini monetari.

\paragraph{Strategia e criterio di ottimalità}
Risolvere un problema rappresentato mediante un albero di decisione significa individuare una \textbf{strategia}, ovvero una \emph{regola che specifica quale decisione assumere in ciascun nodo decisionale che può essere visitato}.  
Assumendo che i risultati abbiano natura monetaria, il criterio più naturale è la \textcolor{NavyBlue}{\textbf{massimizzazione del valore monetario atteso EMV}} (\emph{Expected Monetary Value}).\\
\hspace*{1.5em} Quando un nodo decisionale è seguito da nodi casuali, ciascun nodo casuale viene etichettato con il proprio \textbf{valore monetario atteso}. Questo consente di confrontare le alternative disponibili nel nodo decisionale.  
La procedura di soluzione procede \textbf{a ritroso nel tempo} (\textcolor{NavyBlue}{\emph{backward induction}}), iniziando dai nodi terminali e risalendo progressivamente verso la radice dell’albero. 
\emph{Un nodo può essere valutato solo quando tutti i suoi successori sono già stati valutati.}

\paragraph{Esempio: introduzione di un nuovo prodotto}
Un tipico esempio di applicazione degli alberi di decisione riguarda la scelta se \textbf{lanciare un nuovo prodotto}, vendere una licenza o acquisire \textbf{informazioni aggiuntive} tramite un’indagine di mercato.  
L’indagine fornisce informazioni che modificano le stato di conoscenza, ma non la probabilità di successo del prodotto. La soluzione ottimale si ottiene confrontando il valore monetario atteso delle diverse strategie, tenendo conto del costo dell’informazione e delle decisioni ottimali successive ai diversi esiti osservati.

\subsection{EVPI and VSS}
\paragraph{Problema statico di ottimizzazione stocastica}
Si consideri il valore ottimo di un problema di ottimizzazione stocastica statica:
\[
f^{*} = \min_{x \in S} \mathbb{E}_{P}\!\left[ f(x,\tilde{\xi}) \right].
\]
In questo contesto la decisione viene presa \textcolor{NavyBlue}{\textbf{here-and-now}}, prima della realizzazione dell’incertezza.\\
\hspace*{1.5em} Ci si può chiedere come migliorerebbe il valore ottimo se fosse possibile \textbf{posticipare la decisione} e osservare prima lo scenario realizzato. Formalmente, ciò equivale a scambiare l’operatore di minimo con il valore atteso:
\[
f^{*}_{PI} = \mathbb{E}_{P}\!\left[ \min_{x \in S} f(x,\tilde{\xi}) \right].
\]
Il pedice $PI$ indica che questo valore corrisponde all’ottimo ottenibile in presenza di \textbf{informazione perfetta}.

\paragraph{Valore atteso dell’informazione perfetta (EVPI)} 
Per un problema di minimizzazione vale la disuguaglianza
\(
f^{*}_{PI} \le f^{*},
\)
poiché l’accesso all’informazione perfetta non può peggiorare la soluzione.
La differenza tra i due valori ottimi definisce il \textcolor{NavyBlue}{\textbf{valore atteso dell’informazione perfetta}}:
\[
EVPI = f^{*} - f^{*}_{PI}.
\]
L’EVPI fornisce una misura quantitativa dell’\textbf{impatto dell’incertezza} sul problema decisionale. 
L’EVPI misura il beneficio teorico della \textbf{chiaroveggenza}. In pratica, l’accesso a informazione perfetta è estremamente raro; pertanto, l’EVPI va interpretato come un \textbf{limite superiore} al valore che l’informazione può avere.

\paragraph{Esempio: scelta di una strategia di investimento}
Un esempio illustrativo riguarda la scelta tra due strategie di investimento alternative, una \emph{aggressiva} e una \emph{conservativa}, in presenza di diversi stati possibili dell’economia.  
Nel caso \textcolor{NavyBlue}{\textbf{here-and-now}}, la decisione viene presa prima di osservare lo stato dell’economia e la strategia viene scelta massimizzando il rendimento atteso.  
Nel caso \textcolor{NavyBlue}{\textbf{wait-and-see}}, la strategia ottimale può essere selezionata dopo aver osservato lo scenario realizzato, ottenendo un valore atteso maggiore. La differenza tra i due valori rappresenta l’EVPI.

\paragraph{Soluzione a valore atteso}
Un approccio più semplice consiste nel \textbf{trascurare l’incertezza} e sostituire le variabili aleatorie con i loro valori attesi. In questo modo si ottiene il problema deterministico a valore atteso:
\[
f^{*}_{EV} = \min_{x \in S} f\!\left(x, \mathbb{E}_{P}[\tilde{\xi}]\right),
\]
la cui soluzione è detta \textcolor{NavyBlue}{\textbf{expected-value solution}} e viene indicata con $\bar{x}$.\\
\hspace*{1.5em} La soluzione $\bar{x}$ deve essere valutata nel contesto stocastico originale. Il costo risultante è la variabile aleatoria $f(\bar{x},\tilde{\xi})$, di cui si considera il valore atteso:
\[
f_{EEV} = \mathbb{E}_{P}\!\left[ f(\bar{x},\tilde{\xi}) \right].
\]
Il confronto corretto non è tra $f^{*}_{EV}$ e $f^{*}$, ma tra $f_{EEV}$ e $f^{*}$.
Infatti, il \textcolor{NavyBlue}{\textbf{valore della soluzione stocastica}} è definito come
\[
VSS = f_{EEV} - f^{*},
\]
nel caso di minimizzazione (con scambio dei termini nel caso di massimizzazione).  
È possibile dimostrare che $VSS \ge 0$. Quando il VSS è elevato, \emph{lo sforzo computazionale richiesto per risolvere il problema stocastico completo è giustificato dal miglioramento ottenuto rispetto alla soluzione deterministica a valore atteso.}

\paragraph{Collegamento con modelli multi-stadio}
Il concetto di VSS viene ulteriormente chiarito attraverso esempi numerici e rappresenta un ponte naturale verso i 
\textcolor{NavyBlue}{\textbf{modelli di ottimizzazione a due stadi e multi-stadio}}, nei quali le decisioni possono 
essere adattate dopo l’osservazione dell’incertezza.

\subsection{Un'introduzione a un modello a due stadi: Assemble-to-order (ATO)}
\paragraph{ATO} Si consideri un esempio didattico di \textbf{pianificazione della produzione} in cui i prodotti finiti sono ottenuti assemblando un insieme di componenti. Ogni prodotto finale è caratterizzato da un insieme di caratteristiche, per ciascuna delle quali sono disponibili opzioni alternative che richiedono componenti diversi. Dalla combinazione di un numero limitato di componenti può derivare un numero molto elevato di prodotti finali.\\
\hspace*{1.5em} Per questo motivo, non è possibile proteggersi dall’incertezza della domanda mantenendo scorte di prodotti
finiti. D’altra parte, una strategia puramente \textcolor{NavyBlue}{\textbf{make-to-order}} può risultare poco efficace a
causa dei lunghi tempi di approvvigionamento dei componenti. In un ambiente \textcolor{NavyBlue}{\textbf{assemble-To-Order}}, in cui
l’assemblaggio è rapido, è invece possibile mantenere a magazzino i componenti e assemblare i prodotti finali solo dopo 
aver osservato la domanda.\\
\hspace*{1.5em} Questo contesto suggerisce naturalmente una \textbf{strategia a due stadi}:
\begin{itemize}[label=-]
    \item Pianificazione della produzione dei componenti \textcolor{NavyBlue}{\textbf{here-and-now}}, sotto incertezza sulla domanda.
    \item Assemblaggio dei prodotti finali in modalità \textcolor{NavyBlue}{\textbf{wait-and-see}}, dopo l’osservazione degli ordini.
\end{itemize}

\paragraph{Modello deterministico a valore atteso}
\emph{Ignorando l’incertezza della domanda} e \emph{sostituendola con il valore medio}, si introducono le seguenti variabili decisionali
\begin{itemize}[label=-]
    \item $x_i \in \mathbb{Z}_+$, $i=1,\dots,n_i$, numero di componenti prodotti.
    \item $y_j \in \mathbb{Z}_+$, $j=1,\dots,n_j$, numero di prodotti finali assemblati.
\end{itemize}
Il \textcolor{NavyBlue}{\textbf{problema di massimizzazione del profitto}} può essere formulato così
\begin{align*}
\max \quad & - \sum_{i \in [n_i]} C_i x_i + \sum_{j \in [n_j]} P_j y_j \\[0.3em]
\text{s.t.} \quad
& \sum_{i \in [n_i]} T_{im} x_i \le L_m, 
&& m \in [n_m]  \\[0.3em]
& y_j \le \bar{d}_j,
&& j \in [n_j]  \\[0.3em]
& \sum_{j \in [n_j]} G_{ij} y_j \le x_i,
&& i \in [n_i]  \\[0.3em]
& x_i,\, y_j \in \mathbb{Z}_+,
&& i \in [n_i],\ j \in [n_j]. 
\end{align*}

\paragraph{Limiti della soluzione deterministica}
La soluzione ottenuta è facilmente interpretabile ma estremamente \textbf{sbilanciata}. Essa rappresenta una scommessa sulla domanda del prodotto più redditizio e può risultare molto rischiosa se la domanda effettiva si discosta dal valore medio.

\paragraph{Modello stocastico a due stadi}
Per tenere conto dell’incertezza, si introducono:
\begin{itemize}[label=-]
    \item Una domanda scenario-dipendente $d^s_j$ con probabilità $\pi_s$;
    \item Variabili di secondo stadio $y^s_j \in \mathbb{Z}_+$ per l’assemblaggio adattivo.
\end{itemize}
Il \textcolor{NavyBlue}{\textbf{modello stocastico a due stadi}} è
\begin{align*}
\max \quad 
& - \sum_{i \in [n_i]} C_i x_i 
  + \sum_{s \in [n_s]} \pi^s 
    \left( \sum_{j \in [n_j]} P_j y_j^s \right) \\[0.3em]
\text{s.t.} \quad
& \sum_{i \in [n_i]} T_{im} x_i \le L_m,
&& m \in [n_m] \\[0.3em]
& y_j^s \le \bar{d}_j^{\,s},
&& j \in [n_j],\ s \in [n_s] \\[0.3em]
& \sum_{j \in [n_j]} G_{ij} y_j^s \le x_i,
&& i \in [n_i],\ s \in [n_s] \\[0.3em]
& x_i, y_j^s \in \mathbb{Z}_+,
&& i \in [n_i] j \in [n_j],\ s \in [n_s].\\[0.3em]
\end{align*}
La soluzione stocastica produce quantità di componenti meno estreme e decisioni di assemblaggio che variano a seconda dello scenario. 
Le variabili di secondo stadio rappresentano piani di contingenza e non costituiscono output operativi immediati.

\paragraph{Confronto corretto tra le soluzioni}
Il confronto diretto tra $f^{*}$ e $f_{EV}$ non è significativo. Occorre fissare le decisioni al primo stadio e risolvere, 
per ciascuno scenario $S$, il \textcolor{NavyBlue}{\textbf{problema al secondo stadio}}
\begin{align*}
R^s(\mathbf{x}^\circ) = \max \quad 
& \sum_{j \in [n_j]} P_j y_j^s \\[0.3em]
\text{s.t.} \quad
& y_j^s \le d_j^{\,s},
&& j \in [n_j] \\[0.3em]
& \sum_{j \in [n_j]} G_{ij} y_j^s \le x_i^\circ,
&& i \in [n_i] \\[0.3em]
& y_j^s \in \mathbb{Z}_+,
&& j \in [n_j].
\end{align*}
Dove $R^s(\mathbf{x}^\circ)$ è il \textbf{ricavo ottimo} che otteniamo nello scenario $s$,
dato il vettore di decisione di \textbf{primo stadio} $\mathbf{x}^\circ$, sfruttando in modo
ottimale i componenti disponibili per soddisfare la domanda. Si noti che, in questo modello, la disponibilità dei componenti $\mathbf{x}^\circ$ è data,
a seconda dei casi, dal modello \textbf{stocastico} oppure dal modello
\textbf{deterministico}. In entrambi i casi, il \textbf{ricavo atteso} risultante è dato da
\[
\sum_{s \in S} \pi^s R^s(\mathbf{x}^\circ).
\]

\subsection{Modelli a due stadi}
\paragraph{Dall’ATO ai modelli a due stadi con ricorso}
Il modello \emph{assemble-to-order} fornisce una buona introduzione ai \textbf{modelli di programmazione lineare stocastica a due stadi con ricorso} (\emph{recourse}), che rappresentano l’esempio più semplice di modelli di ottimizzazione adattivi.
In un modello a due stadi si distinguono due tipi di decisioni:
\begin{itemize}[label=-]
    \item Decisioni \textbf{here-and-now} (primo stadio) $x$, che devono essere prese sotto incertezza (ad esempio al tempo $t=0$);
    \item Decisioni \textbf{wait-and-see} (secondo stadio) $y(\tilde{\xi})$, che possono essere prese dopo aver osservato la realizzazione dei fattori di rischio $\tilde{\xi}$ (ad esempio al tempo $t=1$).
\end{itemize}
Le decisioni di secondo stadio sono anche dette \textcolor{NavyBlue}{\textbf{decisioni di ricorso}}. Si osserva immediatamente che tali decisioni sono in realtà \textbf{politiche decisionali}, cioè funzioni che mappano la realizzazione dei fattori di rischio in una decisione ammissibile; pertanto appartengono a uno spazio \textbf{infinito-dimensionale}.

\paragraph{Formulazione annidata del modello}
Il modello può essere formulato come \textcolor{NavyBlue}{\textbf{due problemi di ottimizzazione annidati}}
\[
\begin{aligned}
\min_{x}\;& c^{T}x + Q(x) \\
\text{s.t. }\;& Ax=b \\
& x \ge 0,
\end{aligned}
\]
dove la \textbf{funzione di ricorso} è definita come
\(
Q(x) = \mathbb{E}_{\mathbb{P}}\!\left[Q(x,\tilde{\xi})\right].
\)
Il \textcolor{NavyBlue}{\textbf{problema al secondo stadio}} è
\[
\begin{aligned}
Q(x,\xi) = \min_{y}\;& q(\xi)^{T}y \\
\text{s.t. }\;& Wy = h(\xi) - T(\xi)x \\
& y \ge 0.
\end{aligned}
\]

\paragraph{Ricorso fisso e non-linearità indotta dal ricorso}
Nel problema di secondo stadio sia la decisione di primo stadio $x$ sia la realizzazione $\xi$ dei fattori di rischio
sono dati. Risolvendo il secondo stadio per ogni realizzazione $\xi$, si definisce implicitamente una funzione
$y(\tilde{\xi})$, mostrando che le variabili di secondo stadio sono effettivamente funzioni. In questa formulazione
la \textbf{matrice di ricorso} $W$ non dipende da variabili aleatorie e si parla di \textbf{ricorso fisso} 
(\emph{fixed recourse}). La formulazione evidenzia che \textcolor{NavyBlue}{\textbf{la programmazione lineare stocastica con ricorso è, in generale, 
un problema di programmazione non lineare}} (poiché $Q(x)$ dipende da $x$ tramite un problema di ottimizzazione 
interno).\\
\hspace*{1.5em}
La funzione $Q(x)$ è un valore atteso rispetto alla distribuzione congiunta di $\tilde{\xi}$; se le variabili aleatorie sono continue, essa è un integrale multidimensionale. Inoltre, tale integrale riguarda una funzione che non è nota in forma chiusa, poiché è definita implicitamente dalla soluzione di un problema di ottimizzazione. In molti casi di interesse pratico si possono tuttavia dimostrare proprietà utili della funzione di ricorso, in particolare la \textbf{convessità}.\\
\hspace*{1.5em}
Poiché le variabili di secondo stadio sono funzioni in uno spazio infinito-dimensionale, una strategia comune consiste 
nella discretizzazione tramite campionamento. Si genera un \textbf{albero di scenario} (\emph{fan}) in cui l’evento
 $\omega_{s}$ corrisponde alla realizzazione dello scenario $s\in S$, con $S$ insieme degli scenari.
  Se gli scenari sono \textbf{campionati via Monte Carlo semplice}, le probabilità sono uniformi: $\pi_{s}=1/|S|$ 
  (sono possibili anche metodi più sofisticati di generazione scenari).

\paragraph{Modello deterministico equivalente (LP a grande scala)}
La discretizzazione conduce a
\[
\begin{aligned}
\min\;& c^{T}x + \sum_{s\in S} \pi_{s}\, (q^{s})^{T}y^{s}\\
\text{s.t. }\;& Ax=b\\
& Wy_s + T_sx = h_s, \quad s\in S \\
& x,\; y_s\ge 0.
\end{aligned}
\]

\paragraph{Fattibilità del secondo stadio e concetti di ricorso completo}
Una questione centrale è stabilire se \textcolor{NavyBlue}{\textbf{il secondo stadio sia fattibile per ogni scelta delle variabili di primo stadio}} e 
per ogni realizzazione delle variabili aleatorie. Occorre restringere l’insieme delle decisioni \emph{here-and-now} al
 dominio in cui il secondo stadio è fattibile (equivalentemente, la funzione di ricorso è limitata; come usuale, a un 
 problema infeattibile si associa costo infinito). Riscrivendo i vincoli di collegamento come
\[
Wy_s=h_s-T_sx, \quad s\in S,
\]
il \textbf{secondo stadio è fattibile quando il termine noto può essere espresso come \textcolor{NavyBlue}{combinazione conica} delle colonne di $W$}. 
In tal caso si parla di \textcolor{NavyBlue}{\textbf{complete recourse}}. Se il ricorso completo non vale per decisioni
 arbitrarie di primo stadio, ma vale per le decisioni che soddisfano i vincoli del primo stadio, si parla di
  \textcolor{NavyBlue}{\textbf{relatively complete recourse}}.\\
\hspace*{1.5em} Nei problemi di business è spesso possibile introdurre flessibilità tramite penalità, in modo da evitare infeattibilità al secondo stadio. 

\subsubsection{The plant location model}

\paragraph{Descrizione del problema e rete bipartita}
Il classico \textcolor{NavyBlue}{\textbf{plant location model}} è il più semplice problema di \textbf{network design}. 
Si considera una rete bipartita con un insieme $P$ di nodi sorgente potenziali e un insieme $\cal{D}$ di nodi domanda. I nodi 
sorgente rappresentano impianti produttivi che possono essere aperti oppure no; i nodi domanda possono rappresentare 
magazzini regionali o centri retail. In ciascun nodo domanda si realizza una domanda aleatoria $D_{j}(\omega)$,
 $j\in \cal{D}$, che va soddisfatta al costo minimo.
Sono necessari i seguenti dati
\begin{itemize}[label=-]
    \item Per ogni $i\in P$, un \textbf{costo fisso di apertura} $f_i$ e una \textbf{capacità} $u_i$.
    \item Per ogni $j\in \cal{D}$ e \textbf{scenario} $s\in S$, una domanda $d^{s}_{j}$ (discretizzazione della domanda aleatoria).
    \item Per ogni arco $(i,j)$, un \textbf{costo unitario di trasporto} $c_{ij}$ (costi variabili lineari).
\end{itemize}
Il \textbf{problema è naturalmente a due stadi}: le \emph{decisioni di apertura impianti} devono essere prese sotto incertezza,
mentre le \emph{decisioni di trasporto} possono essere rimandate a dopo l’osservazione della domanda. Si introducono le variabili
\[
y_i =
\begin{cases}
1 & \text{se il nodo sorgente } i\in P \text{ è aperto}\\
0 & \text{altrimenti}
\end{cases}
\qquad
\text{e}
\qquad
x^{s}_{ij}\ge 0,
\]
dove $x^{s}_{ij}$ è il flusso da $i\in P$ a $j\in \cal{D}$ nello scenario $s\in S$.
Le variabili di localizzazione $y_i$ sono \textbf{di primo stadio} (design), mentre i flussi $x^{s}_{ij}$ sono \textbf{di secondo stadio} (controllo).
\\ \hspace*{1.5em} Si ottiene il seguente \textcolor{NavyBlue}{\textbf{modello MILP stocastico a due stadi con ricorso}}
\begin{align*}
\min \quad 
& \sum_{i \in \mathcal{P}} f_i y_i 
+ \sum_{s \in S} \pi^s 
\left(
\sum_{i \in \mathcal{P}} \sum_{j \in \cal{D}} c_{ij} x_{ij}^s
\right) \\[0.4em]
\text{s.t.} \quad
& \sum_{i \in \mathcal{P}} x_{ij}^s = d_j^s,
&& \forall s \in S,\ \forall j \in \mathcal{D} \\[0.3em]
& \sum_{j \in \mathcal{D}} x_{ij}^s \le R_i y_i,
&& \forall s \in S,\ \forall i \in \mathcal{P} \\[0.3em]
& x_{ij}^s \ge 0, y_i \in \{0,1\}.
\end{align*}


\paragraph{Scenari estremi e necessità di formulazioni elastiche}
È opportuno chiedersi \emph{cosa accade se viene incluso uno scenario estremo con domanda molto elevata}.
Potrebbe non essere nemmeno possibile soddisfare tale domanda; inoltre, la pianificazione della capacità verrebbe guidata da
 scenari estremi ma molto improbabili, producendo soluzioni eccessivamente costose. 
 In questi casi conviene ricorrere a formulazioni elastiche, permettendo \textbf{violazioni penalizzate dei vincoli}.

\paragraph{Modello elastico con domanda non soddisfatta penalizzata}
Sia $z^{s}_{j}\ge 0$ la \textbf{quantità di domanda non soddisfatta} nel nodo $j$ nello scenario $S$. 
Tali variabili entrano nell’obiettivo moltiplicate per un coefficiente di penalità $\beta_j$. Il modello diventa
\begin{align*}
\min \quad 
& \sum_{i \in \mathcal{P}} f_i y_i
+ \sum_{s \in S} \pi^s
\left(
\sum_{i \in \mathcal{P}} \sum_{j \in \mathcal{D}} c_{ij} x_{ij}^s
\right)
+ \sum_{s \in S} \pi^s
\left(
\sum_{j \in \mathcal{D}} \beta_j z_j^s
\right) \\[0.4em]
\text{s.t.} \quad
& \sum_{i \in \mathcal{P}} x_{ij}^s + z_j^s = d_j^s,
&& \forall s \in S,\ \forall j \in \mathcal{D} \\[0.3em]
& \sum_{j \in \mathcal{D}} x_{ij}^s \le R_i y_i,
&& \forall s \in S,\ \forall i \in \mathcal{P} \\[0.3em]
& x_{ij}^s \ge 0,\ z_j^s \ge 0, y_i \in \{0,1\}.
\end{align*}
La \textcolor{NavyBlue}{\textbf{quantificazione delle penalità}} $\beta_j$ dipende dal significato di $z^{s}_{j}$: 
può rappresentare \emph{domanda effettivamente non servita} (con penalità differenziate per priorità di mercato)
 oppure \emph{domanda soddisfatta tramite fornitori terzi} (con penalità pari al costo effettivo di tale opzione).

\paragraph{Osservazioni di modellazione}
Il modello considerato è solo un primo passo verso formulazioni più realistiche (costi di trasporto non lineari, più item). Inoltre si trascura la dimensione temporale e l’uso di scorte per gestire variabilità parzialmente prevedibile della domanda, che porterebbero a un modello multistadio più impegnativo. L’output rilevante del modello rimane la \textbf{scelta del network design}.

\subsubsection{Modello del newsvendor a due stadi}

\paragraph{Motivazione e struttura informativa}
Si consideri una \textcolor{NavyBlue}{\textbf{versione a due stadi e multi-item del newsvendor problem}}, soggetta a vincoli di capacità.
La struttura informativa può essere rappresentata da un albero a tre livelli: al nodo radice si decide il
 primo lotto; nei nodi intermedi si decide un secondo lotto sotto incertezza ridotta; nei nodi foglia si osservano
  le vendite effettive e si contabilizzano costi di overage/underage.
Ogni nodo foglia $n\in N_2$ ha esattamente un predecessore (antecedente) $\alpha(n)\in N_1$. Il nodo radice $0$ è l’antecedente diretto di ogni nodo in $N_1$. Le decisioni dell’ultimo livello sono in realtà variabili fittizie di contabilizzazione (surplus/shortfall penalizzati), non variabili di controllo o design.
Sono necessari i seguenti insiemi e parametri
\begin{itemize}[label=-]
    \item $I$: insieme degli item (SKU).
    \item $K_1$ e $K_2$: capacità massime (numero massimo di item producibili) nel primo e nel secondo run produttivo.
    \item $L_i$, $i\in I$: lotti minimi. Le variabili di lotto sono semicontinue: si può non produrre un item, ma se lo si produce esiste un minimo economicamente giustificato.
    \item Domanda $d^{n}_{i}$ per ciascun $i\in I$ e foglia $n \in N_2$, con probabilità associata $\pi^{n}$.
    \item Coefficienti di penalità di overage $c^{o}_{i}$ e underage $c^{u}_{i}$ per ciascun $i\in I$.
\end{itemize}
Si introducono le \textbf{variabili decisionali}
\begin{itemize}[label=-]
    \item $x^{n}_{i}$: quantità prodotta dell’item $i$ nel nodo $n\in\{0\}\cup N_1$, intera non negativa.
    \item Variabili binarie $z^{n}_{i}\in\{0,1\}$ per rappresentare la semicontinuità.
    \item Variabili di contabilizzazione $u^{n}_{i}$ (underage) e $v^{n}_{i}$ (overage) ai nodi foglia $n\in N_2$.
\end{itemize}
La formulazione \textcolor{NavyBlue}{\textbf{nel MILP stocastico}} è
\begin{align*}
\min \quad 
& \sum_{n \in \mathcal{N}_2} \pi^n
\left[
\sum_{i \in \mathcal{I}}
\left(
c_i^{o}\, o_i^n + c_i^{u}\, u_i^n
\right)
\right] \\[0.4em]
\text{s.t.} \quad
& \sum_{i \in \mathcal{I}} x_i^0 \le K_1 \\[0.3em]
& x_i^0 \ge m_i \delta_i^0, \qquad
  x_i^0 \le K_1 \delta_i^0
&& i \in \mathcal{I} \\[0.3em]
& \sum_{i \in \mathcal{I}} x_i^n \le K_2
&& n \in \mathcal{N}_1 \\[0.3em]
& x_i^n \ge m_i \delta_i^n, \qquad
  x_i^n \le K_2 \delta_i^n
&& i \in \mathcal{I},\ n \in \mathcal{N}_1 \\[0.3em]
& x_i^0 + x_i^{a(n)} = d_i^n + o_i^n - u_i^n
&& i \in \mathcal{I},\ n \in \mathcal{N}_2 \\[0.3em]
& x_i^n \in \mathbb{Z}_+, \qquad
  \delta_i^n \in \{0,1\}
&& i \in \mathcal{I},\ n \in \{0\} \cup \mathcal{N}_1 \\[0.3em]
& u_i^n,\ o_i^n \ge 0
&& i \in \mathcal{I},\ n \in \mathcal{N}_2.
\end{align*}

\subsubsection{Programmazione lineare stocastica multistadio con ricorso}
\paragraph{Generalizzazione ai modelli multistadio}
Le \emph{formulazioni multistadio sorgono come generalizzazione dei modelli a due stadi}.
Concettualmente, \textbf{si annidano funzioni di ricorso corrispondenti ai diversi stadi decisionali}.
In questo caso, le decisioni future possono dipendere dall’intera storia del processo stocastico dei fattori di rischio.
Si introduce la notazione
\[
\tilde{\xi}_{[t]} = (\tilde{\xi}_1,\tilde{\xi}_2,\dots,\tilde{\xi}_t),
\qquad
\tilde{\xi}_{[1]}=\tilde{\xi}_1.
\]
La decisione \emph{here-and-now} è $x_0\in X_0$ e si risolve
\[
\min_{x_0\in X_0}\left\{ f_0(x_0) + \mathbb{E}\big[Q(x_0,\tilde{\xi}_1)\big]\right\}.
\]
Il termine $Q(x_0,\xi_1)$ è definito ricorsivamente come
\[
Q(x_0,\xi_1)=\min_{x_1\in X_1(x_0,\xi_1)}
\left\{ f_1(x_1,\xi_1) + \mathbb{E}\!\left[Q\!\big(x_1,\tilde{\xi}_{[2]}\big)\mid \tilde{\xi}_1=\xi_1\right]\right\}.
\]
Allo stesso modo, per $t = 2, \ldots, T$, si ha
\[
Q_t\!\left(x_{t-1}, \tilde{\xi}_{[t]}\right)
=
\min_{x_t \in X_t(x_{t-1}, \tilde{\xi}_{[t]})}
\left\{
f_t\!\left(x_t, {\xi}_{[t]}\right)
+
\mathbb{E}
\left[
Q_{t+1}\!\left(x_t, \tilde{\xi}_{[t+1]}\right)
\;\middle|\;
\tilde{\xi}_{[t]} = \xi_{[t]}
\right]
\right\}.
\]
Possiamo definire la funzione di ricorso
\[
Q_{t+1}\!\left(x_t, {\xi}_{[t]}\right)
\;\doteq\;
\mathbb{E}
\left[
Q_{t+1}\!\left(x_t, \tilde{\xi}_{[t+1]}\right)
\;\middle|\;
\tilde{\xi}_{[t]} = \xi_{[t]}
\right],
\]
e riscrivere l’equazione precedente come
\[
Q_t\!\left(x_{t-1}, {\xi}_{[t]}\right)
=
\min_{x_t \in X_t(x_{t-1}, {\xi}_{[t]})}
\left\{
f_t\!\left(x_t, {\xi}_{[t]}\right)
+
Q_{t+1}\!\left(x_t, {\xi}_{[t]}\right)
\right\}.
\]
Nel problema dell’ultimo stadio, per $t = T$, si può semplicemente porre
\(
Q_T\!\left(x_T, {\xi}_{[T]}\right) \equiv 0.
\)\\
\hspace*{1.5em} Una formulazione alternativa,si ottiene enfatizzando
che le decisioni di ricorso sono in realtà funzioni (\emph{non anticipative}) del cammino
campionario osservato fino a quel momento, $x_t(\tilde{\xi}_{[t]})$.
Questo conduce alla formulazione funzionale
\[
\begin{aligned}
\min_{x_0,\,x_1(\cdot),\,\ldots,\,x_T(\cdot)}
\;&
\mathbb{E}
\Big[
f_0(x_0)
+
f_1\!\left(x_1(\tilde{\xi}_{[1]}), \tilde{\xi}_1\right)
+
\cdots
+
f_T\!\left(x_T(\tilde{\xi}_{[T]}), \tilde{\xi}_T\right)
\Big] \\[0.6em]
\text{s.t.}\quad
& x_0 \in X_0, \\[0.3em]
& x_t({\xi}_{[t]}),\, {\xi}_t \in
X_t\!\left(x_{t-1}({\xi}_{[t-1]}), {\xi}_{t-1}\right),
\qquad
t = 1, \ldots, T.
\end{aligned}
\]

\subsubsection{Un modello di pianificazione finanziaria multistadio con costi di transazione proporzionali} 
\paragraph{Asset--liability management su albero di scenario}
Si formula un modello di \textcolor{NavyBlue}{\textbf{asset--liability management}} in cui si scambia un insieme di
 strumenti finanziari (asset) per far fronte a un flusso di passività (liabilities). L’incertezza è rappresentata 
 mediante un albero di scenario.
\begin{itemize}[label=-]
    \item $L_n$ è la liability da soddisfare nel nodo $n\in N$.
    \item Non si considerano nuovi apporti di cassa lungo il percorso; l’unico modo per generare cassa è vendere asset.
    \item Gli asset sono indicizzati da $i\in I$ e $P^n_i$ è il prezzo dell’asset $i$ nel nodo $n$.
    \item Si considerano costi di transazione proporzionali (lineari): per acquistare o vendere si paga una percentuale $c$ del valore scambiato, sia in acquisto sia in vendita.
\end{itemize}
La radice è il nodo $n_0$, in cui si effettua la prima allocazione considerando le dotazioni iniziali $\bar{h}^{n_0}_i$.
 L’insieme delle foglie è $S$; i nodi di trading intermedi sono
\(
T = N \setminus \big(\{n_0\}\cup S\big).
\)
L’obiettivo è massimizzare l’utilità attesa della ricchezza terminale.
Si introducono  le variabili decisionali:
\begin{itemize}[label=-]
    \item $z^{n}_i$: quantità di asset $i$ acquistata nel nodo $n$.
    \item $y^{n}_i$: quantità di asset $i$ venduta nel nodo $n$.
    \item $x^{n}_i$: quantità detenuta di asset $i$ nel nodo $n$ dopo il ribilanciamento.
    \item $W^{\ell}$: ricchezza terminale nella foglia $s\in S$.
\end{itemize}
Le variabili $z^{n}_i$, $y^{n}_i$, $x^{n}_i$ sono definite per $n\in N\setminus S$ (nessun ribilanciamento alle foglie). Sia $U(\cdot)$ l’utilità non lineare della ricchezza.

\begin{align*}
\max \quad 
& \sum_{s \in S} \pi^s \, u(W^s) \\[0.4em]
\text{s.t.} \quad
& x_i^{n_0} = \bar{h}_i^{n_0} + z_i^{n_0} - y_i^{n_0},
&& i \in \mathcal{I} \\[0.3em]
& x_i^{n} = x_i^{a(n)} + z_i^{n} - y_i^{n},
&& i \in \mathcal{I},\ n \in \mathcal{T} \\[0.3em]
& (1-c)\sum_{i \in \mathcal{I}} P_i^{n} y_i^{n}
-
(1+c)\sum_{i \in \mathcal{I}} P_i^{n} z_i^{n}
= L^{n},
&& n \in \mathcal{N} \setminus S \\[0.3em]
& W^{s} = (1-c)\sum_{i \in \mathcal{I}} P_i^{s} x_i^{a(s)} - L^{s},
&& s \in S \\[0.3em]
& x_i^{n},\, z_i^{n},\, y_i^{n},\, W^{s} \ge 0.
\end{align*}

\paragraph{Osservazioni operative}
In pratica il modello verrebbe risolto ripetutamente con una logica \textcolor{NavyBlue}{\emph{rolling horizon}};
 il ruolo dell’utilità terminale è evitare effetti distorsivi di fine orizzonte, imponendo che 
 il portafoglio sia in una buona posizione alla fine dell’orizzonte di pianificazione. In un modello più realistico
  si dovrebbero includere eventuali contributi in ingresso e shortfall penalizzati rispetto alle liabilities.

\subsection{Modelli multiperiodali e multistadio}
\paragraph{Esplosione dell’albero di scenario nei modelli multistadio}
Un problema chiaro nei modelli di programmazione stocastica \textbf{multistadio} è la possibile 
\textbf{esplosione dell’albero di scenario}. Il numero di nodi necessario per rappresentare l’incertezza può diventare
 un fattore fortemente limitante.
Questo problema è meno rilevante per una struttura ad albero \textbf{lineare}.
 È però importante osservare che una struttura ad albero lineare 
  \textbf{non corrisponde a un modello multistadio}, bensì a un \textbf{modello a due stadi multiperiodale}.

\paragraph{Multiperiodo non significa multistage} È possibile avere un modello multiperiodale ma \textbf{statico} se tutte le decisioni sono prese \textbf{here-and-now} e poi implementate nei periodi successivi senza adattamento al flusso informativo.  
Al contrario, \textbf{multistage} implica \textbf{adattamento} delle decisioni nel tempo, soggetto a vincoli di \textbf{non-anticipatività} delle politiche decisionali.

\paragraph{Caso particolare del two-stage multiperiod}
La struttura è peculiare: è \textbf{a due stadi}, ma coinvolge \textbf{più periodi}. Apparentemente viola la non-anticipatività, perché dopo la prima diramazione successiva alla radice sarebbe possibile prevedere perfettamente il cammino futuro. Tuttavia, una tale struttura può avere senso quando:
\begin{itemize}[label=-]
    \item Si devono prendere \textbf{decisioni strutturali o di design} \textbf{here-and-now} che non possono essere adattate.
    \item Segue un flusso di decisioni di controllo nel tempo, per le quali non si può trarre vantaggio dalla struttura lineare dell’albero.
\end{itemize}
Il modello di pianificazione energetica della sezione successiva illustra un caso di questo tipo.

\subsubsection{Un modello multiperiodo a due stadi: Unit commitment}
\paragraph{Domanda aleatoria come processo discreto nel tempo}
Si consideri un modello stocastico di \textcolor{NavyBlue}{\textbf{unit commitment}}, problema di pianificazione energetica
 in cui l’incertezza della domanda è modellata come processo stocastico a tempo discreto
\[
\xi(\omega)=\big(d_1(\omega),\dots,d_T(\omega)\big),
\qquad
t\in\{1,\dots,T\},\;\omega\in\Omega,
\]
dove $T$ è la lunghezza dell’orizzonte di pianificazione e $\Omega$ è l’insieme (finito) dei cammini campionari. 
Si denoti con $\pi^\omega$ la probabilità dello scenario $\omega$.

\paragraph{Classi di generatori e struttura dei costi}
Si dispone di un insieme di $I$ classi di unità generatrici. Una unità della classe $i$ (con $i=1,\dots,I$) 
può produrre in un intervallo $[m_i,M_i]$ quando è attiva. Per ciascuna classe $i$ sono disponibili $a_i$ unità.
\\ \hspace*{1.5em} Si consideri un insieme di $I$ classi di unità di generazione, che possono fornire
un livello di output nell’intervallo $[m_i, M_i]$, $i \in [I]$, quando sono attive.
Per ciascuna classe $i$ sono disponibili $a_i$ unità.
Anche in questo caso, la potenza erogata è una variabile decisionale
\emph{semicontinua}, che può assumere valore nullo, ma che presenta
un livello minimo $m_i$ per un’unità attiva.
Se un’unità di generazione della classe $i$ è attiva al tempo $t$, si sostiene
un costo $E_i$ per mantenerla al livello minimo $m_i$, e un costo variabile
lineare $C_i$ per ogni unità addizionale di potenza erogata.
Si sostiene inoltre un costo di avviamento $s_i$ per l’accensione di un’unità
della classe $i$.
\\ \hspace*{1.5em} Viene introdotta una variabile decisionale intera non negativa $u_{it}$ che denota
il numero totale di unità della classe $i$ attive durante l’intervallo temporale $t$.
Data la struttura dei costi, non risulta necessario rappresentare ciascuna singola unità.
Si considera inoltre una variabile intera non negativa $s_{it}$ che rappresenta
il numero di unità della classe $i$ che vengono accese all’inizio
dell’intervallo temporale $t$.
Tali variabili costituiscono decisioni di \emph{primo stadio}.
Al contrario, la variabile non negativa $q_{it}^{\omega}$, che rappresenta
l’output totale della classe $i$ durante l’intervallo temporale $t$,
è una variabile decisionale di \emph{secondo stadio}, indicizzata per scenario.
Il \textcolor{NavyBlue}{\textbf{modello}} può essere formulato come segue
\begin{align*}
\min \quad 
& \sum_{i \in [I],\, t \in [T]}
\left( E_i u_{it} + F_i s_{it} \right)
+ \sum_{\omega \in \Omega} \pi^{\omega}
\left[
\sum_{i \in [I],\, t \in [T]}
C_i \left( q_{it}^{\omega} - m_i u_{it} \right)
\right] \\[0.4em]
\text{s.t.} \quad
& \sum_{i \in [I]} q_{it}^{\omega} \ge d_t(\omega),
&& t \in [T],\ \omega \in \Omega \\[0.3em]
& m_i u_{it} \le q_{it}^{\omega} \le M_i u_{it},
&& i \in [I],\ t \in [T],\ \omega \in \Omega \\[0.3em]
& s_{it} \ge u_{it} - u_{i,t-1},
&& i \in [I],\ t \in [T] \\[0.3em]
& u_{it} \le a_i,
&& i \in [I],\ t \in [T] \\[0.3em]
& u_{it} \in \mathbb{Z}_+,\ 
s_{it} \in \mathbb{Z}_+,\ 
q_{it}^{\omega} \ge 0,
&& i \in [I],\ t \in [T],\ \omega \in \Omega.
\end{align*}

\paragraph{Osservazioni critiche sul modello}
Si notano due aspetti rilevanti
\begin{itemize}[label=-]
    \item \textbf{Fattibilità e scenari estremi.} Se il vincolo di domanda è imposto come vincolo rigido in ogni scenario, la soluzione può diventare molto costosa e guidata da pochi scenari improbabili con picchi di domanda elevati. Come nel plant location model, può essere opportuno introdurre una formulazione elastica, ad esempio includendo una unità “di ultima istanza” con capacità illimitata (fornitore esterno) o una penalità per domanda non soddisfatta.
    \item \textbf{Flusso informativo.} Il modello è multiperiodale ma a due stadi: dopo aver osservato la prima domanda al tempo $t=1$, si assume di conoscere tutte le domande per $t\in\{2,\dots,T\}$. Questo non è realistico e si potrebbe pensare a un modello multistadio con vincoli di non-anticipatività. Tuttavia, qui non c’è modo di sfruttare tale informazione, perché le variabili di design sono \emph{here-and-now}. Inoltre, nel modello non esiste uno stato energetico (l’energia non è immagazzinabile), e quindi i periodi sono indipendenti tra loro in termini di potenza erogata.
\end{itemize}

\subsection{Generazione di scenari e stabilità nella programmazione stocastica}
\paragraph{Ruolo centrale dell’albero di scenario}
Nei modelli di programmazione stocastica \textbf{multistadio}, l’elemento chiave è l’\textcolor{NavyBlue}{\textbf{albero di scenario}}:
 la qualità della soluzione dipende in modo critico da quanto bene l’albero rappresenta l’incertezza. In linea di principio si potrebbe campionare un modello dinamico dei fattori di rischio tramite \textbf{simulazione Monte Carlo}, ma sorgono ulteriori difficoltà.

\paragraph{Complicazioni principali}
\begin{itemize}[label=-]
    \item La \textcolor{NavyBlue}{\textbf{dimensione del campione}} deve restare contenuta, perché si associano variabili decisionali agli stati e risolvere un problema di ottimizzazione stocastica (non solo calcolare una media campionaria) può essere computazionalmente oneroso.
    \item Le decisioni devono essere \textcolor{NavyBlue}{\textbf{non-anticipative}}: ciò richiede una struttura ad albero soggetta a crescita \textbf{esponenziale}. Ad esempio, con 100 rami per nodo si ottengono un milione di scenari dopo tre passi.
    \item Non si sta solo stimando un valore atteso, ma si stanno \textcolor{NavyBlue}{\textbf{prendendo decisioni}} risolvendo un problema di ottimizzazione: qualunque \textbf{bias} o incoerenza nell’albero verrà sfruttata dal solver, producendo una soluzione magari buona \emph{in-sample} ma debole \emph{out-of-sample}.
\end{itemize}
Qualunque strategia di generazione scenari si scelga, è quindi importante controllare la \textbf{stabilità} della soluzione risultante.

\paragraph{Due classi di approcci alla generazione di scenari}
Si distinguono due approcci fondamentali
\begin{itemize}[label=-]
    \item \textcolor{NavyBlue}{\textbf{Generazione stocastica di scenari.}} Basata su campionamento casuale \textbf{Monte Carlo} e quindi sulla “forza bruta” di una grande numerosità campionaria. È più gestibile nei modelli \textbf{a due stadi} che nei multistadio; le prestazioni possono essere migliorate con tecniche di \textbf{riduzione della varianza}, ad esempio \emph{importance sampling}.
    \item \textcolor{NavyBlue}{\textbf{Generazione deterministica di scenari.}} Invece della forza bruta, si selezionano scenari in modo “intelligente” tramite metodi quali \textbf{quadratura gaussiana}, \textbf{sequenze a bassa discrepanza} (ad es.\ \textbf{Sobol}) o procedure di generazione \textbf{ottimizzata}.
\end{itemize}

\paragraph{Interpretazione come integrazione numerica (quadratura)}
La generazione scenari può essere vista come un problema di \textbf{integrazione numerica}. Se $f(x,\tilde{\xi})$ dipende dalle decisioni $x$ e da variabili aleatorie $\tilde{\xi}$ con densità congiunta $h(\xi)$, allora
\[
F(x)=
\int f(x,\xi)\,h(\xi)\,d\xi \approx \frac{1}{S}\sum_{s=1}^{S} f(x,\xi_s).
\]
In alternativa, si può usare un insieme (anche deterministico) di punti con pesi $\{(\pi_s,\xi_s)\}$:
\[
F(x)\approx \sum_{s=1}^{S'} \pi_s\, f(x,\xi_s),
\qquad S'<S,
\]
interpretando le coppie $(\pi_s,\xi_s)$ come una \textbf{distribuzione discreta} che approssima quella continua.

\paragraph{Moment/property matching e alternative}
Un’idea comune è scegliere scenari e probabilità in modo da riprodurre (approssimativamente) alcune \textbf{proprietà} della distribuzione, ad esempio i \textbf{momenti} (\emph{moment matching}). Questo approccio è stato criticato perché è possibile costruire distribuzioni diverse che condividono i primi momenti. Un’alternativa è utilizzare \textbf{metriche di distanza} tra distribuzioni: fissata una topologia dell’albero, si cercano valori e probabilità che minimizzano una distanza rispetto alla distribuzione “vera”, oppure si parte da un grande insieme di scenari e si applica \textbf{scenario reduction} per ottenere un albero più gestibile. D’altra parte, spesso ciò che conta non è la distanza tra distribuzioni, ma la \textbf{qualità della soluzione} (ad es.\ in un modello mean--variance può essere sufficiente approssimare bene i primi due momenti).

\paragraph{Linee guida pratiche per progettare l’albero}
\begin{itemize}[label=-]
    \item Se l’obiettivo è una decisione robusta di primo stadio, conviene spesso usare un branching \textbf{ricco al primo stadio} e più \textbf{limitato negli stadi successivi} (in finanza, imponendo che l’albero sia \textbf{arbitrage-free}).
    \item Si può ridurre la complessità usando \textbf{passi temporali non uniformi} (più fitti all’inizio, più radi in seguito).
    \item Si può limitare il numero di stadi \textbf{aumentando l’obiettivo} con un termine che valuta la qualità dello \textbf{terminale}, riducendo decisioni miopi e permettendo alberi più piccoli.
\end{itemize}

\subsubsection{In-- and out--of--sample stability}

\paragraph{Perché serve controllare la stabilità}
Qualunque strategia di generazione scenari venga impiegata, è importante verificare la \textbf{stabilità} della soluzione. In pratica, l’ottimizzazione usa un albero approssimato e quindi occorre capire quanto l’output dipenda da come l’albero è stato generato.

\paragraph{Problema “esatto” e problema approssimato}
Il problema stocastico “esatto” può essere scritto come
\[
\min_{x\in X}\; \mathbb{E}_{\mathbb{P}}\!\left[f(x,\tilde{\xi})\right],
\]
dove $P$ è la \textbf{misura vera} dei fattori di rischio. In pratica si risolve un problema basato su un albero di scenario approssimato $T$
\[
\min_{x\in S}\; \widehat{f}(x;T),
\]
dove $\widehat{f}$ rappresenta una stima del valore atteso indotta dall’albero.

\paragraph{Stabilità del valore obiettivo}
Campionando (o costruendo) alberi diversi si ottengono soluzioni ottime diverse e valori obiettivo diversi. Spesso è più informativo controllare la stabilità del \textbf{valore dell’obiettivo} (piuttosto che della soluzione), perché quando l’obiettivo è piatto soluzioni anche molto differenti possono avere prestazioni simili.

\paragraph{In-sample stability}
La \textcolor{NavyBlue}{\textbf{stabilità \emph{in-sample}}} richiede che, generando due alberi $T_1$ e $T_2$, i valori obiettivo ottenuti non differiscano troppo:
\(
\widehat{f}(x^{*}_{1};T_{1}) \approx \widehat{f}(x^{*}_{2};T_{2}).
\)
Se l’albero è generato in modo deterministico, si può comunque confrontare la stabilità variando leggermente la struttura di branching per verificare che sia sufficientemente ricca.

\paragraph{Out-of-sample stability}
La \textcolor{NavyBlue}{\textbf{stabilità \emph{out-of-sample}}} confronta la performance reale attesa delle soluzioni sotto la distribuzione vera:
\(
\mathbb{E}_{\mathbb{P}}\!\left[f(x^{*}_{1},\tilde{\xi})\right]
\approx
\mathbb{E}_{\mathbb{P}}\!\left[f(x^{*}_{2},\tilde{\xi})\right].
\)
Se gli alberi sono affidabili, le prestazioni reali non dovrebbero cambiare in modo significativo.

\paragraph{Valutazione out-of-sample: due stadi vs multistadio}
Nei modelli a \textbf{due stadi} la verifica out-of-sample è relativamente semplice: si fissa la soluzione di primo stadio e si risolvono molti problemi di secondo stadio corrispondenti a diverse realizzazioni di $\tilde{\xi}$. Nei modelli \textbf{multistadio}, invece, una valutazione realistica richiede tipicamente una simulazione \textbf{rolling horizon}, che è computazionalmente costosa.

\paragraph{Rolling horizon e shrinking horizon}
\begin{itemize}[label=-]
    \item \textcolor{NavyBlue}{\textbf{Sliding/rolling horizon}}: si risolve un problema con $H$ stadi, si applica la prima decisione, si osserva una realizzazione out-of-sample e si risolve di nuovo un problema con $H$ stadi a partire dal periodo successivo.
    \item \textcolor{NavyBlue}{\textbf{Shrinking horizon}}: si risolve con $H$ stadi, si applica la prima decisione, si osserva una realizzazione e poi si risolve con $H-1$ stadi, riducendo progressivamente l’orizzonte. Lo shrinking horizon è più economico quando l’orizzonte di valutazione è ben definito.
\end{itemize}

\paragraph{Test economico alternativo: scambio degli alberi}
Un controllo più economico consiste nel generare soluzioni da due alberi $T_1$ e $T_2$ e \textbf{valutarle scambiando gli alberi}, verificando se
\(
\widehat{f}(x^{*}_{1};T_{2}) \approx \widehat{f}(x^{*}_{2};T_{1}).
\)

\paragraph{Osservazione finale: limite operativo dello stochastic programming}
La necessità di valutazione out-of-sample evidenzia che la programmazione stocastica produce decisioni esplicite \textbf{associate ai nodi dell’albero}; non fornisce direttamente una politica generale facilmente applicabile quando l’incertezza si realizza fuori dagli scenari previsti. Approcci come \textbf{programmazione dinamica} e \textbf{decision rules} restituiscono invece decisioni in \textbf{feedback}, più adatte alla simulazione e alla valutazione delle politiche.

\subsection{Convessità nei modelli di programmazione stocastica}
\paragraph{Vincoli probabilistici congiunti}
Si consideri un vincolo probabilistico congiunto della forma
\(
\mathbb{P}\bigl(\{\omega \mid \mathbf{g}(\mathbf{x};\boldsymbol{\xi}(\omega)) \le \mathbf{0}\}\bigr) \ge 1 - \alpha,
\)
dove $\mathbf{g}$ è una funzione vettoriale.
Un punto $\hat{\mathbf{x}}$ è ammissibile se l’insieme
\(
\mathbb{S}(\hat{\mathbf{x}}) = \{\omega \mid \mathbf{g}(\hat{\mathbf{x}};\boldsymbol{\xi}(\omega)) \le \mathbf{0}\}
\)
ha misura di probabilità almeno pari a $1-\alpha$.\\
\hspace*{1.5em} Sia $\mathcal{F}$ il campo di tutti gli eventi e sia $\mathcal{G} \subseteq \mathcal{F}$ la collezione degli eventi con probabilità almeno $1-\alpha$.  
Allora $\hat{x}$ è \emph{ammissibile} se esiste almeno un evento $G \in \mathcal{G}$ tale che
\(
\mathbf{g}(\hat{\mathbf{x}};\boldsymbol{\xi}(\omega)) \le \mathbf{0}
\quad \forall \,\omega \in G,
\)
ossia
\[
\hat{\mathbf{x}} \in \bigcap_{\omega \in G}
\left\{\mathbf{x} \mid \mathbf{g}(\mathbf{x};\boldsymbol{\xi}(\omega)) \le \mathbf{0}\right\}.
\]

\paragraph{Struttura dell’insieme ammissibile}
L'\textcolor{NavyBlue}{\textbf{insieme ammissibile}} complessivo può quindi essere scritto come
\[
X =
\bigcup_{G \in \mathcal{G}}
\bigcap_{\omega \in G}
\left\{\mathbf{x} \mid \mathbf{g}(\mathbf{x};\boldsymbol{\xi}(\omega)) \le \mathbf{0}\right\}.
\]
Anche se per ogni $\omega$ l’insieme
\(
\{\mathbf{x} \mid \mathbf{g}(\mathbf{x};\boldsymbol{\xi}(\omega)) \le \mathbf{0}\}
\)
è convesso, non ci si può aspettare che l’insieme $X$ sia convesso in generale, poiché l’unione di insiemi convessi non è necessariamente convessa.
La convessità del problema complessivo dipende dal tipo di distribuzione (continua o discreta), dalla sua funzione di distribuzione cumulativa, dalla natura dei vincoli (congiunti o disgiunti) e dalla struttura delle funzioni in $\mathbf{g}$ (lineari, convesse/concave o arbitrarie).

\paragraph{Vincolo probabilistico lineare con distribuzione normale}
Si consideri un vincolo lineare casuale
\(
\mathbf{a}^T \mathbf{x} \le b,
\)
dove $\mathbf{a} \sim \mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$, e si richieda
\(
\mathbb{P}\{\mathbf{a}^T \mathbf{x} \le b\} \ge \eta.
\)
Per un dato vettore $\mathbf{x}$, la variabile casuale $\mathbf{a}^T \mathbf{x}$ è distribuita come
\(
\mathbf{a}^T \mathbf{x} \sim \mathcal{N}(\nu,\sigma^2),
\)
dove $\nu = \boldsymbol{\mu}^T \mathbf{x}$ e $\sigma^2 = \mathbf{x}^T \boldsymbol{\Sigma}\mathbf{x}$.\\
\hspace*{1.5em} Usando la funzione di distribuzione cumulativa della normale standard $\Phi(\cdot)$, il vincolo può essere riscritto come
\[
\mathbb{P}\!\left\{
\frac{\mathbf{a}^{T}\mathbf{x} - \nu}{\sigma}
\le
\frac{b - \nu}{\sigma}
\right\}
=
\Phi\!\left(\frac{b - \nu}{\sigma}\right)
\ge \eta
\;\Longleftrightarrow\;
\frac{b - \nu}{\sigma}
\ge
\Phi^{-1}(\eta).
\]
Assumendo $\eta > 0.5$, quindi $\Phi^{-1}(\eta) > 0$, il vincolo diventa
\[
\boldsymbol{\mu}^T \mathbf{x} + \Phi^{-1}(\eta)\,\|\boldsymbol{\Sigma}^{1/2}\mathbf{x}\|_2 \le b,
\]
che è un vincolo conico del secondo ordine.  
Ne segue che un problema di programmazione lineare con vincoli probabilistici disgiunti di questo tipo è un \textcolor{NavyBlue}{\textbf{problema convesso di tipo SOCP}}.

\paragraph{Convessità della funzione di ricorso}
Si consideri ora la funzione di ricorso
\(
Q(\mathbf{x}) = \mathbb{E}[Q(\mathbf{x};\boldsymbol{\tilde{\xi}})]
\)
nel caso di costo di ricorso deterministico, con
\[
Q(\mathbf{x};\boldsymbol{\xi}) = \min_{\mathbf{y}}
\left\{
\mathbf{q}^T \mathbf{y} \mid \mathbf{W}\mathbf{y} = \mathbf{h}(\boldsymbol{\xi}) - \mathbf{T}(\boldsymbol{\xi})\mathbf{x},\; \mathbf{y} \ge \mathbf{0}
\right\}.
\]
Il dominio effettivo di $Q(\mathbf{x})$ è costituito dai vettori $\mathbf{x}$ ammissibili per i vincoli di primo stadio tali che $Q(\mathbf{x}) < +\infty$, ossia per cui il problema di secondo stadio è (quasi sicuramente) ammissibile.
\\ \hspace*{1.5em} Per \textcolor{NavyBlue}{\textbf{dualità della programmazione lineare}} si ottiene
\[
Q(\mathbf{x};\boldsymbol{\xi}) =
\max_{\boldsymbol{\pi}}
\left\{
[\mathbf{h}(\boldsymbol{\xi})-\mathbf{T}(\boldsymbol{\xi})\mathbf{x}]^T \boldsymbol{\pi}
\mid \mathbf{W}^T \boldsymbol{\pi} \le \mathbf{q}
\right\}.
\]
Si denoti con
\(
\Pi = \{\boldsymbol{\pi} \mid \mathbf{W}^T \boldsymbol{\pi} \le \mathbf{q}\}
\)
l’\textbf{insieme ammissibile del duale}, che è non casuale poiché $\mathbf{q}$ è deterministico.

\paragraph{Dimostrazione della convessità}
Siano $\mathbf{x}_1,\mathbf{x}_2 \in D$, $\lambda \in [0,1]$, e $\mathbf{x}_\lambda = \lambda \mathbf{x}_1 + (1-\lambda)\mathbf{x}_2$. Allora
Si considerino $\mathbf{x}^1,\mathbf{x}^2 \in \mathcal{D}$, $\lambda \in [0,1]$, e
$\mathbf{x}_\lambda = \lambda \mathbf{x}^1 + (1-\lambda)\mathbf{x}^2$.

\[
\begin{aligned}
Q(\mathbf{x}_\lambda)
&\overset{\mathrm{def}}{=}
\int Q(\mathbf{x}_\lambda,\boldsymbol{\xi})\, dP \\[1mm]
&=
\int \max_{\boldsymbol{\pi} \in \Pi}
\left\{
\big[\mathbf{h}(\boldsymbol{\xi}) - \mathbf{T}(\boldsymbol{\xi})\mathbf{x}_\lambda\big]^T
\boldsymbol{\pi}
\right\} dP \\[1mm]
&=
\int \max_{\boldsymbol{\pi} \in \Pi}
\left\{
\lambda
\big[\mathbf{h}(\boldsymbol{\xi}) - \mathbf{T}(\boldsymbol{\xi})\mathbf{x}^1\big]^T
\boldsymbol{\pi}
+
(1-\lambda)
\big[\mathbf{h}(\boldsymbol{\xi}) - \mathbf{T}(\boldsymbol{\xi})\mathbf{x}^2\big]^T
\boldsymbol{\pi}
\right\} dP \\[1mm]
&\le
\lambda
\int
\max_{\boldsymbol{\pi} \in \Pi}
\left\{
\big[\mathbf{h}(\boldsymbol{\xi}) - \mathbf{T}(\boldsymbol{\xi})\mathbf{x}^1\big]^T
\boldsymbol{\pi}
\right\} dP
+
(1-\lambda)
\int
\max_{\boldsymbol{\pi} \in \Pi}
\left\{
\big[\mathbf{h}(\boldsymbol{\xi}) - \mathbf{T}(\boldsymbol{\xi})\mathbf{x}^2\big]^T
\boldsymbol{\pi}
\right\} dP \\[1mm]
&=
\lambda Q(\mathbf{x}^1) + (1-\lambda) Q(\mathbf{x}^2).
\end{aligned}
\]
Pertanto la funzione di ricorso è convessa.

\paragraph{Osservazioni finali}
La dimostrazione precedente risulta particolarmente semplice poiché l’\textbf{insieme ammissibile del problema duale è non casuale}. In un contesto più generale, tuttavia, la \textbf{convessità della funzione di ricorso} può essere dimostrata anche senza questa assunzione.
In particolare, la funzione di ricorso è \textbf{tipicamente continua} quando le variabili aleatorie seguono \textbf{distribuzioni continue}, mentre nel caso di \textbf{distribuzioni discrete} essa assume una struttura \textbf{poliedrica}. In entrambi i casi è possibile applicare il \textbf{metodo dei piani di taglio di Kelley}. Inoltre, grazie alla \textbf{struttura del problema}, tali piani di taglio possono essere generati in modo efficiente tramite \textbf{strategie di decomposizione}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF 2 COMPLESSITA' COMPUTAZIONALE %%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{CAP 2 -- Elementi di complessità computazionale}

\paragraph{Recap} La \textbf{complessità computazionale} dipende dalla dimensione dell’istanza e può crescere
in modo fattoriale, esponenziale o polinomiale. Piccole variazioni nella struttura di un
problema possono modificare radicalmente la sua complessità, come nel caso dei
problemi di scheduling $1//L_{\max}$ e $1/r_i/L_{\max}$.

La \textbf{teoria della NP-completezza} introduce le classi $P$, $NP$, $NPH$ e $NPC$ e il
concetto di riduzione polinomiale per confrontare la difficoltà dei problemi. I
problemi NP-completi sono equivalenti in termini di complessità e l’esistenza di un
algoritmo polinomiale per uno di essi implicherebbe $P = NP$.

La riduzione da subset-sum mostra che la versione decisionale di $1/r_i/L_{\max}$ è
NP-completa e che il corrispondente problema di ottimizzazione è NP-difficile. \textbf{La
codifica dei dati è cruciale}: l’algoritmo per il knapsack ha complessità
pseudo-polinomiale, poiché dipende dal valore numerico dei dati e non dalla loro
lunghezza binaria.


\paragraph{Complessità di problemi e algoritmi}
Si vuole caratterizzare la complessità in funzione della dimensione di un problema. Occorre distinguere la complessità degli algoritmi da quella dei problemi.\\
Nel caso di un algoritmo caratterizzato da un numero finito di passi, possiamo valutare (eventualmente nel caso peggiore) il \textbf{numero di operazioni elementari} in funzione della dimensione $n$ del problema.
\begin{itemize}[label=-]
\item \textbf{Algoritmi enumerativi}: algoritmi che considerano tutte le \textbf{permutazioni} di $n$ oggetti. La loro complessità è $O(n!)$ ed è certamente non praticabile per valori di $n$ anche moderatamente grandi.
\item \textbf{Algoritmi di assegnamento esaustivo}: algoritmi che valutano tutti gli \textbf{assegnamenti possibili} di $n$ variabili binarie. In questo caso la complessità cresce in modo esponenziale ed è pari a $O(2^n)$.
\item \textbf{Algoritmi polinomiali}: un tipico esempio è rappresentato dagli \textbf{algoritmi di ordinamento} di $n$ oggetti. Gli algoritmi più semplici hanno complessità $O(n^2)$, mentre algoritmi più efficienti raggiungono complessità $O(n\log_2 n)$.
\end{itemize}
Nel caso di \textbf{algoritmi iterativi} che generano una sequenza di soluzioni, si può cercare di caratterizzare la velocità di convergenza (es., lineare o quadratica).

\paragraph{Complessità intrinseca di un problema}
Una questione più sottile si pone quando si vuole caratterizzare la complessità intrinseca di un problema.\\
Per comprendere la natura della questione, consideriamo un semplice \textbf{problema di scheduling di $n$ job su macchina singola}. 
Indichiamo con $p_j$ il tempo necessario per il job $j = 1, \dots, n$, e con $d_j$, $j = 1, \dots, n$ la sua data di consegna (\textbf{due date}).\\
La soluzione è una sequenza di job, ovvero una permutazione $\sigma$ in cui $\sigma(k)$ è l’indice del job in posizione $k$. I tempi di completamento sono
\[
\begin{aligned}
C_{\sigma(1)} &= p_{\sigma(1)},\\
C_{\sigma(k)} &= C_{\sigma(k-1)} + p_{\sigma(k)}, \quad k = 2, \dots, n.
\end{aligned}
\]
Si vuole minimizzare la massima lateness,
\[
L_{\max} \dot{=} \max_{j \in [n]} L_j,
\]
dove
\(
L_j \dot{=} C_j - d_j.
\)
Tale problema viene indicato con la stringa \textbf{$1//L_{\max}$}.

\textbf{Teorema (regola EDD -- Earliest Due Date).}
Per il problema $1//L_{\max}$ esiste una soluzione ottima in cui $d_{\sigma(k)} \le d_{\sigma(k+1)}$.\\
\textbf{Dimostrazione.}
Supponiamo che esista una soluzione ottima in cui, per due job consecutivi in sequenza, prima $i = \sigma(k)$ e poi $j = \sigma(k+1)$, si abbia $d_i > d_j$. Indichiamo con $C_i$ e $C_j = C_i + p_j$ i due tempi di completamento nella soluzione corrente, per la quale abbiamo due valori di lateness $L_i = C_i - d_i$ e $L_j = C_j - d_j$.\\
Se scambiamo i due job, avremo $C'_j < C_j$ e $C'_i = C_j$. Per il job $j$, che anticipiamo, abbiamo $C'_j < C_j$ e $L'_j < L_j$, e quindi la lateness del job $j$ non può che migliorare nella nuova soluzione. Per il job $i$, che viene spostato in avanti nella sequenza, abbiamo
\[
L'_i = C'_i - d_i = C_j - d_i < C_j - d_j = L_j,
\]
quindi il job $i$ peggiora la sua lateness, che però non potrà essere peggio della vecchia lateness del job $j$. La nuova soluzione migliora quella precedente, contraddicendo l’assunzione di ottimalità. \\

Abbiamo quindi un algoritmo di complessità polinomiale per il problema.
\textbf{Ma cosa accade se complichiamo leggermente il problema, introducendo dei tempi di rilascio (release time o ready time) $r_i$, $i \in [n]$, dei job?} Per il problema $1/r_i/L_{\max}$ non sono noti algoritmi di complessità polinomiale, ed è facile costruire controesempi alla regola EDD (da adattare comunque alla disponibilità di job).\\
Esistono algoritmi \textbf{branch-and-bound} per il problema $1/r_i/L_{\max}$ (quindi complessità esponenziale). Non si conosce un algoritmo di complessità polinomiale per questo problema di ottimizzazione combinatoria (e per molti altri), ma neppure è stato dimostrato che esso non possa esistere.

\subsection{Caratterizzazione della complessità di problemi: classi P, NPC e NPH}
Esiste una classe molto ampia di problemi di ottimizzazione per cui non sono disponibili algoritmi
di complessità polinomiale. Tuttavia, la questione dell’esistenza o meno di un algoritmo di
complessità polinomiale per essi rimane aperta.\\
La \textbf{teoria della NP-completezza} ci permette di dare una risposta parziale alla questione,
mostrando come questi problemi siano equivalenti tra di loro, nel senso che un algoritmo di 
complessità polinomiale per anche uno solo di essi fornirebbe un algoritmo di complessità
polinomiale per tutti i problemi di una classe molto ampia.\\
Il fatto che decenni di ricerca sulla soluzione di tutti questi problemi non abbiano prodotto 
un algoritmo polinomiale \textbf{suggerisce che esso in effetti non esiste}.\\
Occorre distinguere \textbf{problemi di decisione} e \textbf{problemi di ottimizzazione}.
Esempi di problema di decisione sono i seguenti.

\textbf{Problema $K_0$ (subset sum)}
Dati $n+1$ numeri interi positivi $a_1, a_2, \dots, a_n$ e $b$, esiste un sottoinsieme 
$J \subseteq [n]$ tale che
\(
\sum_{i \in J} a_i = b
\)?

\textbf{Problema $LS_0$}
Dato un \textbf{problema di lot sizing multiprodotto}, con tempi e costi di setup, esiste una
soluzione ammissibile rispetto al soddisfacimento della domanda e ai vincoli di capacità
produttiva?\\
Data una specifica istanza di un problema di decisione, la risposta è sì oppure no.\\
Dal punto di vista teorico, è più agevole trattare problemi di decisione, \textbf{ma è facile vedere il
legame tra problemi di ottimizzazione e problemi di decisione}. Dato un problema di ottimizzazione
\(
\min_{x \in S} f(x),
\)
\textbf{possiamo definirne una versione decisionale}, scegliendo un numero $k$ e chiedendoci se esiste
$x \in S$ tale che $f(x) \le k$, dove $k$ è un numero intero.
Indichiamo con $PO$ il problema di ottimizzazione, e con $PD$ il corrispondente problema di decisione.\\
Se abbiamo a disposizione un algoritmo efficiente per $PO$, allora possiamo risolvere in modo 
efficiente anche $PD$: basta risolvere il problema di ottimizzazione e verificare se 
$f(x^*) \le k$. Questo ci permette di scrivere
\textbf{\(
PD \rightarrow PO,
\)}
nel senso che \textbf{il problema di decisione può essere ricondotto al problema di ottimizzazione}.\\
\textbf{Non è detto che tale trasformazione sia conveniente}, ma possiamo escludere che $PO$ sia facile
se $PD$ è difficile, perché un ipotetico algoritmo efficiente per $PO$ risolverebbe anche
facilmente $PD$.\\
Quindi, per dimostrare che un problema di ottimizzazione è difficile, può bastare dimostrare che
è difficile il corrispondente problema di decisione.\\
D’altro canto, un algoritmo di decisione efficiente potrebbe, in certi casi, essere utilizzato
per risolvere il problema di ottimizzazione. Se la funzione di costo in $PO$ assume valori interi
non negativi, e abbiamo un upper bound $U$ sul costo ottimo, possiamo applicare una procedura di
bisezione.\\
Indichiamo con $P$ la \textbf{classe dei problemi di decisione per cui esiste un algoritmo di complessità
polinomiale}, in grado di risolvere tutte le possibili istanze del problema.
Con questo vogliamo dire che il numero di passi, e quindi la complessità temporale dell’algoritmo
è limitata superiormente da una funzione polinomiale dello spazio di memoria necessario per
descrivere ogni istanza del problema.
Un tale algoritmo è in grado di fare due cose: \textbf{generare una soluzione} e \textbf{verificarne la correttezza}.
Esistono problemi, come $K_0$, per cui la prima parte del compito è difficile, ma la seconda no.
Se enumeriamo, mediante un albero di ricerca, tutti i possibili sottoinsiemi $J$, possiamo
verificare se una specifica istanza ha risposta positiva o negativa, ma chiaramente tale
algoritmo ha \textbf{complessità esponenziale}.\\
Tuttavia, se disponessimo di un ipotetico \textbf{calcolatore non deterministico}, in grado di eseguire
un numero infinito di processi di calcolo in parallelo, saremmo in grado di risolvere il 
problema in tempo polinomiale.

\paragraph{Classe NP}
Si definisce \textbf{classe $NP$} l’insieme dei problemi di decisione le cui istanze che hanno risposta 
positiva sono verificabili in tempo polinomiale.\\
Per definizione, $P \subseteq NP$. Una questione meno ovvia è se valga $P \equiv NP$ o
$P \subset NP$ in senso stretto. È ragionevole, da questo punto di vista, cercare di caratterizzare la sottoclasse dei problemi 
più difficili in $NP$.

\paragraph{Riduzione in tempo polinomiale}
Siano $P$ e $Q$ due problemi di decisione, per cui ogni istanza $I_P$ di $P$ può essere
trasformata in tempo polinomiale in un’istanza $I_Q$ di $Q$ tale che $I_P$ ha risposta positiva
se e solo se $I_Q$ ha risposta positiva.
Diremo che $P$ è riducibile in tempo polinomiale a $Q$, e useremo la notazione
\(
P \prec Q.
\))
La notazione $P \prec Q$ sottolinea che la complessità di $P$ non è maggiore della
complessità di trasformare $P$ in $Q$ e poi risolvere $Q$,
\[
\text{compl}(P) \le \text{compl}(Q) + \text{compl}(P \to Q).
\]
Se la trasformazione ha una complessità trascurabile, la riduzione di $P$ a $Q$ mostra
che $Q$ non è più facile di $P$. Se $P$ è difficile e $P \prec Q$, $Q$ non può essere facile.
Altrimenti, potremmo trasformare un’istanza di $P$ in una di $Q$, e poi applicare
l’algoritmo efficiente per $Q$.

\paragraph{Problemi NP-difficili}
Un problema di decisione $P$ è detto \textbf{NP-difficile} (\emph{NP-hard}) se ogni
problema nella classe $NP$ è riducibile a $P$.
Indichiamo con \textbf{$NPH$} la classe dei problemi NP-difficili.

\paragraph{Problemi NP-completi}
Un problema di decisione $P$ è detto NP-completo se è in Np ed è NP-hard. Indichiamo con NNPC la classe dei problemi NP-completi.
Le implicazioni pratiche di tali definizioni sono:
\begin{enumerate}
    \item Un problema NP-difficile non è più facile di un problema qualsiasi in NP.
    \item La classe NPC è la classe dei problemi più difficili in NP.
\end{enumerate}
Per dimostrare che un problema di decisione P è NP-completo, occorre dimostrare:
\begin{itemize}[label=-]
    \item Che $P$ è in NP;
    \item Che un problema NP-completo $Q$ può essere ridotto in tempo polinomiale a P.
\end{itemize}

Osserviamo che, dato che $Q$ è NP-completo, $P \prec Q$, e se trascuriamo la
complessità della trasformazione, questo implica
\(
\text{compl}(P) \le \text{compl}(Q).
\)
Ma il secondo passo della dimostrazione di NP-completezza, ovvero dimostrare che
$Q \prec P$, implica anche che
\(
\text{compl}(Q) \le \text{compl}(P).
\)
Le due disuguaglianze, sempre a meno della complessità della trasformazione,
mostrano che
\(
\text{compl}(Q) = \text{compl}(P).
\)

In altre parole, \textbf{i problemi della classe NPC sono equivalenti in termini di complessità
computazionale, e un algoritmo polinomiale per uno di essi permetterebbe di risolverli
tutti in tempo polinomiale}. Avremmo quindi $P = NP$, ipotesi non troppo plausibile a
causa dell’equivalenza di una vasta classe di problemi per i quali non è noto un
algoritmo di complessità polinomiale, nonostante essi siano stati oggetto di ampio
studio nel corso degli anni.\\
Se accettiamo l’ipotesi $P \neq NP$, possiamo rappresentare le relazioni tra le classi
$P$, $NP$ e $NPC$ come in figura. Essa ipotizza una gerarchia per cui la classe $P$
sarebbe la classe dei problemi più facili in $NP$, e $NPC$ quella dei problemi più
difficili.\\
Tutti i problemi in $NP$ si possono trasformare nel problema $Q \in NPC$. Se
$P \in NP$, per dimostrare che $P \in NPC$, occorre trasformare $Q$ in $P$.

Se dimostriamo che un problema $P$ è in $NPC$, questo può a sua volta essere
trasformato in altri problemi, permettendoci di ampliare la classe dei problemi noti in
$NPC$. Il punto critico, chiaramente, è trovare l’innesco della catena, ovvero il primo
problema in $NPC$, al quale tutti i problemi in $NP$ possono essere ricondotti.

Il \textbf{teorema di Cook} dimostra che il problema della soddisfacibilità soddisfa i requisiti
necessari e ci fornisce la soluzione.

\textbf{Teorema di Cook.} Data una formula Booleana in forma canonica disgiuntiva, decidere se esiste
un assegnamento di valori ai suoi elementi che la rende vera.
Per esempio, la formula
\(
(A \ \text{or} \ B) \ \text{and} \ (\text{not}(A) \ \text{or} \ C);
\)
definita rispetto alle variabili Booleane $A$, $B$ e $C$, è soddisfatta se $B$ e $C$
sono entrambe vere. Al contrario,
\(
(A \ \text{or} \ B) \ \text{and} \ (\text{not}(A) \ \text{or} \ B) \ \text{and} \ (\text{not}(B))
\)
non può essere soddisfatta da alcun assegnamento di verità alle variabili.

A partire dal problema della soddisfacibilità, si può ricavare per riduzioni polinomiali
successive una famiglia crescente di problemi NP-completi, compreso il problema
$K_0$, che può essere considerato come un cugino in versione decisionale del problema
knapsack.\\
Il fatto che tale problema faccia parte della classe $NPC$ ci permette di dimostrare il
teorema seguente, che risolve la questione da cui siamo partiti.

\textbf{Teorema.}
Consideriamo una versione decisionale del problema $1/r_i/L_{\max}$: dati i tempi di
rilascio $r_i$, le date di consegna $d_i$ e i tempi di lavorazione $p_i$, tutti a valori
interi positivi, per $n$ job $J_i$, $i \in [n]$, esiste una soluzione in cui nessun job è
completato in ritardo? Tale problema di decisione è NP-completo.\\
\textbf{Dimostrazione.}
Il problema è chiaramente in $NP$, poiché per una data sequenza è facile verificare
se i job vengono completati in tempo rispetto alle due date.\\
Dati gli interi positivi $a_j$, $j \in [n]$, creiamo $n$ job $J_j$ con parametri
\[
r_j = 0, \quad p_j = a_j, \quad d_j = 1 + \sum_{k \in [n]} a_k, \qquad j \in [n].
\]
Creiamo un ulteriore job $J_0$ con parametri
\(
r_0 = b, \quad p_0 = 1, \quad d_0 = b + 1.
\)
Perché tutti i job rispettino la data di consegna, è necessario che il job $J_0$ inizi la
lavorazione al tempo $t = b$. Inoltre, dato che la data di consegna degli altri job è
pari alla somma di tutti i tempi di lavorazione, la soluzione non può presentare periodi
di tempo in cui la macchina è ferma, prima di avere completato l’intero insieme di job.\\
Questo richiede che sia possibile individuare un sottoinsieme $J$ di job da schedulare
prima di $J_0$, in modo tale che
\[
\sum_{j \in J} p_j = r_0.
\]
Tale insieme risolve il problema \emph{subset-sum}.

\subsection{Dai problemi di decisione ai problemi di ottimizzazione}
Il teorema dimostra che un problema di decisione legato al problema di ottimizzazione
$1/r_i/L_{\max}$ è NP-completo, \textbf{ma cosa possiamo dire del problema di ottimizzazione
stesso?}\\
Per definizione, la classe $NPC$ contiene solo problemi di decisione. Possiamo però
estendere le classi $P$ e $NPH$, includendo in esse anche problemi di ottimizzazione.\\
I \textbf{problemi di ottimizzazione} per cui è noto un algoritmo di complessità polinomiale
stanno in $P$. Nella classe $NPH$ possiamo includere problemi di ottimizzazione ai quali possiamo
ricondurre un corrispondente problema di decisione.
\textbf{La versione decisionale di $1/r_i/L_{\max}$ si riduce chiaramente al
problema di ottimizzazione.}\\
Inoltre, nella dimostrazione abbiamo assunto che i dati fossero numeri interi. Ma il
problema a numeri interi può evidentemente essere ridotto al problema generale.
Possiamo quindi affermare che il problema di scheduling $1/r_i/L_{\max}$ è
NP-difficile.

\paragraph{L’impatto della codifica di un problema}
Nella trattazione ci siamo limitati alle classi fondamentali e siamo stati piuttosto
informali e imprecisi. Non possiamo però fare a meno di considerare l’impatto del
modo in cui si codifica un problema.
Sarebbe infatti errato, per esempio, associare a un problema knapsack una dimensione
pari al numero di oggetti. La dimensione si riferisce a una codifica binaria che
comprende tutti i dati del problema.
Per mostrare la rilevanza di ciò, \textbf{consideriamo un classico algoritmo di
programmazione dinamica} per la soluzione del problema knapsack:
\[
\boxed{
\begin{aligned}
\max \quad & \sum_{k=1}^{n} v_k x_k \\
\text{s.t.} \quad & \sum_{k=1}^{n} w_k x_k \le B \\
& x_k \in \{0,1\}, \quad k = 1, \dots, n.
\end{aligned}
}
\]
Definiamo la funzione valore
\[
V_k(s) := \text{valore del sottoinsieme ottimale tra gli oggetti } \{k, k+1, \dots, n\},
\]
quando la capacità residua è $s$.
In sostanza, la funzione valore assume che siano già state fatte scelte di inserimento
o meno degli oggetti da $1$ a $k-1$; a valle di tale selezione, abbiamo una capacità
residua $s$, e ci chiediamo come utilizzarla al meglio per le scelte rimanenti. Se i dati
$w_k$ e $B$ del problema sono interi, lo sarà anche la capacità residua $s$.\\
Per risolvere il problema, ovvero trovare il valore di $V_1(B)$, si applica una relazione
ricorsiva:
\[
\boxed{
V_k(s) =
\begin{cases}
V_{k+1}(s), & 0 \le s < w_k, \\
\max \{ V_{k+1}(s), \, V_{k+1}(s - w_k) + v_k \}, & w_k \le s \le B.
\end{cases}
}
\]
Si tratta di una equazione funzionale con condizione terminale:
\[
\boxed{
V_n(s) =
\begin{cases}
0, & 0 \le s < w_n, \\
v_n, & w_n \le s \le B.
\end{cases}
}
\]
Occorre tabulare tutte le funzioni $V_k(s)$, $k = 1, \dots, n$, per valori interi di $s$,
che assume valori nel range da $0$ a $B$. Pertanto, tale algoritmo ha complessità
$O(nB)$.\\
Questo non implica che il problema knapsack abbia complessità polinomiale: per
rappresentare il valore $B$ in aritmetica binaria bastano $\log_2 B$ bit.
Quindi \textbf{l’algoritmo, rispetto a tale codifica binaria, ha complessità esponenziale}. Se si
utilizzasse un \textbf{computer con una codifica unaria}, l’algoritmo che abbiamo considerato
avrebbe \textbf{complessità polinomiale}.
Si dice infatti che un algoritmo di questo tipo è \emph{pseudo-polinomiale}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CAP 3 --Metodi di decomposizione in ottimizazione}

\subsection{Motivation}

\paragraph{Ruolo dei metodi di decomposizione}
I metodi di \textbf{decomposizione} svolgono un ruolo centrale nell’ottimizzazione, poiché consentono:
\begin{itemize}[label=-]
\item di sfruttare \textbf{strutture favorevoli} del problema, come:
\begin{itemize}[label=-]
\item sottoproblemi di rete;
\item trasformazioni da problemi \textbf{NP-hard} a problemi risolvibili in \textbf{tempo polinomiale};
\end{itemize}
\item di \textbf{parallelizzare} la soluzione di problemi di grande scala;
\item di affrontare modelli di \textbf{ottimizzazione stocastica} di grandi dimensioni basati su scenari;
\item di trattare problemi combinatori difficili mediante una \textbf{sequenza di sottoproblemi più semplici}, eventualmente combinando diverse strategie di soluzione;
\item di evitare difficoltà di modellazione dovute a \textbf{vincoli molto complessi}.
\end{itemize}

\subsection{Problemi separabili}

\paragraph{Problema separabile}
Si consideri il problema di ottimizzazione
\[
\operatorname{opt} \sum_{j \in [n]} f_j(x_j)
\quad \text{s.t. } x_j \in S_j.
\]
In questo caso il problema può essere \textbf{decomposto} in sottoproblemi indipendenti
\[
\operatorname{opt}_{x_j \in S_j} f_j(x_j),
\]
poiché i sottovettori $x_j$ sono soggetti a \textbf{vincoli indipendenti}.

\paragraph{Interpretazione nei modelli di programmazione lineare}
Nel caso di un modello di programmazione lineare
\[
\min c^T x
\quad \text{s.t. } Ax = b,
\quad x \ge 0,
\]
la separabilità corrisponde a una struttura \textbf{a blocchi diagonali} della matrice tecnologica:
\[
A =
\begin{bmatrix}
D_1 \\
D_2 \\
D_3 \\
\vdots \\
D_n
\end{bmatrix}.
\]

\subsection{Fattori complicanti}

\paragraph{Assenza di struttura completamente separabile}
Nella maggior parte dei casi reali, non si dispone di una struttura così favorevole e sono presenti \textbf{fattori complicanti}.

\paragraph{Struttura block-angular}
Un caso tipico è quello di un problema LP di grande scala con struttura \textbf{block-angular}:
\[
A =
\begin{bmatrix}
C_1 & C_2 & C_3 & \cdots & C_n \\
D_1 \\
D_2 \\
D_3 \\
\vdots \\
D_n
\end{bmatrix}.
\]
In questo caso il fattore complicante è rappresentato da un insieme di \textbf{vincoli di interazione} che accoppiano i sottoproblemi e impediscono la decomposizione basata sulla struttura a blocchi diagonali.

\paragraph{Rilassamento dei vincoli di interazione}
La decomposizione diventa possibile se i vincoli di interazione vengono \textbf{rilassati} in qualche modo, ad esempio tramite la \textbf{decomposizione lagrangiana duale}.

\paragraph{Variabili di interazione}
Un secondo caso è quello in cui la matrice ha la forma
\[
A =
\begin{bmatrix}
C_1 & D_1 \\
C_2 & D_2 \\
C_3 & D_3 \\
\vdots & \vdots \\
C_n & D_n
\end{bmatrix}.
\]
Qui il fattore complicante è costituito da \textbf{variabili di interazione}, che impediscono la decomposizione.

\paragraph{Fissaggio delle variabili}
La decomposizione è possibile se si \textbf{fissano} le variabili di interazione, come avviene nella \textbf{decomposizione L-shaped}, che corrisponde alla \textbf{decomposizione di Benders} nella programmazione stocastica.

\subsection{Panoramica dei metodi di decomposizione}

\paragraph{Classificazione dei metodi}
Esiste un ampio insieme di metodi di decomposizione tra loro correlati. Alcuni sono \textbf{esatti}, almeno in linea di principio, mentre altri conducono ad \textbf{algoritmi approssimati}:
\begin{itemize}[label=-]
\item rilassamento lagrangiano e decomposizione lagrangiana;
\item euristiche duali;
\item decomposizione di Dantzig--Wolfe;
\item generazione di colonne;
\item decomposizione gerarchica;
\item matheuristics (da non confondere con le metaeuristiche);
\item decomposizione di Benders per MILP con struttura speciale;
\item decomposizione L-shaped nella programmazione stocastica a due stadi con ricorso;
\item progressive hedging per la programmazione stocastica multistadio con ricorso;
\item programmazione dinamica (decomposizione basata sul tempo).
\end{itemize}

\subsection{Part 1 -- Dual decomposition and progressive hedging for stochastic programming}

\subsubsection{Dual decomposition}

\paragraph{Problema di partenza}
Per comprendere come la dualità possa essere utilizzata in un contesto semplice, si consideri il problema:
\[
\max \sum_{i=1}^n f_i(x_i)
\tag{1}
\]
\[
\text{s.t. } \sum_{i=1}^n g_i(x_i) \le b,
\tag{2}
\]
\[
x_i \in S_i, \quad i = 1, \ldots, n.
\tag{3}
\]
Le variabili decisionali $x_i$, $i = 1, \ldots, n$, possono essere interpretate come \textbf{attività} che generano un profitto $f_i(x_i)$ e consumano una quantità di risorsa $g_i(x_i)$.  
La funzione obiettivo (1) rappresenta il \textbf{profitto totale}, mentre il vincolo (2) è un \textbf{vincolo di budget} sulla risorsa.

\paragraph{Osservazione sulle unità di misura}
La funzione obiettivo è misurata in termini monetari, mentre il parametro $b$ è espresso in unità di risorsa.  
Se fosse possibile eliminare il vincolo di budget, il problema risulterebbe immediatamente \textbf{decomponibile}.

\paragraph{Dualizzazione del vincolo di budget}
Il vincolo di budget viene dualizzato introducendo un moltiplicatore di Lagrange $\mu \ge 0$ e scrivendo la funzione lagrangiana:
\[
L(x; \mu) =
\sum_{i=1}^n f_i(x_i)
+ \mu
\left(
b - \sum_{i=1}^n g_i(x_i)
\right)
=
\sum_{i=1}^n \bigl[f_i(x_i) - \mu g_i(x_i)\bigr]
+ \mu b.
\]

\paragraph{Problemi primali decomposi}
La funzione lagrangiana deve essere massimizzata rispetto alle variabili primali, ottenendo un insieme di sottoproblemi indipendenti:
\[
\max_{x_i \in S_i}
\bigl[
f_i(x_i) - \mu g_i(x_i)
\bigr]
\equiv \pi_i(x_i).
\]
Ogni sottoproblema consiste nel massimizzare il contributo di profitto netto, dato dal profitto meno il costo della risorsa.

\paragraph{Interpretazione economica del moltiplicatore}
Il moltiplicatore $\mu$ rappresenta un \textbf{prezzo ombra} della risorsa, misurato in unità monetarie per unità di risorsa.  
In questo caso, il problema duale consiste nel \textbf{minimizzare} la funzione duale rispetto a $\mu$.

\paragraph{Sottogradiente della funzione duale}
Date le soluzioni rilassate $x_i^\ast$, un sottogradiente della funzione duale è:
\[
\sum_{i=1}^n g_i(x_i^\ast) - b.
\]
Questo valore è positivo quando il budget viene superato; in tal caso il prezzo della risorsa deve essere aumentato.  
Il prezzo deve invece essere ridotto quando il budget non viene completamente utilizzato.

\paragraph{Schema domanda--offerta}
Il meccanismo risultante può essere interpretato come uno schema di \textbf{domanda--offerta}, in cui il prezzo della risorsa viene aggiornato in funzione dello squilibrio tra consumo e disponibilità.

\paragraph{Osservazioni sulla dual decomposition}
La decomposizione duale può convergere lentamente nella pratica, ma può risultare efficace per alcuni problemi di grande scala con struttura particolare.  
Talvolta è sufficiente ottenere una soluzione di buona qualità: se da una decomposizione duale è possibile ricostruire una soluzione primale ammissibile, si ottiene una \textbf{dual heuristic}.

\paragraph{Lagrangiano aumentato}
I metodi lagrangiani possono essere integrati con metodi a funzione di penalità, ottenendo schemi di \textbf{Lagrangiano aumentato}, ad esempio minimizzando:
\[
f(x)
+ \sum_{i \in I} \lambda_i h_i(x)
+ \sigma \sum_{i \in I} h_i^2(x),
\]
nel caso di problemi con vincoli di uguaglianza.

\subsubsection{Progressive hedging for multistage stochastic programming}

\paragraph{Rappresentazione dell’incertezza}
Nella programmazione stocastica multistadio, l’incertezza è rappresentata tramite \textbf{alberi di scenario}.  
In una formulazione compatta del modello, si definiscono variabili decisionali $x_n$ associate ai nodi $n$ dell’albero.

\paragraph{Formulazione split-variable}
In una formulazione a variabili separate, si introducono variabili $x_t^s$, associate allo scenario $s$ al tempo $t$.

\paragraph{Vincoli di non anticipatività}
In questo caso è necessario imporre esplicitamente i \textbf{vincoli di non anticipatività}: variabili decisionali corrispondenti a scenari diversi allo stesso tempo $t$ devono assumere lo stesso valore se gli scenari sono indistinguibili a quel tempo.

\paragraph{Esempio di non anticipatività}
Nel caso illustrato, i nodi attraversati dagli scenari $s = 1,2,3,4$ al tempo $t = 1$ sono indistinguibili; pertanto:
\[
x_{1i}^1 = x_{1i}^2 = x_{1i}^3 = x_{1i}^4,
\quad i = 1, \ldots, I.
\]
Analogamente, al tempo $t = 2$ si hanno vincoli come:
\[
x_{2i}^5 = x_{2i}^6,
\quad i = 1, \ldots, I.
\]

\paragraph{Insiemi di scenari indistinguibili}
Si denoti con $\mathcal{F}_{s}^t$ l’insieme degli scenari indistinguibili da $s$ fino al tempo $t$.  
Ad esempio:
\[
\mathcal{F}_1^0 = \{1,2,3,4,5,6,7,8\},
\quad
\mathcal{F}_2^1 = \{1,2,3,4\},
\quad
\mathcal{F}_5^2 = \{5,6\}.
\]
I vincoli di non anticipatività possono essere scritti come:
\[
x_{it}^s = x_{it}^{s'}, \quad \forall i,t,\; s,s' \in \mathcal{F}_s^t.
\]

\paragraph{Formulazione del problema multistadio}
Si consideri un sistema dinamico con stato $z_t$ e equazione di stato:
\[
z_{t+1} = G_t(z_t, x_t, \xi_{t+1}), \quad t = 0,1,\ldots,T,
\]
dove $x_t$ è la variabile di controllo e $\xi_{t+1}$ è una variabile aleatoria.

Sia $s = (\xi_1^s, \xi_2^s, \ldots, \xi_{T+1}^s)$ uno scenario con probabilità $\pi_s$.  
Il problema di scenario individuale è:
\[
\min \sum_{t=0}^T \gamma^t f_t(z_t, x_t, \xi_{t+1}^s)
+ \gamma^{T+1} Q(z_{T+1})
\]
soggetto a:
\[
z_{t+1} = G_t(z_t, x_t, \xi_{t+1}^s),
\quad t = 0,1,\ldots,T,
\]
\[
L_t(z_t) \le x_t \le U_t(z_t),
\quad t = 0,1,\ldots,T.
\]

\paragraph{Problema stocastico con non anticipatività}
Il problema complessivo può essere scritto come:
\[
\min
\sum_{s \in S} \pi_s
\left(
\sum_{t=0}^T \gamma^t f_t(z_t^s, x_t^s, \xi_{t+1}^s)
+ \gamma^{T+1} Q(z_{T+1}^s)
\right)
\]
soggetto a:
\[
z_{t+1}^s = G_t(z_t^s, x_t^s, \xi_{t+1}^s),
\]
\[
L_t(z_t^s) \le x_t^s \le U_t(z_t^s),
\]
\[
x_t^s =
\frac{
\sum_{s' \in \mathcal{F}_s^t} \pi_{s'} x_t^{s'}
}{
P(\mathcal{F}_s^t)
},
\quad \forall s,t.
\tag{4}
\]

\paragraph{Dualizzazione e regolarizzazione}
Introducendo moltiplicatori di Lagrange $w_t^s$ per i vincoli di non anticipatività e dualizzando, il problema può essere decomposto per scenario.  
Per migliorare la convergenza si introduce un termine di penalità quadratico, ottenendo per ogni scenario:
\[
\min
\sum_{t=0}^T \gamma^t
\left\{
f_t(z_t^s, x_t^s, \xi_{t+1}^s)
+ w_t^s x_t^s
+ \frac{\rho}{2}
\bigl(x_t^s - x(\mathcal{F}_s^t)\bigr)^2
\right\}
+ \gamma^{T+1} Q(z_{T+1}^s).
\]

\paragraph{Osservazioni finali}
Il termine quadratico può essere interpretato come una \textbf{regolarizzazione}.  
La scelta del parametro $\rho$ e della strategia di aggiornamento dei moltiplicatori è critica per la convergenza.  
Nonostante ciò, il \textbf{progressive hedging} è uno strumento utile per sfruttare la struttura del problema ed è stato utilizzato per costruire euristiche per problemi stocastici multistadio misti interi difficili.

\subsection{Part 2 -- Benders decomposition and L-shaped decomposition for stochastic programming}

\subsubsection{Background: Kelley's cutting planes}

\paragraph{Problema convesso e iperpiani di supporto}
Si consideri il problema convesso
\[
\min_{x \in S} f(x),
\]
e si supponga che, dato un punto $x^k$, sia possibile calcolare non solo il valore della funzione $f(x^k)=\alpha_k$, ma anche un \textbf{subgradiente} $\gamma_k$, che esiste se la funzione è convessa sull’insieme $S$.

In altre parole, è possibile trovare una funzione affine tale che
\[
f(x^k)=\alpha_k+\gamma_k^T x^k
\tag{5}
\]
\[
f(x)\ge \alpha_k+\gamma_k^T x \quad \forall x\in S.
\tag{6}
\]

\paragraph{Approssimazione dal basso}
La disponibilità di un tale iperpiano di supporto suggerisce la possibilità di approssimare $f$ \textbf{dal basso}, tramite l’\textbf{inviluppo superiore} degli iperpiani di supporto.

\paragraph{Idea dell’algoritmo di Kelley}
L’algoritmo di \textbf{Kelley cutting planes} sfrutta questa idea costruendo e migliorando una funzione di bound inferiore fino a soddisfare un criterio di convergenza.
Se $S$ è \textbf{poliedrale}, occorre risolvere una sequenza di \textbf{LP}.

\subsubsection{Kelley's cutting planes come algoritmo}

\paragraph{Algoritmo (cutting planes di Kelley)}
\textbf{Step 0.} Sia $x^1\in S$ una soluzione ammissibile iniziale; inizializzare il contatore delle iterazioni $k \leftarrow 0$, l’upper bound $u_0=f(x^1)$, il lower bound $l_0=-1$, e la funzione di bound inferiore $\phi_0(x)=-1$.\\
\textbf{Step 1.} Incrementare il contatore $k \leftarrow k+1$. Trovare un subgradiente di $f$ in $x^k$ tale che (5) e (6) siano soddisfatte.\\
\textbf{Step 2.} Aggiornare l’upper bound
\[
u_k=\min\{u_{k-1},\, f(x^k)\},
\]
e la funzione di bound inferiore
\[
\phi_k(x)=\max\bigl\{\phi_{k-1}(x),\, \alpha_k+\gamma_k^T x \bigr\}.
\]
\textbf{Step 3.} Risolvere il problema
\[
l_k=\min_{x\in S}\phi_k(x),
\]
e sia $x^{k+1}$ la soluzione ottima.\\
\textbf{Step 4.} Se $u_k-l_k<\varepsilon$, arrestare: $x^{k+1}$ è un’approssimazione soddisfacente della soluzione ottima; altrimenti, tornare allo Step 1.

\subsubsection{Convexity properties of stochastic programs}

\paragraph{Due-stadi SLP con ricorso}
Si consideri il problema SLP a due stadi con ricorso:
\[
\min \ c^T x + \mathbb{E}_{\xi}\bigl[Q(x;\xi)\bigr]
\]
\[
\text{s.t. } Ax=b,\quad x\ge 0,
\]
dove
\[
Q(x;\xi)\equiv
\min_y
\left\{
q(\xi)^T y \ \big|\ W(\xi)y = h(\xi)-T(\xi)x,\ y\ge 0
\right\}.
\]
Si parla di \textbf{fixed recourse} quando $W$ è deterministica, mentre si parla di \textbf{random recourse} nel caso generale.

\paragraph{Funzione di ricorso con costo deterministico}
Si consideri la funzione di ricorso $Q(x)\equiv \mathbb{E}[Q(x;\xi)]$ nel caso di costo di ricorso deterministico:
\[
Q(x;\xi)\equiv
\min_y
\left\{
q^T y \ \big|\ Wy = h(\xi)-T(\xi)x,\ y\ge 0
\right\}.
\]

\paragraph{Dominio effettivo}
Il dominio effettivo di $Q(x)$ consiste dei vettori $x$ che sono ammissibili per i vincoli di primo stadio e tali che $Q(x)<+\infty$, cioè per cui il problema di secondo stadio è (quasi sicuramente) ammissibile.

\paragraph{Nota sulle ipotesi}
Si assume che, esclusa l’inammissibilità del secondo stadio, l’aspettazione sia sempre definita, cioè che le variabili aleatorie non siano heavy-tailed.

\paragraph{Dualità LP e forma duale della funzione di ricorso}
Per dualità LP,
\[
Q(x;\xi)=
\max_{\pi}
\left\{
\bigl[h(\xi)-T(\xi)x\bigr]^T \pi \ \big|\ W^T \pi \le q
\right\}.
\]
Si denoti l’insieme ammissibile del duale con
\[
\Gamma=\{\pi \mid W^T\pi \le q\}.
\]
Poiché $q$ è deterministico, tale insieme è \textbf{non aleatorio}.

\paragraph{Dimostrazione di convessità (caso con regione duale non aleatoria)}
Siano $x_1,x_2\in D$, $\lambda \in [0,1]$ e $x_\lambda=\lambda x_1+(1-\lambda)x_2$. Allora:
\[
Q(x_\lambda)=\int_{\Omega}Q(x_\lambda;\xi)\,dP
=\int_{\Omega}\max_{\pi\in\Gamma}\left\{\bigl[h(\xi)-T(\xi)x_\lambda\bigr]^T\pi\right\}\,dP
\]
\[
=\int_{\Omega}\max_{\pi\in\Gamma}
\left\{
\lambda\bigl[h(\xi)-T(\xi)x_1\bigr]^T\pi
+(1-\lambda)\bigl[h(\xi)-T(\xi)x_2\bigr]^T\pi
\right\}\,dP
\]
\[
\le
\lambda\int_{\Omega}\max_{\pi\in\Gamma}\left\{\bigl[h(\xi)-T(\xi)x_1\bigr]^T\pi\right\}\,dP
+(1-\lambda)\int_{\Omega}\max_{\pi\in\Gamma}\left\{\bigl[h(\xi)-T(\xi)x_2\bigr]^T\pi\right\}\,dP
\]
\[
=\lambda Q(x_1)+(1-\lambda)Q(x_2).
\]

\paragraph{Convexity of the recourse function}
La dimostrazione precedente è semplice perché la regione ammissibile duale è non aleatoria. In generale, la convessità della funzione di ricorso può essere mostrata anche in contesti più generali.  
In sintesi:
\begin{itemize}[label=-]
\item la funzione di ricorso è tipicamente \textbf{differenziabile} per distribuzioni continue;
\item la funzione di ricorso è \textbf{poliedrale} per distribuzioni discrete.
\end{itemize}
In entrambi i casi, si può fare affidamento sui \textbf{cutting planes di Kelley} per risolvere il problema.
Dato lo \textbf{schema di decomposizione} del problema, i cutting planes possono essere ottenuti tramite una strategia di decomposizione.

\subsubsection{L-shaped decomposition}

\paragraph{Deterministic equivalent}
Si consideri l’equivalente deterministico:
\[
\min \ c^T x + \sum_{s\in S} p_s q^T y^s
\]
\[
\text{s.t. } Ax=b,
\]
\[
W y^s + T_s x = h_s \quad \forall s\in S,
\]
\[
x,\ y^s \ge 0.
\]
La matrice tecnologica del problema complessivo ha una struttura \textbf{dual block-angular}:
\[
\begin{bmatrix}
A & 0 & 0 & \cdots & 0\\
T_1 & W & 0 & \cdots & 0\\
T_2 & 0 & W & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
T_S & 0 & 0 & \cdots & W
\end{bmatrix}.
\]
Dato un primo stadio fissato, si ottengono problemi di secondo stadio \textbf{indipendenti per scenario}.

\paragraph{Riformulazione con variabile ausiliaria}
Si riscrive il problema a due stadi come:
\[
\min \ c^T x + \theta
\]
\[
\text{s.t. } Ax=b,
\]
\[
\theta \ge Q(x)
\tag{7}
\]
\[
x\ge 0.
\]
Si costruisce un \textbf{master problem rilassato} rilassando (7), che viene approssimato tramite cutting planes:
\begin{itemize}[label=-]
\item \textbf{optimality cuts} della forma $\ \theta \ge \alpha^T x + \beta$;
\item \textbf{feasibility cuts} della forma $\ 0 \ge \alpha^T x + \beta$.
\end{itemize}
I coefficienti di ciascun taglio si ottengono risolvendo i sottoproblemi di scenario per una data decisione di primo stadio.

\paragraph{L-shaped decomposition: optimality cuts}
Sia $\hat{x}$ la soluzione ottima del master iniziale e si consideri il duale del problema di secondo stadio per lo scenario $s$:
\[
Q_s(\hat{x}) \equiv \max (h_s-T_s\hat{x})^T \pi^s
\quad \text{s.t. } W^T \pi^s \le q_s.
\tag{8}
\]
Data una soluzione duale ottima $\hat{\pi}^s$, valgono:
\[
Q_s(\hat{x})=(h_s-T_s\hat{x})^T \hat{\pi}^s
\tag{9}
\]
\[
Q_s(x)\ge (h_s-T_s x)^T \hat{\pi}^s \quad \forall x.
\tag{10}
\]
La (10) deriva dal fatto che $\hat{\pi}^s$ è ottima per $\hat{x}$, ma non necessariamente per un generico $x$.

Sommando (10) sugli scenari:
\[
Q(x)=\sum_{s\in S} p_s Q_s(x) \ge \sum_{s\in S} p_s (h_s-T_s x)^T \hat{\pi}^s.
\]
Quindi si aggiunge al master rilassato il seguente \textbf{optimality cut}:
\[
\theta \ge \sum_{s\in S} p_s (h_s-T_s x)^T \hat{\pi}^s.
\]

\paragraph{L-shaped decomposition: feasibility cuts}
Se il ricorso non è completo, alcuni sottoproblemi di scenario possono essere inammissibili per una decisione di primo stadio $\hat{x}$.  
Si può allora sfruttare il duale del sottoproblema di scenario per trovare un \textbf{feasibility cut} che elimina $\hat{x}$ da ulteriori considerazioni.

Si osservi che la regione ammissibile del duale non dipende dalle decisioni di primo stadio, poiché $\hat{x}$ non compare nei vincoli (8).  
Se il duale è inammissibile, allora il problema di secondo stadio dello scenario corrispondente sarebbe inammissibile per qualsiasi decisione di primo stadio; questo caso viene escluso e può indicare un errore di modellazione.

Quando il primale è inammissibile, il duale è illimitato. Esiste dunque un raggio estremo della regione duale lungo il quale la soluzione duale tende a infinito.
Il duale del secondo stadio è illimitato se esistono variabili duali $\pi^\ast$ tali che
\[
W^T \pi^\ast \le 0,\quad (h_s-T_s\hat{x})^T \pi^\ast > 0.
\]
La fattibilità del primale richiede
\[
Wy = h_s - T_s x
\ \Rightarrow\
(\pi^\ast)^T Wy = (\pi^\ast)^T(h_s-T_s x),
\]
e, usando $W^T\pi^\ast \le 0$ e $y\ge 0$, si ottiene
\[
(\pi^\ast)^T(h_s-T_s x)\le 0.
\]
Quindi
\[
(\pi^\ast)^T(h_s-T_s x)\le 0
\]
è un taglio valido che viene aggiunto al master rilassato.

\paragraph{Schema iterativo dell’L-shaped decomposition}
L-shaped decomposition alterna:
\begin{enumerate}[label=\arabic*.]
\item la soluzione del master problem rilassato, che produce $\hat{\theta}$ e $\hat{x}$;
\item la soluzione dei corrispondenti sottoproblemi di scenario.
\end{enumerate}
A ogni iterazione, vengono aggiunti tagli al master.
L’algoritmo si ferma quando la soluzione ottima del master soddisfa
\[
\hat{\theta} \ge Q(\hat{x}).
\]
Questa condizione può essere rilassata se è sufficiente una soluzione near-optimal.

\paragraph{L-shaped decomposition: variations on the theme}
Esistono varianti per gestire i tagli e migliorare la convergenza (ad esempio \textbf{regularized L-shaped decomposition}).  
L’idea può essere generalizzata a problemi multistadio:
\begin{itemize}[label=-]
\item nested Benders decomposition;
\item abridged nested Benders decomposition;
\item stochastic dual dynamic programming.
\end{itemize}
Nei metodi di decomposizione stocastica (Higle e Sen) si ha un’interazione tra campionamento e ottimizzazione: l’idea è costruire tagli asintoticamente validi.  
Strategie alternative si basano sul vincolare la funzione di ricorso superiormente e inferiormente, partizionando il dominio per ottenere bound stretti.  
Quando metodi del simplesso o interior point riescono a risolvere un LP stocastico, spesso non c’è vantaggio nell’uso della decomposizione; tuttavia, la decomposizione è necessaria per problemi realmente di grande scala.

\subsection{Part 3 -- Dantzig--Wolfe decomposition and column generation}

\subsubsection{Cutting stock by column generation}

\paragraph{Problema di cutting stock}
Si devono tagliare rotoli (tutti di larghezza $L$) di un certo materiale in rotoli più corti, in modo da soddisfare una domanda assegnata.
Si introduca un insieme di $n$ item con domanda $d_i$ e larghezza $w_i \le L$, $i\in[n]$.
L’obiettivo è impacchettare gli item in modo da utilizzare il \textbf{numero minimo di rotoli}.
Questo è un esempio stilizzato di una vasta classe di problemi noti come \textbf{cutting-stock problems}. :contentReference[oaicite:1]{index=1}

\paragraph{Modello MILP con variabili per rotolo}
Siano disponibili $m$ rotoli, indicizzati da $j\in[m]$, e si introducano le variabili decisionali:
\begin{itemize}[label=-]
\item $\delta_j \in \{0,1\}$, uguale a $1$ se si usa il rotolo $j$;
\item $x_{ij}\in \mathbb{Z}_+$, numero di item $i$ tagliati dal rotolo $j$.
\end{itemize}
Un possibile modello MILP è:
\[
\min \sum_{j\in[m]} \delta_j
\]
\[
\text{s.t. } \sum_{i\in[n]} w_i x_{ij} \le L\delta_j, \quad j\in[m]
\]
\[
\sum_{j\in[m]} x_{ij} = d_i, \quad i\in[n]
\]
\[
\delta_j\in\{0,1\},\ x_{ij}\in \mathbb{Z}_+, \quad i\in[n],\ j\in[m].
\]
Pur essendo corretto dal punto di vista teorico, il modello soffre di difficoltà legate a vincoli \textbf{big-$M$} e a problemi di \textbf{simmetria} (molte soluzioni ottime equivalenti possono penalizzare i solver commerciali). :contentReference[oaicite:2]{index=2}

\paragraph{Modello a pattern di taglio}
Si passa a una formulazione basata su un insieme di \textbf{cutting patterns}.
Ogni pattern, indicizzato da $k$, è associato al numero $a_{ik}$ di item $i$ ottenuti da un rotolo.
Dato un insieme $K$ di pattern, si introduce il numero intero $\gamma_k$ di rotoli tagliati secondo il pattern $k$:
\[
\min \sum_{k\in K} \gamma_k
\]
\[
\text{s.t. } \sum_{k\in K} a_{ik}\gamma_k = d_i,\quad i\in[n]
\]
\[
\gamma_k\in \mathbb{Z}_+,\quad k\in K.
\]
Questa formulazione risolve la simmetria, ma la cardinalità di $K$ è enorme: solo pochi pattern sono davvero utili e non è ovvio come restringere $K$ \emph{a priori}.
Si parte quindi con un numero limitato di pattern e li si genera dinamicamente tramite \textbf{column generation}. 

\subsubsection{Generating a new column by pricing}

\paragraph{LP-rilassato del modello ristretto}
Si consideri un LP-rilassato di una versione ristretta del modello:
\[
\min \sum_{k\in K'} \gamma_k
\]
\[
\text{s.t. } \sum_{k\in K'} a_{ik}\gamma_k = d_i,\quad i\in[n]
\tag{11}
\]
\[
\gamma_k \ge 0,\quad k\in K',
\]
dove $K'\subset K$. Come migliorare la soluzione introducendo una nuova colonna? :contentReference[oaicite:4]{index=4}

\paragraph{Costo ridotto e scelta della colonna entrante}
Siano $\pi_i$, $i\in[n]$, le variabili duali dei vincoli (11).
Dalla teoria del simplesso primale, conviene introdurre in base una variabile con \textbf{costo ridotto negativo}.
Per un pattern alternativo $k$, il costo ridotto è:
\[
\hat{c}_k = 1 - \sum_{i\in[n]} \pi_i a_{ik}.
\]
Si cerca la colonna $q$ con costo ridotto minimo, $\hat{c}_q = \min_k \hat{c}_k$.
Se $\hat{c}_q \ge 0$, la base corrente è ottima; altrimenti si introduce la variabile non basica $\gamma_q$ e si itera. :contentReference[oaicite:5]{index=5}

\paragraph{Pricing come knapsack}
La minimizzazione si ottiene risolvendo un problema intero di knapsack:
\[
\max \sum_{i\in[n]} \pi_i y_i
\]
\[
\text{s.t. } \sum_{i\in[n]} w_i y_i \le L
\]
\[
y_i \in \mathbb{Z}_+.
\]
Questo produce un nuovo pattern, che viene introdotto se il valore ottimo è maggiore di $1$. 

\paragraph{Osservazioni su branch-and-price e complessità}
Questo fornisce un lower bound per un algoritmo \textbf{branch-and-price}.
In alternativa, si può generare un insieme di colonne e poi ripristinare l’integralità delle variabili $\gamma_k$ risolvendo un singolo IP (euristica).
In casi meno stilizzati, la complessità dei vincoli può essere “nascosta” nel sottoproblema di pricing.
Lo schema di column generation è al cuore della \textbf{decomposizione di Dantzig--Wolfe}. 

\subsubsection{Dantzig--Wolfe decomposition}

\paragraph{Modello LP e separazione dei vincoli}
Si consideri un modello LP:
\[
\min c^T x
\]
\[
\text{s.t. } Ax \ge b
\tag{12}
\]
\[
Dx \ge f
\tag{13}
\]
\[
x \ge 0_n.
\]
Si separano i vincoli interpretando (12) come vincoli \textbf{complicanti}, mentre (13) (insieme alla non-negatività) come vincoli \textbf{facili}. 

\paragraph{Poliedro facile e Minkowski--Weyl}
Si definisce il poliedro:
\[
X := \{x\in\mathbb{R}^n \mid Dx \ge f,\ x \ge 0_n\}.
\]
Si assume che minimizzare una funzione lineare su $X$ sia facile (ad esempio perché $D$ è block-diagonal o per struttura favorevole come un network flow).
Per il teorema di Minkowski--Weyl, $X$ si può rappresentare come somma di un politopo e di un cono:
\[
X = \operatorname{conv}\{V\} + \operatorname{cone}\{D\},
\]
dove $V=\{v^1,\ldots,v^Q\}$ è l’insieme dei punti estremi e $D=\{d^1,\ldots,d^R\}$ l’insieme delle direzioni estreme. 

\paragraph{Rappresentazione mediante combinazioni convesse e coniche}
Il vincolo $x\in X$ si esprime come:
\[
x = \sum_{q\in V} \lambda_q v^q + \sum_{r\in D} \mu_r d^r
\tag{14}
\]
\[
\sum_{q\in V} \lambda_q = 1
\tag{15}
\]
\[
\lambda_q \ge 0,\ q\in V;\quad \mu_r \ge 0,\ r\in D.
\tag{16}
\]
dove (15) è il \textbf{vincolo di convessità}.
Sostituendo (14) nel modello originale si ottiene un problema in $\lambda_q$ e $\mu_r$. 

\paragraph{Master Problem (MP)}
Definendo gli scalari e vettori:
\[
c_q := c^T v^q,\quad a_q := Av^q,\quad q\in V,
\]
\[
c_r := c^T d^r,\quad a_r := Ad^r,\quad r\in D,
\]
si ottiene il \textbf{master problem} (MP):
\[
(\text{MP})\ \min \sum_{q\in V} c_q \lambda_q + \sum_{r\in D} c_r \mu_r
\]
\[
\text{s.t. } \sum_{q\in V} a_q \lambda_q + \sum_{r\in D} a_r \mu_r \ge b
\]
\[
\sum_{q\in V} \lambda_q = 1
\]
\[
\lambda_q \ge 0,\ q\in V;\quad \mu_r \ge 0,\ r\in D.
\]
Il problema è che MP può coinvolgere un numero enorme di variabili e non è in generale possibile enumerare tutti i punti/direzioni estremi di $X$ a partire dalla descrizione per disuguaglianze. 

\paragraph{Restricted Master Problem (RMP)}
La strategia si basa su \textbf{column generation}.
Si inizializza un Restricted Master Problem con un sottoinsieme $V'\subset V$ di punti estremi e $D'\subset D$ di direzioni estreme:
\[
(\text{RMP})\ \min \sum_{q\in V'} c_q \lambda_q + \sum_{r\in D'} c_r \mu_r
\]
\[
\text{s.t. } \sum_{q\in V'} a_q \lambda_q + \sum_{r\in D'} a_r \mu_r \ge b
\tag{17}
\]
\[
\sum_{q\in V'} \lambda_q = 1
\tag{18}
\]
\[
\lambda_q \ge 0,\ q\in V';\quad \mu_r \ge 0,\ r\in D'.
\]
Siano $\pi$ e $\pi^0$ le variabili duali associate rispettivamente ai vincoli (17) e (18). 

\paragraph{Costo ridotto per punti estremi e direzioni estreme}
Nel simplesso standard, una colonna entra in base se il costo ridotto è negativo.
Per i punti estremi, si verifica se esiste $q\in V$ tale che:
\[
c_q = c_q -
\begin{bmatrix}
\pi^T & \pi^0
\end{bmatrix}
\begin{bmatrix}
a_q\\
1
\end{bmatrix}
= c_q - \pi^T a_q - \pi^0
= c_q - \pi^T A v^q - \pi^0 < 0.
\]
Per le direzioni estreme:
\[
c_r = c_r -
\begin{bmatrix}
\pi^T & \pi^0
\end{bmatrix}
\begin{bmatrix}
a_r\\
0
\end{bmatrix}
= c_r - \pi^T a_r
= c_r - \pi^T A d^r < 0.
\]


\paragraph{Auxiliary problem e pricing problem (PP)}
In linea di principio si dovrebbe risolvere:
\[
\min\left\{
\min_{q\in V} c_q,\ \min_{r\in D} c_r
\right\},
\]
ma non è affrontabile per enumerazione completa.
Si riconduce allora a un \textbf{pricing problem}:
\[
(\text{PP})\ \min \ (c^T - \pi^T A)x
\]
\[
\text{s.t. } Dx \ge f
\]
\[
x \ge 0_n.
\]
Si trascura il termine costante $\pi^0$ e si usa lo stesso PP per trovare un vettore $x\in\mathbb{R}^n$ che giochi il ruolo di punto estremo o direzione estrema.
Sia $z^\ast_{PP}$ il valore ottimo di PP. 

\paragraph{Casi per l'aggiunta di colonne}
\begin{itemize}[label=-]
\item Se $z^\ast_{PP}=-\infty$ (PP illimitato inferiormente), si è trovata una \textbf{direzione estrema} $x^\ast$ con costo ridotto negativo; si aggiunge a RMP una nuova variabile $\mu_r$ associata alla colonna
\[
\begin{bmatrix}
Ax^\ast\\
0
\end{bmatrix}.
\]
\item Se $-\infty < z^\ast_{PP} < \pi^0$ (PP limitato e valore inferiore alla variabile duale del vincolo di convessità), si è trovato un \textbf{nuovo punto estremo} $x^\ast$ con costo ridotto negativo; si aggiunge a RMP una nuova variabile $\lambda_q$ associata alla colonna
\[
\begin{bmatrix}
Ax^\ast\\
1
\end{bmatrix}.
\]
\item Se $z^\ast_{PP} \ge \pi^0$, non esiste alcun punto/direzione utile da aggiungere e ci si può fermare.
\end{itemize}


\paragraph{Interpretazione intuitiva}
In termini intuitivi, le variabili duali ottime ottenute risolvendo RMP suggeriscono una direzione
\[
(c - A^T \pi)
\]
lungo la quale si “sonda” il poliedro $X$.
Se $X$ è un politopo si può trovare solo un punto estremo; se $X$ è illimitato si può trovare una direzione estrema.
Non serve trovare tutti i punti e tutte le direzioni: interessano solo quelli con costo ridotto negativo, perché possono migliorare il valore ottenuto risolvendo RMP. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CAP 4 -- Sistemi MRP/ERP e approccio JIT}
\paragraph{Recap}
I metodi classici di gestione delle scorte risultano inadeguati in presenza di strutture di prodotto complesse, vincoli di capacità e ambienti non make-to-stock, poiché la propagazione dei fabbisogni lungo la distinta base può generare \textbf{amplificazione della variabilità}. In questo contesto si colloca l’evoluzione dalla logica MRP ai sistemi MRPII ed ERP, come risposta al problema del lot-sizing multilivello.

La logica \textbf{MRP} si fonda su un’ipotesi di capacità infinita, in cui il vincolo di capacità è surrogato tramite un \textbf{lead time fissato a priori}, e su una procedura ricorsiva di esplosione dei fabbisogni a partire dall’MPS. Gli ordini pianificati non sono esecutivi e il processo è soggetto al fenomeno del \textbf{nervosismo}, per cui piccole variazioni dell’MPS possono produrre grandi variazioni negli ordini, anche per effetto di bordo dovuto alla ripianificazione rolling horizon.

A livello di shop floor, la \textbf{Factory Physics} mette in relazione throughput, flow time e WIP tramite la \textbf{legge di Little} e mostra come la variabilità e l’elevata utilizzazione aumentino i tempi di attraversamento. L’approccio \textbf{Just-In-Time (Toyota)} mira a ridurre la variabilità alla fonte mediante produzione livellata, controllo \textbf{pull} del WIP e riduzione dei tempi di setup, evidenziando il legame strutturale tra variabilità, livelli di magazzino e prestazioni del sistema produttivo.


\subsection{Limiti degli approcci classici di gestione delle scorte}

\paragraph{Inadeguatezza dei modelli tradizionali}
I classici approcci di controllo delle scorte presentano \textbf{limiti severi} nei seguenti contesti:
\begin{itemize}[label=-]
\item Ambienti \emph{non make-to-stock}, come \emph{make-to-order} e \emph{assemble-to-order}, in cui viene ignorata la variabilità prevedibile;
\item Presenza di \textbf{vincoli di capacità produttiva}, per cui tali modelli risultano più adatti a problemi retail;
\item Strutture di prodotto \textbf{complesse}, rappresentate mediante distinte base multilivello.
\end{itemize}

\paragraph{Effetto di amplificazione della variabilità}
Anche in presenza di una domanda regolare per il prodotto finito, la propagazione dei fabbisogni lungo la distinta base può indurre un’\textbf{amplificazione della variabilità} sui componenti a valle.

\subsection{Evoluzione dai modelli MRP ai sistemi ERP}

\paragraph{Lot-sizing multilivello e difficoltà computazionali}
In linea di principio è possibile costruire un modello MILP per il problema di \textbf{lot-sizing multilivello}, collegando domanda indipendente e domanda dipendente.  
Tali modelli risultano tuttavia difficili da risolvere, e erano computazionalmente impraticabili fino agli anni Settanta.\\
Negli anni Settanta sono stati introdotti i sistemi \textbf{MRP (Material Requirements Planning)}, basati su una \textbf{logica a capacità infinita}.  
Il vincolo di capacità produttiva viene rilassato, consentendo un \textbf{disaccoppiamento parziale tra item}, ma non tra domanda indipendente e dipendente.\\
L’interazione tra item viene anticipata e surrogata mediante un \textbf{lead time fissato a priori}.

\paragraph{Evoluzione verso MRPII ed ERP}
L’evoluzione successiva ha portato ai sistemi \textbf{MRPII (Manufacturing Resource Planning)}, che includono strumenti di verifica della capacità:
\begin{itemize}[label=-]
\item \textbf{RCCP} (Rough Cut Capacity Planning).
\item \textbf{CRP} (Capacity Requirement Planning): per verificare il soddisfacimento dei vincoli di capacità.
\end{itemize}
L’ulteriore evoluzione è rappresentata dai sistemi \textbf{ERP (Enterprise Resource Planning)}, caratterizzati dall’integrazione con le funzioni commerciali e finanziarie.

\subsection{Logica MRP}
\paragraph{Assunzione di capacità infinita}
La logica MRP assume \textbf{capacità infinita}: il vincolo di capacità produttiva non viene considerato esplicitamente, ma è rappresentato indirettamente dal lead time.\\
Il lead time impone di lanciare gli ordini di produzione o di acquisto con sufficiente anticipo rispetto alla domanda.  
Il meccanismo di \textbf{lead time offsetting} consente di anticipare gli ordini pianificati rispetto ai fabbisogni.

\paragraph{Record MRP}
Ogni codice è descritto mediante un record MRP articolato per periodo, contenente: fabbisogni lordi, consegne attese, magazzino disponibile, fabbisogni netti e ordini pianificati.\\
Prima del lead time offsetting è necessario calcolare i \textbf{fabbisogni netti}, ottenuti nettificando i fabbisogni lordi tenendo conto di:
giacenze disponibili (\emph{on-hand}) e ordini già emessi (\emph{on-order}).

\subsection{Esplosione dei fabbisogni e struttura del processo MRP}
\paragraph{Domanda indipendente e MPS}
Per i prodotti finiti (\emph{end item}), che costituiscono la radice della distinta base, i fabbisogni lordi sono definiti dal \textbf{Master Production Schedule (MPS)}.
La logica MRP procede in modo \textbf{ricorsivo} a partire dagli end item, scendendo lungo le distinte base ed esplodendo i fabbisogni di ciascun codice fornendo i tempi di lancio degli ordini a tutti i livelli.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{Architettura_MRP.pdf}
\end{figure}

\paragraph{Ordini pianificati, rilascio e regole di lot-sizing}
I lotti corrispondono ai fabbisogni netti secondo la regola \textbf{lot-for-lot}. È importante comprendere la dinamica di
 un sistema MRP: gli ordini pianificati \textbf{non sono ordini esecutivi}; gli ordini pianificati dell’\emph{action bucket},
  una volta rilasciati, vengono trasformati in consegne attese. Un’ulteriore differenza tra ordini pianificati e ordini 
  operativi è che, quando un ordine di produzione viene rilasciato, vengono modificati i record relativi alle giacenze di
   magazzino dei componenti; una parte della giacenza viene allocata per l’ordine ed è da considerarsi non disponibile 
   nel calcolo dei fabbisogni netti. I pacchetti MRP permettono di specificare un orizzonte temporale di release; solo gli 
   ordini che ricadono all’interno di tale orizzonte dovrebbero essere rilasciati, in quanto gli altri sono soggetti a 
   incertezze eccessive. Nel calcolo dei fabbisogni è possibile tenere conto di scorte di sicurezza; in questo caso, i 
   fabbisogni netti vengono generati non quando il magazzino disponibile va sotto zero, ma quando va sotto un certo livello 
   di soglia. Esiste una gamma di regole di lot-sizing, a quantità fissa o variabile, oppure euristiche per la 
   minimizzazione dei costi totali di giacenza e di ordinazione.

\subsection{Il problema del nervosismo}
Il dimensionamento dei lotti a partire dai livelli alti della distinta base può generare effetti non intuitivi in presenza di variazioni del MPS.  
Le regole a quantità variabile sono soggette al fenomeno del \textbf{nervosismo} (\emph{nervousness}).\\
Il nervosismo si manifesta quando \textbf{piccole variazioni nei fabbisogni} producono \textbf{grandi variazioni negli ordini pianificati}, anche a livelli inferiori della distinta base. Il fenomeno può portare a: ordini urgenti, instabilità del piano e risultati anti-intuitivi.

\paragraph{Effetto di bordo, nervosismo e strategie di mitigazione}
Un altro punto da considerare è l’\textbf{effetto di bordo} dovuto alla ripianificazione \emph{rolling horizon}, in cui si aggiunge un \emph{time bucket} all’orizzonte di pianificazione. L’aggiunta del fabbisogno relativo a tale time bucket può alterare l’accorpamento dei fabbisogni, con esiti imprevedibili. Esistono diversi modi per evitare il fenomeno del nervosismo. L’adozione di regole a quantità fissa permette di filtrare naturalmente variazioni di piccola entità, al costo di un aumento nel livello delle scorte. È anche possibile adottare strategie di gestione differenziata dell’orizzonte temporale di pianificazione, dette strategie di \textbf{time fencing}. Nell’immediato non è possibile cambiare nulla nell’MPS; in un periodo intermedio è possibile alterare l’MPS solo dopo specifica analisi e autorizzazione; è invece possibile cambiare l’MPS a piacere nei periodi più lontani. Infine, uno strumento utile per evitare il nervosismo e per risolvere situazioni anomale è l’uso dei \textbf{firm planned orders}; si tratta di ordini che non possono essere modificati dall’MRP quando le condizioni cambiano, ma soltanto dietro istruzione del pianificatore.

\subsection{Master Production Scheduling e pianificazione della capacità}
\paragraph{Master Production Scheduling (MPS)}
Il \textbf{Master Production Schedule (MPS)} costituisce l’input primario per la logica MRP ed è basato in parte su ordini cliente e in parte su attività di \emph{forecasting} (demand planning). Nella logica MRP tradizionale, l’MPS può essere validato mediante moduli \textbf{RCCP (Rough Cut Capacity Planning)}. Non è detto che l’MPS venga costruito esclusivamente per i codici alla radice delle distinte base: può esistere domanda indipendente per parti di ricambio e, in contesti \emph{assemble-to-order} (ATO), può essere preferibile adottare una struttura a due livelli, con un MPS a livello di moduli e un \emph{Final Assembly Scheduling} a livello superiore.

\paragraph{Capacity Requirements Planning (CRP)}
La logica MRP è basata su un’assunzione di \textbf{capacità infinita}. È tuttavia possibile effettuare una verifica a posteriori del carico di lavoro rispetto alla capacità effettivamente disponibile tramite moduli di \textbf{Capacity Requirements Planning (CRP)}. La risoluzione manuale di eventuali situazioni di non ammissibilità risulta complessa, a meno di ricorrere a modelli di ottimizzazione o a procedure euristiche a capacità finita. Inoltre, per evitare ritardi, si tende spesso a gonfiare il lead time presunto, generando work in process che a sua volta allunga ulteriormente i lead time, dando luogo a un potenziale \textbf{circolo vizioso}.

\subsection{Factory Physics e legge di Little}
A livello di \emph{shop floor}, le misure di prestazione fondamentali sono il \textbf{throughput}, il \textbf{flow time} (strettamente legato al lead time e comprensivo di tempi di attesa, lavorazione e movimentazione) e il \textbf{work in process (WIP)}, associato ai materiali in coda. Idealmente, si desidererebbe un throughput elevato con flow time e WIP contenuti; tuttavia, il raggiungimento simultaneo di tali obiettivi è fortemente influenzato dalla variabilità, sia prevedibile sia imprevedibile.

\paragraph{Legge di Little}
Un risultato fondamentale della teoria delle code è la \textbf{legge di Little}, che esprime la relazione tra WIP, throughput e flow time:
\[
\boxed{
\text{WIP} = \text{throughput} \times \text{flow time}
}
\]
La legge mette in evidenza il legame strutturale tra il livello di materiali in lavorazione e il tempo di attraversamento del sistema.

\subsection{Ruolo della variabilità nel flusso dei materiali}

\paragraph{Modello di una singola macchina}
Si consideri una singola macchina dotata di un buffer per il WIP. Si introducono le seguenti grandezze: il tempo medio di attesa in coda \( W_q \), il tempo medio di lavorazione \( t_s \), il tasso medio di servizio \( \mu = 1/t_s \), il tasso medio di arrivo \( \lambda \), che in condizioni di equilibrio coincide con il throughput, e la lunghezza media della coda \( L \), che rappresenta il WIP. In tale contesto, la legge di Little può essere riscritta come:
\[
\boxed{
L = \lambda \,(W_q + t_s)
}
\]
L’utilizzazione del sistema è definita come \( u = \lambda / \mu \), ed è compresa tra 0 e 1.

\paragraph{Effetto della variabilità}
Una coda del tipo \( G/G/1 \) non è trattabile analiticamente in forma chiusa; tuttavia, una formula approssimata, esatta nel caso \( M/M/1 \), fornisce una stima del tempo medio di attesa in coda:
\[
\boxed{
W_q \approx 
\left(
\frac{C_a^2 + C_s^2}{2}
\right)
\left(
\frac{u}{1-u}
\right)
t_s
}
\]
dove \( C_a \) e \( C_s \) sono i coefficienti di variazione dei tempi di interarrivo e di servizio, rispettivamente. La relazione mostra come l’attesa in coda cresca rapidamente all’aumentare dell’utilizzazione e della variabilità.

\subsection{Buffering Law e produzione livellata}

\paragraph{Buffering Law}
Per ovviare agli effetti della variabilità è necessario introdurre dei \textbf{buffer}, che possono assumere la forma di magazzino o WIP, capacità in eccesso oppure tempo, sotto forma di lead time gonfiato. In assenza di una riduzione della variabilità, il sistema paga un prezzo in termini di WIP elevato, capacità sottoutilizzata e peggioramento del livello di servizio al cliente, manifestato da vendite perse, lead time lunghi e consegne in ritardo.

\paragraph{Produzione livellata e fonti di variabilità}
Uno dei fondamenti dell’approccio Toyota tradizionale è la \textbf{produzione livellata} (\emph{production smoothing}). La variabilità non è legata esclusivamente a eventi casuali, come i guasti alle macchine, ma può derivare anche da batching dovuto ai tempi di setup, batching nella movimentazione (\emph{wait to move}), scarso coordinamento nell’assemblaggio (\emph{wait to match}) e variabilità della domanda riflessa nell’MPS.

%%%%%%%%%%%%%%%%
\subsection{Approccio Just-In-Time (Toyota)}

\paragraph{Principi dell’approccio Just-In-Time}
L’approccio \textbf{Just-In-Time (JIT)}, sviluppato nell’ambito del sistema Toyota, si basa sull’idea di ridurre la variabilità mediante la ripetizione di un \textbf{mix di produzione ripetitivo} (\emph{mixed-model}), realizzando una produzione livellata (\emph{production smoothing}). Il presupposto fondamentale è la riduzione o l’annullamento dei tempi di setup, che consente di produrre lotti piccoli e frequenti. Il controllo del \textbf{work in process} avviene tramite un approccio \textbf{pull}, basato sul prelievo fisico dei materiali, in contrapposizione alla logica \emph{push}, basata sul rilascio di ordini pianificati a partire da previsioni di fabbisogno. Lo strumento chiave del controllo pull è il \textbf{sistema kanban}, con possibile alternativa nel sistema \emph{CONWIP}.

\subsection{Toyota Goal Chasing}

\paragraph{Sequenziamento e regolarità dei flussi}
A parità di mix produttivo, esistono diverse sequenze di assemblaggio che realizzano lo stesso numero di prodotti finiti. Il problema del \textbf{Toyota Goal Chasing} consiste nello scegliere una sequenza che renda il più possibile \textbf{regolare il fabbisogno di componenti} sulle linee laterali che alimentano la linea principale di assemblaggio. L’obiettivo è consentire il controllo delle linee a monte mediante un sistema pull, che richiede flussi regolari e prevedibili.

\paragraph{Formalizzazione del problema}
Si consideri una linea di assemblaggio che realizza \( N \) prodotti finiti sulla base di \( M \) moduli prodotti su linee laterali, con una distinta base piatta. Sia \( b_{ij} \) il numero di componenti di tipo \( j \) richiesti per assemblare un’unità del prodotto finito \( i \), e sia \( Q_i \) il numero di finiti di tipo \( i \) previsti nel mix ripetitivo. Il fabbisogno per ciclo dei componenti di tipo \( j \) è dato da:
\[
\boxed{
N_j = \sum_{i=1}^{N} b_{ij} Q_i
}
\]
Indicando con \( Q = \sum_{i=1}^{N} Q_i \) il numero totale di assemblaggi per ciclo, il consumo cumulato ideale dei componenti di tipo \( j \) al passo \( k \) dovrebbe essere pari a:
\[
\boxed{
\frac{k N_j}{Q}
}
\]

\paragraph{Funzione obiettivo del goal chasing}
Sia \( X_{jk} \) il consumo cumulato effettivo del componente \( j \) al passo \( k \). L’obiettivo del problema di goal chasing è minimizzare la distanza tra consumo ideale e consumo reale, misurata tramite la funzione:
\[
\boxed{
\sum_{k=1}^{Q} \sum_{j=1}^{M}
\left(
\frac{k N_j}{Q} - X_{jk}
\right)^2
}
\]

\paragraph{Ruolo dei tempi di setup nel JIT}
Nel JIT viene posta forte enfasi sulla riduzione dei \textbf{tempi di setup}, poiché essa consente di ridurre la dimensione dei lotti e, di conseguenza, il livello medio delle giacenze. Tuttavia, l’impatto dei tempi di setup deve essere analizzato congiuntamente agli altri fattori del sistema produttivo. In particolare, la riduzione dei livelli di magazzino non dipende esclusivamente dalla diminuzione dei setup, ma anche dal tasso di produzione e dal grado di saturazione del sistema.

\subsection{Modello di rotazione ciclica dei prodotti}

\paragraph{Impostazione del modello}
Si consideri una linea su cui ruotano ciclicamente \( N \) prodotti. Per ciascun prodotto \( i \) si introducono il tasso di produzione \( p_i \) (pezzi per unità di tempo), il tasso di domanda \( d_i < p_i \), assunto costante, e il tempo di setup \( s_i \), assunto indipendente dalla sequenza. Il periodo di rotazione \( T_c \) deve essere ridotto al fine di contenere il livello medio di giacenza.\\
Indicando con \( T_i \) la durata del lotto del prodotto \( i \) in ciascuna rotazione, in condizioni di equilibrio deve valere:
\[
\boxed{
p_i T_i = d_i T_c
\qquad \Rightarrow \qquad
T_i = \frac{d_i}{p_i} T_c
}
\]
Inoltre, il periodo di rotazione deve soddisfare il vincolo:
\[
\boxed{
T_c \geq \sum_{i=1}^{N} s_i + \sum_{i=1}^{N} T_i
}
\]
L’eventuale \emph{slack} è utile per assorbire guasti, ritardi ed effettuare attività di manutenzione.

\paragraph{Limite inferiore del periodo di rotazione}
Sostituendo l’espressione di \( T_i \) nel vincolo precedente, si ottiene il limite inferiore:
\[
\boxed{
T_c \geq 
\frac{\sum_{i=1}^{N} s_i}
{1 - \sum_{i=1}^{N} \frac{d_i}{p_i}}
}
\]
Il risultato mostra che, oltre ai tempi di setup, il periodo di rotazione è fortemente influenzato dal rapporto tra tassi di domanda e tassi di produzione, evidenziando un legame diretto con i fenomeni di congestione già osservati nei modelli di coda.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF 5 %%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{CAP 5 -- Schedulazione nella produzione e nei servizi}
\paragraph{Recap}
La \textbf{schedulazione} consiste nell’assegnare \textbf{risorse} a \textbf{job} nel tempo, rispettando \textbf{vincoli tecnologici} e di \textbf{capacità} e ottimizzando una \textbf{misura di prestazione}; rispetto alla \textbf{pianificazione della produzione} (es., \textbf{MRP}) opera a un livello di dettaglio superiore.  
I job sono descritti da \textbf{operazioni} con \textbf{precedenze}, \textbf{tempi di processo}, \textbf{tempi di rilascio} e \textbf{due date}; tipicamente si assume lavorazione su \textbf{una macchina per volta}, \textbf{una sola operazione per macchina} e \textbf{assenza di interruzioni}, pur con possibili violazioni (lot streaming, batch, pre-emption).  

Le strutture di problema derivano da diverse \textbf{precedenze} e layout di macchine (\textbf{macchina singola}, \textbf{parallele} $P/Q/R$, \textbf{flow shop}, \textbf{job shop}, \textbf{open shop}), e le prestazioni si misurano tramite funzioni di penalità sui tempi di completamento (\textbf{$C_i$}, \textbf{$F_i$}, \textbf{$L_i$}, \textbf{$T_i$}, \textbf{$E_i$}, \textbf{$U_i$}), anche in forma \textbf{aggregata} (min-sum o min-max, con \textbf{pesi} $w_i$). Alcune misure risultano \textbf{equivalenti} fino a costanti (lateness totale e flow time totale) e $L_{\max}$ implica ottimalità anche per $T_{\max}$, con $T_{\max}=\max\{L_{\max},0\}$. Le misure \textbf{regolari} (non decrescenti in $C_i$) permettono di ricavare il \textbf{timing} dal \textbf{sequenziamento} tramite \textbf{soluzioni attive}; con misure \textbf{non regolari} (es., somma pesata di earliness e tardiness) sequencing e timing non coincidono e la soluzione ottima non è necessariamente attiva o semiattiva.  

La notazione di \textbf{Graham} \textbf{$\alpha|\beta|\gamma$} classifica i problemi tramite layout, vincoli aggiuntivi e misura obiettivo. Esistono casi risolvibili in modo \textbf{polinomiale} (regole \textbf{EDD}, \textbf{WSPT}, algoritmo di \textbf{Johnson}), ma in generale i problemi sono \textbf{NP-hard} e si ricorre a \textbf{euristiche} e \textbf{metaeuristiche}, includendo regole avanzate come \textbf{ATC}. La formulazione tramite \textbf{grafo disgiuntivo} rappresenta precedenze (archi congiuntivi) e vincoli di capacità (archi disgiuntivi), e il \textbf{cammino critico} determina il \textbf{makespan}; da tale grafo si può derivare un \textbf{MILP} per $J//C_{\max}$, utile come base per strategie \emph{destroy and repair} pur non essendo trattabile in pratica. Nella ricerca locale, perturbare archi disgiuntivi sul \textbf{cammino critico} evita cicli ed è l’unica perturbazione utile per $J//C_{\max}$.  

La procedura \textbf{shifting bottleneck} decompone $J//C_{\max}$ in problemi su macchina singola del tipo \textbf{$1/r_i/L_{\max}$}, usando \textbf{teste} $h_{ib}$ (rilasci $r_{ib}$) e \textbf{code} $t_{ib}$ per costruire \textbf{due date locali} $d_{ib}=K-t_{ib}$; il collo di bottiglia è la macchina con \textbf{$L_{\max}$} peggiore e viene congelata iterativamente fino a schedulare tutte le macchine, con possibili raffinamenti ed estensioni a \textbf{$J/r_i/L_{\max}$}.

\subsection{Modelli classici di machine scheduling}

Risolvere un problema di \textbf{schedulazione} richiede di assegnare \textbf{risorse} a \textbf{job} da eseguire nel tempo, rispettando \textbf{vincoli tecnologici} e di \textbf{capacità}, in modo da ottimizzare una \textbf{misura di prestazione}.  \\
Un problema di scheduling si pone a un livello di dettaglio superiore rispetto a un problema di \textbf{pianificazione della produzione} (ad esempio a livello \textbf{MRP}). Esiste una grande varietà di problemi di scheduling, ma si inizia con la definizione di un \textbf{problema minimale}.

\paragraph{Caratteristiche dei job}
I job sono caratterizzati da:
\begin{itemize}[label=-]
    \item Un \textbf{set di operazioni} da eseguire, legate da \textbf{vincoli di precedenza};
    \item \textbf{Tempi di lavorazione} per l’esecuzione su ogni macchina;
    \item \textbf{Date di rilascio} e \textbf{date di consegna}.
\end{itemize}
Assunzioni comuni sono che un job possa essere in lavorazione su \textbf{al più una macchina per volta}, che ogni macchina possa lavorare \textbf{un job per volta}, e che le lavorazioni \textbf{non possano essere interrotte}. Ognuna di tali assunzioni può essere violata (\textbf{lot streaming}, \textbf{processori batch}, \textbf{pre-emption}).

\paragraph{Soluzioni e diagrammi di Gantt}
Una soluzione di un problema di scheduling è caratterizzata dalle \textbf{sequenze di lavorazione} sulle macchine.   Una soluzione può essere visualizzata mediante un \textbf{diagramma di Gantt}, che rappresenta graficamente l’allocazione temporale dei job sulle macchine. \\
Dall’esempio si ricava che una stessa istanza può ammettere più soluzioni ammissibili, caratterizzate da diverse sequenze sulle macchine.

\subsection{Tipi di flusso}

\paragraph{Strutture di precedenza}
Esistono diverse \textbf{strutture di precedenza} tra le operazioni dei job, che caratterizzano la \textbf{tecnologia del ciclo di lavorazione}.  
Sono possibili \textbf{strutture lineari}, oppure \textbf{strutture arbitrarie} rappresentabili mediante \textbf{grafi}.
Le precedenze si traducono in \textbf{vincoli} di un problema di ottimizzazione.
\\
Il \textbf{flusso dei materiali} caratterizza diverse strutture in termini di macchine:
\begin{itemize}[label=-]
    \item \textbf{macchina singola};
    \item \textbf{macchine parallele} (identiche: $p_{im}=p_i$, correlate: $p_{im}=p_{ism}$, scorrelate);
    \item \textbf{flow shop};
    \item \textbf{job shop};
    \item \textbf{open shop}.
\end{itemize}
Oltre a tali strutture prototipali, si osservano varianti sul tema, e sono possibili \textbf{strutture ibride} (ad esempio \textbf{flexible flow shop}).

\subsection{Misure di prestazione}
Si indica con \textbf{$C_i$} il \textbf{tempo di completamento} del job $i\in[n]$, ovvero il tempo di completamento della sua \textbf{ultima operazione}.

\paragraph{Funzioni di penalità}
Un tipico modo per costruire misure di prestazione è associare ad ogni job $J_i$ un \textbf{termine di penalità} $\gamma_i(C_i)$.
I termini tipicamente utilizzati sono:
\begin{itemize}[label=-]
    \item \textbf{tempo di completamento}: $\gamma_i(C_i)=C_i$;
    \item \textbf{flow time}: $\gamma_i(C_i)=F_i=C_i-r_i$;
    \item \textbf{lateness}: $\gamma_i(C_i)=L_i=C_i-d_i$;
    \item \textbf{tardiness}: $\gamma_i(C_i)=T_i=\max\{C_i-d_i,0\}$;
    \item \textbf{earliness}: $\gamma_i(C_i)=E_i=\max\{d_i-C_i,0\}$;
    \item \textbf{indicatore di ritardo}:
    \(
    \gamma_i(C_i)=U_i=
    \begin{cases}
    1 & \text{se } C_i>d_i,\\
    0 & \text{se } C_i\le d_i.
    \end{cases}
    \)
\end{itemize}

\paragraph{Pesi e priorità dei job}
È possibile associare \textbf{pesi $w_i$} ai job, allo scopo di esprimere una \textbf{priorità}.  
Si può per esempio valutare per ogni job $J_i$ la \textbf{tardiness pesata} $w_iT_i$.
\\ Sulla base di questi ``\textbf{mattoncini}'' si possono costruire misure di prestazione \textbf{aggregate}, ottenendo funzioni del tipo:
\[
\min\sum_{i=1}^n\gamma_i(C_i),
\qquad
\min\max_{i=1,\dots,n}\gamma_i(C_i).
\]

\paragraph{Principali misure di prestazione aggregate}
Le principali misure di prestazione aggregate sono:
\begin{itemize}[label=-]
    \item \textbf{flow time totale}: $\sum_i F_i$;
    \item \textbf{flow time totale pesato}: $\sum_i w_iF_i$;
    \item \textbf{massima lateness}: $L_{\max}=\max_i L_i$;
    \item \textbf{tardiness totale pesata}: $\sum_i w_iT_i$;
    \item \textbf{makespan}: $C_{\max}=\max_i C_i$;
    \item \textbf{numero di job in ritardo}: $\sum_i U_i$.
\end{itemize}

\paragraph{Equivalenza tra misure di prestazione}
Alcune misure sono tra di loro \textbf{equivalenti}, nel senso che una soluzione ottima rispetto a una di esse è ottima anche per l’altra.
Ad esempio, la \textbf{lateness totale} è equivalente al \textbf{flow time totale}:
\[
\sum_{i=1}^n L_i
=
\sum_{i=1}^n C_i-\sum_{i=1}^n d_i
=
\sum_{i=1}^n F_i+\sum_{i=1}^n(r_i-d_i).
\]

Le due misure differiscono per una \textbf{costante}. Inoltre, una soluzione ottima rispetto a \textbf{$L_{\max}$} è ottima anche rispetto a \textbf{$T_{\max}$} (ma non è assicurato il viceversa):
\(
T_{\max}=\max\{L_{\max},0\}.
\)

\paragraph{Misure di prestazione regolari}
Le misure di prestazione finora elencate sono \textbf{funzioni non decrescenti} dei tempi di completamento; si parla in questo caso di \textbf{misure di prestazione regolari}.
Nel caso di misure regolari, le informazioni di \textbf{tempistica} possono essere ricavate dal \textbf{sequenziamento}, in quanto ogni operazione è eseguita \textbf{al più presto} (\textbf{soluzioni attive}).

\paragraph{Misure di prestazione non regolari}
Nel caso di \textbf{misure non regolari}, ciò non è più vero e il problema risulta \textbf{più complesso}.
Tali misure sono utili per tenere conto dell’\textbf{inopportunità di completare un job prima della due date}, ad esempio adottando come misura:
\[
\sum_{i=1}^n\bigl(w_i^T T_i + w_i^T E_i\bigr).
\]

In questo caso esiste una regione in cui la funzione \textbf{decresce} al crescere del tempo di completamento, e \textbf{sequencing e timing non coincidono} e la soluzione ottima non è necessariamente attiva o semiattiva.

\subsection{Notazione di Graham}

Per classificare i problemi di schedulazione si utilizza la codifica a tre campi \textbf{$\alpha|\beta|\gamma$} dovuta a \textbf{Graham}.

\paragraph{Campo $\alpha$: layout delle macchine}
Il campo $\alpha$ descrive il \textbf{layout delle macchine}:
\begin{itemize}[label=-]
    \item \textbf{$1$}: macchina singola;
    \item \textbf{$P,Q,R$}: macchine parallele identiche, correlate e scorrelate;
    \item \textbf{$F$}: flow shop (ad esempio $F2$);
    \item \textbf{$J$}: job shop.
\end{itemize}

\paragraph{Campo $\beta$: vincoli aggiuntivi}
Il campo $\beta$ descrive la presenza di eventuali \textbf{vincoli aggiuntivi}:
\begin{itemize}[label=-]
    \item \textbf{res}: risorse aggiuntive;
    \item \textbf{prec}: vincoli di precedenza tra job;
    \item \textbf{$r_i$}: tempi di rilascio non nulli;
    \item \textbf{$\bar d_i$}: deadline hard;
    \item \textbf{$s_{ij}$}: setup dipendenti dalla sequenza;
    \item \textbf{perm}: flow shop a permutazione, la frequenza di job si mantiene su tutte le macchine;
    \item \textbf{no-wait}: assenza di buffer tra operazioni: ragioni tecnologiche impongono che le operazioni vengano eseguite consecutivamente senza interruzioni.
\end{itemize}

\paragraph{Campo $\gamma$: misura di prestazione}
Il campo $\gamma$ descrive la \textbf{misura di prestazione} da ottimizzare.

\subsection{Algoritmi di soluzione}
Esistono alcuni casi semplici risolvibili mediante \textbf{algoritmi polinomiali}:
\begin{itemize}[label=-]
    \item \textbf{$1//L_{\max}$}: regola \textbf{EDD} che ordina i job in ordine di \textbf{due date} crescente;
    \item \textbf{$1//\sum_i w_i c_i$}: regola \textbf{WSPT} (weigthed shortest processing time) che ordina i job in ordine decrescente del rapporto $w_i/p_i$;
    \item \textbf{$F2//C_{\max}$}: algoritmo di \textbf{Johnson} (in questo caso la soluzione ottima applica la stessa sequenza sulle due macchine).
\end{itemize}

\paragraph{Problemi NP-hard ed euristiche}
In generale, i problemi di schedulazione sono \textbf{NP-hard} e vengono affrontati
mediante \textbf{euristiche}, sia costruttive sia iterative (\textbf{metaeuristiche} basate su ricerca locale).
Le \textbf{regole di priorità} costituiscono una strategia costruttiva di base.

\paragraph{Regola ATC (Apparent Tardiness Cost)} Esistono anche regole non banali, come la \textbf{regola ATC (Apparent Tardiness Cost)},
proposta per problemi \textbf{$J//\sum_i w_i T_i$}.
L’espressione della priorità del job $J_i$ sulla macchina $M_j$ al tempo $t$ è:
\[
\frac{w_i}{p_{ij}}
\exp\!\left(
-
\left[
\frac{d_i - t - p_{ij} -
\sum_{q=j+1}^{m_i}(W_{iq}+p_{iq})}{k \bar{p}}
\right]^+
\right),
\]
dove $w_i$ è il peso del job $J_i$ e $p_{ij}$ è il suo tempo di processo sulla macchina
$M_j$; la notazione $[x]^+$ è equivalente a $\max\{0,x\}$; $W_{iq}$ è una stima del tempo
di attesa di $J_i$ sulla macchina $M_q$; $m_i$ è il numero di macchine che il job deve
ancora visitare; $k$ è un parametro da selezionare; $\bar{p}$ è il tempo di processo
medio dei job in coda.

\paragraph{Interpretazione del termine di slack}
L’espressione
\[
t^* = d_i - p_{ij} -
\sum_{q=j+1}^{m_i}(W_{iq}+p_{iq})
\]
può essere interpretata come il massimo tempo di inizio di $J_i$ su $M_j$, ottenuto
sottraendo un lead time stimato dalla due date $d_i$.
Se $t \le t^*$ si può ritenere di essere in tempo, altrimenti si corre il rischio di
arrivare in ritardo.
Se si è in tempo, il termine tra parentesi quadre è positivo, e la priorità cresce
esponenzialmente al crescere di $t$. \\
Quando si è in ritardo, il termine tra parentesi quadre è negativo, per cui
l’argomento dell’esponenziale è zero. La priorità è quindi $w_i/p_{ij}$, che coincide
con la regola WSPT.

\paragraph{Relazione con la tardiness totale pesata}
Se molti job sono in ritardo, la tardiness totale pesata diventa una funzione obiettivo
quasi equivalente al tempo di completamento totale pesato:
\[
\sum_i w_i T_i =
\sum_i w_i \max\{C_i-d_i,0\}
\approx
\sum_i w_i C_i - \sum_i w_i d_i .
\]

Il problema di minimizzazione di $\sum_i w_i C_i$ è risolto, su macchina singola,
dalla regola WSPT.

\paragraph{Lookahead e ricerca locale}
La miopia delle regole di priorità può essere ridotta introducendo un certo grado di
lookahead, ad esempio mediante strategie di \emph{beam search}.
Una vasta classe di algoritmi iterativi si basa su perturbazioni locali della soluzione
corrente, che definiscono una struttura di vicinato.\\
I principali problemi sono:
\begin{itemize}[label=-]
    \item sfuggire a minimi locali (tabu search, algoritmi genetici, questi richiedono attenta codifica e decodifica delle soluzioni);
    \item esplorare grandi vicinati in maniera efficiente (LNS, Large Neighborhood
    Search, utile anche per misure non regolari);
    \item evitare la creazione di cicli.
\end{itemize}
Molte di tali questioni richiedono una formulazione mediante un grafo disgiuntivo.

\subsection{Grafi disgiuntivi}
In un \textbf{grafo disgiuntivo}, i nodi rappresentano le operazioni, con l’aggiunta di nodi
dummy iniziali e finali.
Gli \textbf{archi congiuntivi} rappresentano vincoli di precedenza tecnologici, associati ai
cicli di ciascun job.
Gli \textbf{archi disgiuntivi} vanno orientati e rappresentano i vincoli di capacità per ogni
macchina (una clique per macchina).
Il \textbf{cammino critico}, ossia il cammino di lunghezza massima dal nodo iniziale al nodo
finale, fornisce il makespan.

\subsection{Modello MILP per $J//C_{max}$}

\paragraph{Formulazione} Sulla base del grafo disgiuntivo possiamo costruire un modello MILP per il problema $J//C_{max}$.
Sia $N=\{0,1,\dots,N-1,N\}$ l’insieme dei nodi, dove $0$ e $N$ sono nodi dummy.
Sia $P$ l’insieme degli archi congiuntivi e $D$ l’insieme degli archi disgiuntivi.
La variabile binaria $x_{ij}$, se posta pari a $1$, indica che l’operazione $i$ precede
l’operazione $j$.

\begin{align*}
\min \quad & C_N \\
\text{s.t.}\quad
& C_j \ge C_i + p_j && \forall (i,j)\in P \\
& C_j \ge C_i + p_j - M(1-x_{ij}) && \forall (i,j)\in D \\
& C_i \ge C_j + p_i - M x_{ij} && \forall (i,j)\in D \\
& x_{ij}\in\{0,1\} && \forall (i,j)\in D \\
& C_i \ge p_i && \forall i\in N .
\end{align*}

Il modello può essere adattato ad altre misure di prestazione, ma non è trattabile in
pratica a causa di bound deboli. Tuttavia costituisce una base utile per
metaeuristiche e approcci LNS (\emph{destroy and repair}).

\paragraph{Il ruolo del cammino critico}
In una strategia di ricerca locale, perturbazioni arbitrarie delle sequenze possono
creare cicli. Nel problema $J//C_{\max}$, se si perturbano archi disgiuntivi appartenenti al
cammino critico, non si possono creare cicli, e tali perturbazioni sono le sole utili.

\subsection{Procedura Shifting Bottleneck}

\paragraph{Idea generale}
La procedura \emph{shifting bottleneck}, originariamente pensata per il problema
$J//C_{\max}$, sfrutta il grafo disgiuntivo per decomporre il problema in una
sequenza di problemi del tipo $1/r_i/L_{\max}$, per i quali sono disponibili algoritmi branch-and-bound efficienti.
Se una macchina $M_b$ è il collo di bottiglia, le altre macchine possono essere
trattate come risorse a capacità infinita, eliminando i relativi archi disgiuntivi corrispondenti alle altre macchine
e mantenere solo quelle corrispondenti alle altre macchine.

\paragraph{Teste e code nel grafo disgiuntivo}
Si consideri il nodo $O_{ib}$ del grafo, corrispondente all’operazione del job $J_i$
sulla macchina $M_b$, e sia $C_{ib}$ il suo tempo di completamento.
Si consideri il \textbf{cammino critico} dal nodo fittizio iniziale al nodo $O_{ib}$.
La sua lunghezza $h_{ib}$ è la somma dei tempi di processo delle operazioni di $J_i$
che precedono l’operazione $O_{ib}$, e ne costituisce la \textbf{testa}, ovvero il
\textbf{tempo di rilascio} dell’operazione $O_{ib}$.
In modo analogo si definisce la \textbf{coda} $t_{ib}$ come la lunghezza del cammino
critico dal nodo $O_{ib}$ al nodo fittizio finale. In questo caso $t_{ib}$ è la somma
dei tempi di processo delle operazioni che seguono $O_{ib}$.

\paragraph{Approssimazione del makespan}
Si definisce una data di consegna per il job $J_i$ sulla macchina $M_b$,
approssimando il makespan come
\(
C_{\max} = \max_i \{C_i\} \;\approx\; \max_i \{C_{ib} + t_{ib}\}.
\)
Se si sottrae una costante arbitraria $K$ dalla misura di prestazione, la soluzione
ottima del problema non cambia, ottenendo il problema equivalente:
\[
\max_i \{C_{ib} + t_{ib}\} - K
= \max_i \{C_{ib} - (K - t_{ib})\}
= \max_i \{C_{ib} - d_{ib}\},
\]
dove è stata introdotta una \textbf{due date locale}
\(
d_{ib} = K - t_{ib}.
\)
Più lunga è la coda $t_{ib}$, più stretta è la data di consegna $d_{ib}$.

\paragraph{Riduzione a $1/r_i/L_{max}$}
Il problema $J//C_{\max}$ si riconduce quindi a un problema
$1/r_i/L_{\max}$, in cui:
\(
r_{ib} = h_{ib}, \; d_{ib} = K - t_{ib}.
\)
I tempi di rilascio sono dati dalle \textbf{teste}, mentre le date di consegna
dipendono dalle \textbf{code}.

\paragraph{Identificazione del collo di bottiglia}
Un modo ragionevole per identificare il \textbf{collo di bottiglia} consiste nel
risolvere i problemi $1/r_i/L_{\max}$ per tutte le macchine, definendo per ciascuna
le teste e le code delle operazioni.
Il collo di bottiglia è la macchina che presenta il valore di $L_{\max}$ peggiore.
Dopo avere risolto i problemi su macchina singola, si individua il collo di bottiglia
$M_b$ e la relativa sequenza. È quindi possibile orientare gli archi disgiuntivi
corrispondenti a $M_b$ e rischedulare le altre macchine.
In pratica, si risolvono i problemi su macchina singola per le rimanenti $M-1$
macchine, aggiornando teste e code. Il procedimento prosegue fino ad avere
schedulato tutte le macchine, congelandole una per volta.
Sono possibili raffinamenti iterativi, e la strategia si estende facilmente al problema
$J/r_i/L_{max}$; sono state proposte anche ulteriori estensioni.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF 6 %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CAP 6 -- Fondamenti microeconomici del pricing e modelli di scelta discreta}

\subsection{Driver del profitto}

\paragraph{Modello base del profitto}
Il profitto è determinato da quattro fattori fondamentali ed è descritto dal seguente modello semplice:
\[
\text{Profitto} = (\text{Prezzo} - \text{Costo variabile}) \cdot \text{Volume} - \text{Costo fisso}.
\]

\paragraph{Caso base}
Si consideri il seguente caso:
\begin{itemize}[label=-]
    \item Prezzo: 100 euro per unità;
    \item Volume: 1 milione di unità;
    \item Costo variabile: 60 euro per unità;
    \item Costo fisso: 30 milioni di euro.
\end{itemize}
Il profitto risulta:
\[
(100 - 60)\cdot 10^6 - 30 \cdot 10^6 = 10 \text{ milioni di euro}.
\]

\paragraph{Sensibilità del profitto ai fattori}
Si considerano variazioni percentuali dei fattori nell’intervallo da $-10\%$ a $+10\%$ e il loro impatto percentuale sul profitto.
Un aumento del prezzo del $10\%$ comporta un raddoppio del profitto. Al contrario, una riduzione del costo variabile del $10\%$ implica un aumento del profitto pari al $50\%$. Gli altri due fattori hanno un impatto minore.
Il grafico risulta simmetrico rispetto alle variazioni positive e negative.

\paragraph{Osservazioni}
È importante notare che la modifica del prezzo è essenzialmente istantanea e priva di costo, a differenza degli investimenti in pubblicità o riduzione dei costi.
Tuttavia, il modello è poco realistico poiché il prezzo influisce sul volume e il costo non è necessariamente una funzione affine del volume. È quindi necessario considerare le interazioni tra i fattori, pur utilizzando questo modello grezzo per sviluppare intuizioni.

\subsection{Prezzo e volume}

\paragraph{Riduzione del prezzo}
Si assuma una riduzione del prezzo del $20\%$ rispetto al caso base. Per mantenere il profitto invariato, si deve risolvere:
\[
10 = (80 - 60)V_{\text{new}} - 30 \quad \Rightarrow \quad V_{\text{new}} = 20 \text{ milioni}.
\]
È quindi necessario un aumento del volume del $100\%$.

\paragraph{Aumento del prezzo}
Viceversa, se il prezzo aumenta del $20\%$, si ottiene:
\[
10 = (120 - 60)V_{\text{new}} - 30 \quad \Rightarrow \quad V_{\text{new}} = \frac{2}{3} \cdot 20 \text{ milioni}.
\]
È quindi possibile perdere un terzo del volume.

\paragraph{Catene causali}
È necessario modellare il legame tra prezzo e volume (domanda) ed essere consapevoli delle possibili catene causali:
\begin{itemize}[label=-]
    \item Prezzo $\rightarrow$ Volume $\rightarrow$ Ricavi $\rightarrow$ Profitto;
    \item Prezzo $\rightarrow$ Volume $\rightarrow$ Costi $\rightarrow$ Profitto;
    \item Prezzo $\rightarrow$ Prezzi dei concorrenti $\rightarrow$ Quota di mercato $\rightarrow$ Volume $\rightarrow$ Ricavi $\rightarrow$ Profitto;
    \item Prezzo del produttore $\rightarrow$ Prezzo del rivenditore $\rightarrow$ Volume $\rightarrow$ Ricavi $\rightarrow$ Profitto.
\end{itemize}

\subsection{Pricing e revenue management}

\paragraph{Profitto e costi}
L’obiettivo di un’impresa viene comunemente identificato nella massimizzazione del profitto. Tuttavia, l’analisi precedente mostra che ciò non si riduce alla sola minimizzazione dei costi.

\paragraph{Revenue management}
Se il profitto è ricavo meno costo, si aprono ulteriori possibilità. Le strategie di revenue e yield management sono diventate comuni a partire dall’industria aerea.

\paragraph{Strategie di pricing}
Le principali strategie di pricing includono:
\begin{itemize}[label=-]
    \item \textbf{Pricing basato sui costi}: valutazione del costo del prodotto e applicazione di un markup, ignorando le reazioni di consumatori e concorrenti;
    \item \textbf{Pricing basato sulla domanda}: il prezzo emerge dall’equilibrio tra domanda e offerta, modellabile in modo semplice in condizioni di monopolio;
    \item \textbf{Competizione}: modellata tramite la teoria dei giochi, sia tra imprese sia nell’interazione impresa–cliente.
\end{itemize}

\subsection{Forme di competizione}

\paragraph{Tipologie di interazione}
Le forme di competizione e interazione sono molteplici:
\begin{itemize}[label=-]
    \item competizione tra imprese (oligopolio vs. monopolio);
    \item interazioni lungo la supply chain (produttori e rivenditori);
    \item comportamento strategico dei clienti.
\end{itemize}

\paragraph{Esempio: trasporto aereo}
Un vettore aereo affronta:
\begin{itemize}[label=-]
    \item competizione diretta con altre compagnie;
    \item competizione con altri mezzi di trasporto (aereo vs. treno);
    \item competizione indiretta (viaggiare vs. meeting online).
\end{itemize}

\subsection{Pricing nella pratica}

\paragraph{Cambio di paradigma}
È necessario un cambio di paradigma:
\[
\text{Design--Build--Price} \;\longrightarrow\; \text{Price--Design--Build}.
\]

\paragraph{Posizionamento di prezzo}
Occorre selezionare una strategia di posizionamento coerente:
luxury, premium, medium, low, ultra-low.

\paragraph{Attributi del prodotto}
La strategia di pricing dipende dagli attributi chiave del prodotto o servizio:
funzionali, emotivi, simbolici, etici.

\paragraph{Scelta del modello}
È necessario scegliere il modello appropriato in funzione del contesto:
\begin{itemize}[label=-]
    \item acquisti ripetuti vs. acquisti singoli;
    \item beni di prima necessità vs. beni voluttuari;
    \item business-to-business (B2B) vs. business-to-consumer (B2C).
\end{itemize}

\subsection{Meccanismi alternativi}

\paragraph{Oltre il prezzo}
Il prezzo non è l’unico strumento per migliorare profitto e ricavi. Esistono diversi meccanismi alternativi:
\begin{itemize}[label=-]
    \item bundling e tying, sconti di quantità, pacchetti e servizi ancillari; talvolta unbundling;
    \item utilizzo di coupon;
    \item abbonamenti personalizzati;
    \item gestione della disponibilità di classi differenti;
    \item overbooking per gestire no-show e capacità non stoccabile;
    \item condivisione del rischio e progettazione di contratti e incentivi.
\end{itemize}

\subsection{Parte 1 -- Pricing e funzioni di domanda}

\paragraph{Un modello introduttivo: relazione lineare tra domanda e prezzo}
La domanda è una variabile casuale, influenzata da una molteplicità di fattori, incluso (ma non solo) il prezzo.
Come punto di partenza si introduce un modello semplice basato su una funzione di domanda lineare che lega prezzo e domanda:
\[
d(p) = \alpha - \beta p. \tag{1}
\]

\paragraph{Interpretazione dei parametri}
Un’ipotesi apparentemente ovvia è $\beta > 0$, ma esistono controesempi forniti dai beni di lusso e dai casi in cui il prezzo funge da segnale di qualità.
L’intercetta $\alpha$ rappresenta la domanda quando $p = 0$, un livello di prezzo per il quale il modello ha poco senso. In altri modelli, la domanda tende all’infinito per $p \to 0$, ma se la popolazione dei consumatori è finita, $\alpha$ può essere interpretato come la dimensione della popolazione.

\paragraph{Dominio di validità}
Poiché la domanda non può essere negativa, il modello ha senso nell’intervallo di prezzo
\[
p \in [0, p_{\text{lim}}],
\]
dove si definisce il prezzo limite
\[
p_{\text{lim}} := \frac{\alpha}{\beta}
\]
come il prezzo massimo per il quale la domanda è positiva.

\paragraph{Forma troncata della funzione di domanda}
Il modello può essere riscritto in una forma potenzialmente migliore come
\[
d(p) = (\alpha - \beta p)^+,
\]
dove $(x)^+ \equiv \max\{0, x\}$.

\paragraph{Funzione di domanda inversa}
In microeconomia è comune invertire la funzione di domanda per ottenere il prezzo di mercato quando una quantità $q$ viene prodotta e offerta sul mercato. Invertendo l’equazione (1), si ottiene la funzione di domanda inversa:
\[
p(q) = a - bq,
\]
dove $a = \alpha / \beta$ e $b = 1 / \beta$.
Questa forma è utile nello studio della competizione basata sulle quantità nella microeconomia elementare, ma risulta meno naturale nel contesto di una modellazione statistica appropriata per il pricing management.
Tuttavia, esistono mercati in cui il prezzo emerge da meccanismi complessi (ad esempio aste) che dipendono dalla disponibilità totale.

\subsection{Massimizzazione dei ricavi e del profitto}

\paragraph{Profitto come funzione della quantità}
Un obiettivo naturale è la massimizzazione del profitto. Nella sua forma più semplice, il profitto è dato da ricavi meno costi, e può essere espresso come funzione del prezzo o della quantità.
La seconda scelta è più naturale per esprimere i costi:
\[
\pi(q) = p(q)\cdot q - c(q) = r(q) - c(q),
\]
dove la funzione di costo $c(q)$ è una funzione generica della quantità e può tenere conto di costi fissi, economie o diseconomie di scala. Si introduce inoltre la funzione di ricavo
\[
r(q) := p(q)\cdot q.
\]

\paragraph{Condizione di ottimalità}
Assumendo che tutte le funzioni siano differenziabili e che la funzione di profitto complessiva sia concava, si applica la condizione di ottimalità del primo ordine:
\[
\pi'(q^*) = r'(q^*) - c'(q^*) = 0 \;\;\Rightarrow\;\; r'(q^*) = c'(q^*).
\]
La quantità ottimale è tale per cui il ricavo marginale e il costo marginale coincidono.
Se il ricavo marginale è maggiore del costo marginale, il beneficio in termini di ricavi derivante da un aumento della produzione supera l’aumento dei costi, e conviene aumentare $q$. Se il costo marginale è maggiore del ricavo marginale, conviene ridurre $q$.

\paragraph{Caso lineare con costo variabile costante}
Nel caso di funzione di domanda inversa lineare e costo variabile costante, il profitto è una funzione quadratica concava:
\[
\pi(q) = (a - bq)\cdot q - cq = (a - c)\cdot q - bq^2.
\]
La condizione di ottimalità del primo ordine è:
\[
\pi'(q) = (a - c) - 2bq,
\]
da cui si ottiene:
\[
q^* = \frac{a - c}{2b}.
\]

\paragraph{Osservazioni}
La massimizzazione del profitto non è equivalente alla minimizzazione dei costi.
La minimizzazione dei costi è appropriata quando una certa domanda deve comunque essere soddisfatta e occorre scegliere tra diverse tecnologie produttive o piani di produzione nel tempo.
In alcuni settori il costo marginale di un’unità aggiuntiva è trascurabile (industria aerea, servizi web) o il costo di produzione è un costo sommerso (markdown management). In questi casi può essere sensata la massimizzazione dei ricavi, naturalmente espressa come funzione del prezzo:
\[
\max r(p) = p\cdot d(p) = p\cdot(\alpha - \beta p) = \alpha p - \beta p^2.
\]

\paragraph{Soluzione ottima per la massimizzazione dei ricavi}
La soluzione ottima è:
\[
p^* = \frac{\alpha}{2\beta}
\quad\Rightarrow\quad
d^* = \frac{\alpha}{2}
\quad\Rightarrow\quad
r^* = \frac{\alpha^2}{4\beta}.
\]

\paragraph{Interpretazione geometrica}
La soluzione può essere interpretata geometricamente: il prezzo ottimale è il punto medio dell’intervallo dei prezzi e massimizza l’area del rettangolo associato ai ricavi.

\subsection{Stima del modello}

\paragraph{Problema della stima}
Nella microeconomia elementare si utilizzano modelli deterministici, ma anche un modello di domanda lineare apparentemente banale come
\[
d(p) = \alpha - \beta p
\]
contiene parametri ignoti che devono essere stimati.

\paragraph{Apprendimento e sperimentazione}
Anche assumendo che il modello lineare sia corretto, esiste un problema rilevante: il costo della procedura di stima.
È necessario apprendere online piuttosto che offline, e la pianificazione degli esperimenti è cruciale.
Quali prezzi utilizzare per apprendere efficacemente i parametri?
L’attenzione dovrebbe concentrarsi sulla pendenza $\beta$, poiché l’intercetta $\alpha$ è principalmente un costrutto matematico.

\paragraph{Errore di stima}
L’errore standard della stima della pendenza è dato da:
\[
SE_\beta =
\sqrt{
\frac{\sigma_\varepsilon^2}
{\sum_{k=1}^n (p_k - \bar{p})^2}
},
\]
dove $\sigma_\varepsilon$ è la deviazione standard (ignota) degli errori.
Questo mostra che maggiore è la dispersione dei prezzi sperimentati, migliore è l’apprendimento. Tuttavia, prezzi molto bassi o molto elevati possono danneggiare i ricavi, generando un trade-off tra apprendimento e costo dell’apprendimento.

\paragraph{Problemi pratici aggiuntivi}
Nella pratica emergono ulteriori difficoltà:
\begin{itemize}[label=-]
    \item prezzi commerciali: non tutti i prezzi sono applicabili;
    \item grandi variazioni di prezzo enfatizzano la non linearità;
    \item ruolo del tempo: nei beni di moda il tempo è essenziale e il periodo di apprendimento è limitato;
    \item fattori confondenti: una variazione della domanda può dipendere da elementi diversi dal prezzo;
    \item problemi operativi ed esecutivi, ad esempio la modifica dei cartellini in un negozio fisico;
    \item sperimentazione eccessiva può generare insoddisfazione nei consumatori e indurre comportamenti strategici.
\end{itemize}

\subsection{Parte 1 -- Limiti dei modelli di domanda lineare}

\paragraph{Criticità del modello lineare}
Il modello di domanda lineare presenta diversi limiti:
\begin{itemize}[label=-]
    \item l’ipotesi di pendenza costante (sensibilità al prezzo) non è realistica;
    \item si basa su un’ipotesi irrealistica sulla willingness-to-pay;
    \item si assume che la pendenza sia negativa; tuttavia, nel caso di beni di lusso e quando il prezzo funge da segnale di qualità, si possono osservare comportamenti localmente differenti;
    \item è un modello statico: l’introduzione del tempo è essenziale nelle strategie dinamiche di markdown;
    \item non si considera il ruolo della capacità (si veda, ad esempio, il peak-load pricing);
    \item si considera la vendita di un singolo bene o servizio (si veda tying, bundling e assortment management);
    \item non si considera la scelta del consumatore tra alternative che possono differire non solo per il prezzo;
    \item non si considerano aspetti comportamentali e asimmetrie informative; la psicologia del pricing può essere rilevante.
\end{itemize}

\subsection{La funzione di costo}

\paragraph{Costo fisso e costo variabile}
È comune considerare una componente di costo fisso e una variabile; la forma più semplice di costo variabile è lineare:
\[
c(q) = F + cq.
\]

\paragraph{Costo fisso vs. costo con carica fissa}
Una funzione di costo alternativa è:
\[
c(q) =
\begin{cases}
F + cq & \text{se } q > 0, \\
0 & \text{se } q = 0.
\end{cases}
\]
che può essere scritta come
\[
c(q) = F \cdot \delta(q) + cq,
\]
dove $\delta(q) = 1$ se $q > 0$ e $\delta(q) = 0$ altrimenti.
Per evitare ambiguità, si parla di \textbf{costo fisso} nel primo caso e di \textbf{carica fissa} nel secondo.
Si noti che questa distinzione può dipendere dalla scala temporale e dal livello decisionale gerarchico: alla scala temporale appropriata, tutti i costi sono variabili (si veda il concetto di costi semivariabili).

\paragraph{Costo marginale e costo medio}
Data una funzione di costo, si definiscono i seguenti concetti:
\begin{itemize}[label=-]
    \item costo marginale $c'(q)$;
    \item costo medio $\dfrac{c(q)}{q}$.
\end{itemize}

\paragraph{Regolarità della funzione di costo}
Per definire il costo marginale è necessario assumere la differenziabilità.
Oltre alle cariche fisse, discontinuità possono essere introdotte da sconti di quantità all-units, mentre sconti incrementali introducono punti angolosi.
Quando la produzione è discreta e $q$ assume valori interi, il costo marginale può essere approssimato come
\[
c(q+1) - c(q).
\]

\paragraph{Economie e diseconomie di scala}
Il costo marginale può essere crescente o decrescente.
Quando il costo marginale è decrescente, assumendo la differenziabilità, si ha $c''(q) < 0$, e quindi la funzione di costo è concava. Questo modella un’economia di scala, ossia un aumento dell’efficienza all’aumentare della produzione.
Al contrario, una funzione di costo convessa, caratterizzata da un costo marginale crescente, modella una diseconomia di scala.

\subsection{Funzioni di domanda}

\paragraph{Classi di modelli di domanda}
Esiste una grande varietà di modelli di domanda:
\begin{itemize}[label=-]
    \item si può modellare la domanda aggregata dell’intero mercato, di un canale selezionato o di una parte del mercato, oppure la domanda di un singolo individuo;
    \item si possono considerare acquisti ripetuti oppure no, a seconda della natura del bene o servizio (beni stock vs. beni flusso) e dell’orizzonte temporale;
    \item si può considerare o meno l’incertezza.
\end{itemize}

\paragraph{Input e output dei modelli}
I fattori di input possono includere tempo, vendite passate, prezzi dei concorrenti o di beni alternativi.
L’output del modello può essere:
\begin{itemize}[label=-]
    \item una variabile reale (domanda aggregata, eventualmente approssimante un valore intero elevato);
    \item una variabile intera (quantità acquistata da un singolo consumatore);
    \item una variabile binaria (acquisto o non acquisto);
    \item una variabile categoriale o un vettore (bundle o portafoglio di beni acquistati).
\end{itemize}

\subsection{La funzione di domanda lineare}

\paragraph{Forma del modello}
Per scopi principalmente illustrativi, si è già considerata la funzione di domanda lineare:
\[
d(p) = (\alpha - \beta p)^+.
\]

\paragraph{Questioni aperte}
È naturale interrogarsi sulla validità di un modello così semplice:
\begin{itemize}[label=-]
    \item come può essere giustificato dal punto di vista economico? È necessario indagare una fondazione microeconomica;
    \item è un modello validato empiricamente o è contraddetto dal buon senso e dal comportamento reale dei consumatori?
\end{itemize}

\paragraph{Pendenza della domanda}
Assumendo la differenziabilità, una caratteristica fondamentale di ogni funzione di domanda è la sua pendenza $d'(p)$, che per buon senso dovrebbe essere negativa, come nel modello lineare.
In effetti è solitamente negativa, ma esistono eccezioni:
\begin{itemize}[label=-]
    \item segnale di qualità;
    \item beni di lusso e consumo vistoso.
\end{itemize}
Possono quindi esistere intervalli di prezzo per i quali la pendenza è positiva, un fatto confermato da evidenze empiriche (si vedano esempi in Simon, 2015).

\subsection{Funzioni di domanda: elasticità puntuale}

\paragraph{Elasticità di prezzo della domanda}
Poiché il valore della pendenza dipende dalla scala, ovvero dalle unità di misura della domanda, una misura più appropriata è l’elasticità di prezzo della domanda:
\[
\frac{\Delta d / d}{\Delta p / p}
=
\frac{\Delta d}{\Delta p} \cdot \frac{p}{d}.
\]
Facendo tendere $\Delta p \to 0$, si ottiene l’elasticità puntuale:
\[
\varepsilon(p) = -\frac{d'(p)p}{d(p)}, \tag{2}
\]
dove il segno meno è introdotto per comodità, poiché la pendenza è tipicamente negativa.

\paragraph{Classificazione della domanda}
Le funzioni di domanda possono essere classificate come:
\begin{itemize}[label=-]
    \item elastiche, quando $\varepsilon(p) > 1$ (perfettamente elastiche se pari a 1);
    \item a elasticità unitaria, quando $\varepsilon(p) = 1$;
    \item anelastiche, quando $\varepsilon(p) < 1$ (perfettamente anelastiche se pari a 0).
\end{itemize}
La natura della domanda può dipendere dal punto specifico considerato.

\subsection{Esempio: funzione di domanda lineare}

\paragraph{Elasticità vs. pendenza}
L’elasticità non deve essere confusa con la pendenza, come mostra il seguente esempio.
Il modello di domanda lineare presenta una pendenza costante, ma ciò non implica un’elasticità puntuale costante.

\paragraph{Elasticità nel caso lineare}
Per una funzione di domanda lineare, l’elasticità è:
\[
\varepsilon(p) =
\frac{\beta p}{\alpha - \beta p}.
\]
Essa cresce da zero a $+1$ nell’intervallo $[0, p_{\text{lim}}]$.
L’elasticità è pari a 1 per
\[
p = \frac{\alpha}{2\beta} = p^*,
\]
ossia nel punto medio dell’intervallo dei prezzi.
La domanda è anelastica per $p < p^*$ ed elastica per $p > p^*$.
Nel limite, quando $p \to p_{\text{lim}} = \alpha/\beta$, diventa perfettamente elastica: una piccola riduzione di prezzo fa passare la domanda da $d = 0$ a $d > 0$.
L’elasticità non è costante per una funzione di domanda lineare.

\paragraph{Domanda a elasticità costante}
È naturale chiedersi se esista una funzione di domanda con elasticità costante.
Imponendo
\[
\frac{d'(p)p}{d(p)} = -\varepsilon,
\]
si ottiene l’equazione differenziale ordinaria:
\[
d'(p) = -\varepsilon \cdot \frac{d(p)}{p},
\]
che ammette come soluzione:
\[
d(p) = c \cdot p^{-\varepsilon},
\]
dove $c = d(1)$.

\paragraph{Nota tecnica}
Per risolvere l’equazione differenziale precedente, si procede per separazione delle variabili:
\[
\frac{dy}{y} + \varepsilon \frac{dx}{x} = 0,
\]
dove $y \equiv d(p)$ e $x \equiv p$.
Integrando si ottiene:
\[
\int \frac{dz}{z} + \varepsilon \int \frac{dz}{z} = 0
\Rightarrow \log y + \varepsilon \log x = K,
\]
dove $K$ è una costante di integrazione, da cui:
\[
\log(y \cdot x^{\varepsilon}) = K
\Rightarrow y = c x^{-\varepsilon},
\]
ponendo $c = e^K$.
Imponendo $p = 1$, si ottiene:
\[
d(1) = c.
\]

\paragraph{Stima tramite trasformazione logaritmica}
È possibile derivare un legame tra domanda a elasticità costante e domanda lineare mediante una trasformazione dei dati basata sui logaritmi:
\[
d = c p^{-\varepsilon}
\;\Rightarrow\;
\log d = \log c - \varepsilon \log p.
\]
In questo modo, gli strumenti standard di regressione lineare possono essere utilizzati per stimare il modello.
L’uso dei logaritmi è utile per semplificare modelli basati su effetti moltiplicativi anziché additivi.

\paragraph{Elasticità come derivata logaritmica}
In termini di incrementi, per $x > 0$ vale:
\[
\frac{d \log x}{dx} = \frac{1}{x}
\quad\Rightarrow\quad
\Delta \log x \approx \frac{\Delta x}{x}.
\]
Di conseguenza, l’elasticità può essere riscritta come:
\[
\frac{\Delta d / d}{\Delta p / p}
\approx
\frac{d \log d(p)}{d \log p}.
\]

\paragraph{Ricavi con domanda a elasticità costante}
Nel caso di domanda a elasticità costante, il ricavo è:
\[
R(p) = p \cdot c p^{-\varepsilon} = c p^{1-\varepsilon}.
\]
Il ricavo è costante nel caso di elasticità unitaria, crescente nel caso di domanda anelastica e decrescente nel caso di domanda elastica.
Pertanto, una riduzione di prezzo aumenta i ricavi solo se la domanda è elastica.

\subsection{Parte 1 -- Interpretare i modelli di domanda: willingness-to-pay}

\paragraph{Confronto tra modelli e razionale economico}
Come si possono confrontare diversi modelli di domanda? La funzione di domanda lineare è un modello sensato?
Un modello lineare può essere (nel migliore dei casi) un’approssimazione locale, ma è necessario comprendere più a fondo la razionalità economica alla base di tale modello.
Ciò significa che occorre capire che cosa determina realmente la domanda.

\paragraph{Reserve price e willingness-to-pay}
Un meccanismo di base dietro i modelli di domanda è il concetto di prezzo di riserva o willingness-to-pay: ogni consumatore è caratterizzato dal prezzo massimo al quale è disposto ad acquistare un bene.

\paragraph{Esempio con segmentazione: studenti e non studenti}
Come esempio semplice, si consideri la seguente tabella:
\[
\begin{array}{c|cc}
 & \text{Studenti} & \text{Non studenti} \\
\hline
\text{Willingness-to-pay} & \text{e }5 & \text{e }10 \\
\text{Dimensione del mercato} & 200 & 300
\end{array}
\]
Questo esempio ipotetico descrive la willingness-to-pay per un certo tipo di spettacolo, se la popolazione può essere segmentata in due gruppi, studenti e non studenti, con diversa willingness-to-pay.

\paragraph{Domanda a tratti}
Questo produce una funzione di domanda a tratti costante, come indicato in figura.

\paragraph{Popolazione differenziata e funzione a gradini}
Si immagini ora una popolazione più differenziata di $N$ consumatori potenziali, indicizzati da $k = 1,\dots,N$, ognuno con una willingness-to-pay unica:
\[
p_k = p_{\max}\,\frac{k}{N}.
\]
L’indice $k=1$ corrisponde al consumatore meno disposto a pagare, tale che tutti acquistano e la domanda è $N$. Per $k=N$, soltanto un consumatore acquista.
La domanda è legata a $k$ da:
\[
d(p_k) = N - k + 1,\qquad k = 1,\dots,N,
\]
da cui risulta una funzione decrescente a gradini.
L’intuizione suggerisce che, nel limite continuo, assumendo una willingness-to-pay uniformemente distribuita, si ottenga una funzione di domanda lineare.
Funzioni di domanda alternative possono essere definite assumendo distribuzioni differenti del prezzo di riserva e studiando la sensibilità della domanda rispetto al prezzo.

\subsection{Surplus del consumatore e discriminazione di prezzo}

\paragraph{Definizione di consumer surplus}
Si consideri un consumatore disposto a pagare e100 per un bene. Se il prezzo è e70, si dice che il consumer surplus è la differenza tra willingness-to-pay e prezzo pagato, in questo caso e30.
Quando viene applicato un prezzo unico, una parte di consumer surplus è ottenuta dall’insieme dei consumatori che acquistano.

\paragraph{Consumer surplus in forma geometrica}
Geometricamente, ciò è visualizzabile come il triangolo $CS$ in figura, con area:
\[
CS = \frac{1}{2}\left(\frac{\alpha}{\beta} - p_0\right)\,(\alpha - \beta p_0)
= \frac{1}{2\beta}\,(\alpha - \beta p_0)^2,
\]
dove $p_0$ è il prezzo scelto.
Si noti che il consumer surplus è nullo quando $p_0 = p_{\lim} = \alpha/\beta$, ed è pari a $\alpha^2/(2\beta)$, ossia l’area totale del triangolo, quando $p_0 = 0$.
In figura, il ricavo raccolto è rappresentato dall’area rettangolare $REV$.

\paragraph{Interpretazione economica}
In microeconomia, il consumer surplus è una misura approssimata dell’utilità del consumatore derivante dallo scambio.
Guardando dalla prospettiva opposta, il consumer surplus è, in un certo senso, ricavo perso per l’impresa.
Lo stesso vale per il triangolo $LR$, che rappresenta ricavo perso dovuto ai consumatori che non acquistano perché $p_0$ eccede la loro willingness-to-pay.

\paragraph{Prezzo unico e perdite di ricavo}
Considerando solo i ricavi, l’obbligo di scegliere un solo prezzo genera perdite su entrambi i lati: consumer surplus (opportunità mancata di profitto maggiore) e vendite perse.
Idealmente, l’impresa vorrebbe applicare un prezzo diverso a ciascun individuo, pari alla sua willingness-to-pay (assumendo costo marginale nullo).

\paragraph{Discriminazione di prezzo}
Naturalmente, fissare un prezzo individuale non è pratico, per non dire apertamente illegale.
Come può un’impresa estrarre almeno parzialmente consumer surplus?
La chiave è la discriminazione di prezzo basata sulla segmentazione dei consumatori, illustrata da un esempio semplice (tratto da Shy, 2008, p. 6).

\subsection{Esempio di discriminazione di prezzo}

\paragraph{Caso senza discriminazione}
Si consideri nuovamente la willingness-to-pay della tabella precedente.
Se non è possibile discriminare, cioè si deve applicare lo stesso prezzo a tutta la popolazione, ci sono due scelte:
\begin{itemize}[label=-]
    \item Opzione 1: vendere a e10, con ricavo
    \[
    10 \cdot 300 = \text{e3000}.
    \]
    \item Opzione 2: vendere a e5, con ricavo
    \[
    5 \cdot (200+300) = \text{e2500}.
    \]
\end{itemize}
È quindi preferibile chiedere il prezzo più alto, escludendo gli studenti dall’acquisto del biglietto.

\paragraph{Caso con discriminazione}
Se si può discriminare e applicare prezzi diversi, il profitto aumenta:
\[
5 \cdot 200 + 10 \cdot 300 = \text{e4000}.
\]
Una tale politica è praticabile se si possono identificare gli studenti, ad esempio richiedendo un tesserino, e prevenire l’arbitraggio (uno studente che compra a e5 e rivende a un non studente a e10), che avrebbe un effetto di diluizione sul ricavo.

\paragraph{Tipi di discriminazione}
Applicando discriminazione di prezzo, l’impresa riesce a estrarre consumer surplus. D’altra parte, una sottopopolazione che sarebbe esclusa dallo scambio può partecipare.
In generale, non è detto che si ottenga una soluzione win--win.
Inoltre, potrebbe non essere possibile discriminare in modo netto.
Esistono diversi tipi di discriminazione:
\begin{itemize}[label=-]
    \item discriminazione completa: ogni consumatore paga un prezzo specifico;
    \item segmentazione diretta: segmenti identificabili pagano prezzi diversi;
    \item segmentazione indiretta: si offrono varianti di prodotto/servizio e i consumatori scelgono; ciò funziona anche con caratteristiche non identificabili.
\end{itemize}
Esempi: acquisto anticipato e prenotazione; coupon di sconto.

\subsection{Formalizzare willingness-to-pay e consumer surplus}

\paragraph{Distribuzione della willingness-to-pay}
Dopo un’introduzione informale, si passa a un quadro più formale.
La willingness-to-pay è il prezzo massimo (prezzo di riserva) che un consumatore è disposto a pagare per un bene.
La distribuzione della willingness-to-pay nella popolazione determina la forma della funzione di domanda.

\paragraph{Modello continuo e funzione $w(x)$}
Si consideri un modello continuo e si definisca $w(x)$ come distribuzione della willingness-to-pay.
Essa svolge un ruolo simile alla densità di una variabile casuale, nel senso che
\[
\int_{p_1}^{p_2} w(x)\,dx
\]
è la frazione di popolazione con willingness-to-pay tra $p_1$ e $p_2$.
Quindi:
\[
d(p) = D_{\max}\int_{p}^{+\infty} w(x)\,dx,
\]
dove $D_{\max}=d(0)$ è la domanda massima ottenibile (assumendo una popolazione finita).
Poiché
\[
d'(p) = -D_{\max}\,w(p),
\]
si può legare domanda e willingness-to-pay tramite:
\[
w(p) = -\frac{d'(p)}{D_{\max}}. \tag{3}
\]

\paragraph{Surplus del consumatore aggregato}
Se un consumatore disposto a pagare $p_1$ paga invece $p_0<p_1$, gode di un consumer surplus $p_1-p_0$.
Dato $w(x)$, si integra il surplus al prezzo $p_0$ sulla popolazione che acquista e si definisce il consumer surplus totale (netto):
\[
S(p_0)=D_{\max}\int_{p_0}^{+\infty} w(x)\,(x-p_0)\,dx.
\]
Usando (3) e integrando per parti:
\[
S(p_0)= -\int_{p_0}^{+\infty} d'(x)\,(x-p_0)\,dx
= -\int_{p_0}^{+\infty} d'(x)x\,dx + p_0\int_{p_0}^{+\infty} d'(x)\,dx
\]
\[
= -\left[\left.d(x)\,x\right|_{p_0}^{+\infty}-\int_{p_0}^{+\infty} d(x)\,dx\right]-p_0d(p_0)
= \int_{p_0}^{+\infty} d(x)\,dx.
\]
Quindi, il consumer surplus è l’area sotto la curva di domanda, a destra del prezzo applicato $p_0$.

\subsection{Esempio: consumer surplus con domanda lineare e costo}

\paragraph{Dati e prezzo limite}
Si consideri un articolo con costo unitario di produzione e3 e funzione di domanda aggregata:
\[
d(p)=(1000-100p)^+.
\]
Il prezzo limite che comporta domanda nulla è:
\[
p_{\lim}=\frac{1000}{100}=\text{e10}.
\]

\paragraph{Prezzo ottimo, domanda, profitto e surplus}
Il prezzo ottimo è:
\[
p^*=\frac{1000+100\cdot 3}{2\cdot 100}=6.5,
\]
con domanda, profitto e consumer surplus dati rispettivamente da:
\[
d(p^*)=1000-100\cdot 6.5=350,
\]
\[
\pi^*=350\cdot(6.5-3)=\text{e1225},
\]
\[
S(p^*)=\frac{(10-6.5)\cdot 350}{2}=612.5.
\]
Il consumer surplus è l’area ombreggiata ed è interpretabile come la perdita di opportunità per l’impresa dovuta all’impossibilità di chiedere il prezzo di riserva a ciascun consumatore.

\paragraph{Obiettivo di estrarre consumer surplus}
Un obiettivo naturale per l’impresa è ``estrarre'' consumer surplus almeno parzialmente tramite una forma di discriminazione di prezzo.
Se l’impresa è in grado di discriminare, migliorerà il proprio profitto.
L’intuizione suggerisce che ciò avvenga necessariamente a spese dei consumatori.
L’esempio seguente mostra che non è necessariamente così, se consumatori con willingness-to-pay minore possono acquistare a un prezzo minore.

\subsection{Esempio: consumer surplus e discriminazione di prezzo}

\paragraph{Domanda aggregata come somma di due popolazioni}
Si consideri la stessa funzione di domanda dell’esempio precedente, ma si immagini che derivi dall’aggregazione di due popolazioni con funzioni di domanda:
\[
d_1(p)=\min\left\{500,\,(1000-100p)^+\right\},
\qquad
d_2(p)=(500-100p)^+.
\]
Ciò implica che la popolazione 1 include consumatori con willingness-to-pay maggiore di e5, mentre la popolazione 2 include consumatori con willingness-to-pay minore di e5.

\paragraph{Prezzo ottimo per la popolazione 1}
Se l’impresa può discriminare senza arbitraggio o diluizione, il prezzo ottimo per la popolazione 1 resta lo stesso di prima, così come domanda e profitto:
\[
p_1^*=\text{e6.5},\qquad d_1(p_1^*)=350,\qquad \pi_1^*=\text{e1225}.
\]

\paragraph{Prezzo ottimo per la popolazione 2}
L’impresa può vendere anche alla popolazione 2, con prezzo ottimo, domanda e profitto:
\[
p_2^*=\frac{500+100\cdot 3}{2\cdot 100}=\text{e4},
\]
\[
d_2(p_2^*)=500-100\cdot 4=100,
\]
\[
\pi_2^*=100\cdot(4-3)=\text{e100}.
\]

\paragraph{Profitto totale e surplus addizionale}
Poiché è possibile vendere sopra costo alla popolazione 2 senza ridurre il ricavo dalla popolazione 1, l’impresa ottiene un profitto addizionale e il profitto totale è:
\[
\pi^*=\pi_1^*+\pi_2^*=\text{e1325}.
\]
L’impresa beneficia della segmentazione, ma anche i consumatori.
Esiste un consumer surplus addizionale, poiché 100 consumatori possono acquistare a un prezzo inferiore al proprio prezzo di riserva:
\[
S_2(p_2^*)=\frac{100\cdot(5-4)}{2}=50.
\]
Il surplus totale è composto dai due triangoli ombreggiati.

\subsection{La funzione logit}

\paragraph{Da willingness-to-pay a domanda}
Integrare la densità di willingness-to-pay, analogamente a quanto si fa con una PDF per ottenere una CDF, fornisce una funzione di domanda.
Un modello di domanda lineare non deriva da una distribuzione sensata della willingness-to-pay; è ragionevole ipotizzare una funzione ``a campana'' con un massimo (moda) concentrato attorno al prezzo di riserva più tipico.

\paragraph{Probit e logit}
Nella letteratura di statistica e marketing si utilizzano due funzioni per questo e altri scopi, che portano a modelli probit e logit.
I modelli probit derivano dalla CDF di una normale standard, mentre i modelli logit si basano sulla funzione logistica (sigmoide).

\paragraph{Definizione della funzione logistica}
La funzione logistica è definita come:
\[
L(x)=\frac{1}{1+e^{-x}}=\frac{e^x}{1+e^x},
\]
e presenta la classica forma a S.
Si osservi che $L(0)=0.5$ e che la funzione logit ha la proprietà di simmetria:
\[
L(-x)=1-L(x).
\]

\paragraph{Derivata della funzione logistica}
In figura si mostra anche la derivata:
\[
L'(x)=\frac{e^x}{(1+e^x)^2}=L(x)\,(1-L(x)).
\]
La derivata ha la proprietà di simmetria $L'(x)=L'(-x)$ e il suo massimo si ha in $x=0$.

\paragraph{Funzione di domanda logit decrescente}
Poiché le funzioni di domanda devono essere decrescenti, si deve ribaltare l’asse $x$ da sinistra a destra e introdurre fattori di scala e traslazione sensati nella funzione logit, ottenendo la seguente funzione di domanda logit (decrescente):
\[
d(p)=\frac{c\,e^{-(\alpha+\beta p)}}{1+e^{-(\alpha+\beta p)}},
\]
dove $\beta,\,c>0$.
Più grande è $\beta$, maggiore è la sensibilità al prezzo, e la funzione è più ripida per
\[
\alpha+\beta p^*=0 \;\Rightarrow\; p^*=-\frac{\alpha}{\beta},
\]
il che suggerisce $\alpha<0$.
Questo è il valore per cui la densità di willingness-to-pay è massima e può essere considerato un ``prezzo di mercato''.

\paragraph{Willingness-to-pay corrispondente}
La willingness-to-pay corrispondente è:
\[
w(x)=-\frac{d'(p)}{d(0)}
=\frac{K\,e^{-(\alpha+\beta x)}}{\left(1+e^{-(\alpha+\beta x)}\right)^2},
\]
dove $K=\beta c/d(0)$.

\paragraph{Esempio numerico}
Si consideri il caso $c=1000$, $p^*=20$ e $\beta=0.5$.
In tal caso,
\[
\alpha=-p^*\beta=-20\cdot 0.5=-10,
\]
e la domanda per $p=0$ è:
\[
d(0)=\frac{1000\cdot e^{-\alpha}}{1+e^{-\alpha}}
=
\frac{1000\cdot e^{10}}{1+e^{10}}
=999.9546 \approx c.
\]

\subsection{Parte 1 -- Un modello ibrido}

\paragraph{Prezzi e quantità come variabili decisionali}
Modelli diversi, come si vedrà, possono essere costruiti basandosi su quantità o prezzi.
Qui si considera un esempio numerico istruttivo (Kuyumcu e Popescu, 2006) che mostra che prezzi e quantità non devono necessariamente essere variabili decisionali mutuamente esclusive.

\paragraph{Domanda lineare per due beni complementari}
Si considerino due articoli che seguono un modello di domanda lineare:
\[
d_1 = 500 - p_1 - 5p_2,
\qquad
d_2 = 10 - 0.01p_1 - 0.05p_2.
\]
L’ispezione dei segni mostra che i due beni sono \textbf{complementi}, e non sostituti (ridurre $p_2$ aumenta $d_1$).
Nel lavoro originale, i due beni sono camere standard e sale riunioni in un hotel, che chiaramente non sono sostituti. Questo aiuta a comprendere i valori numerici dei parametri.

\paragraph{Vincoli di capacità}
In pratica possono esistere limitazioni di capacità. In questo caso, la disponibilità è limitata a 250 e 6, rispettivamente.
È quindi necessario modellare esplicitamente la domanda.

\subsection{Ottimizzazione dei prezzi con domanda legata alle vendite}

\paragraph{Prima formulazione (non sensata)}
Per trovare i prezzi ottimi si potrebbe considerare il seguente modello:
\[
\max \; p_1 d_1 + p_2 d_2
\]
soggetto a
\[
d_1 = 500 - p_1 - 5p_2,
\]
\[
d_2 = 10 - 0.01p_1 - 0.05p_2,
\]
\[
p_1,\; p_2,\; d_1,\; d_2 \ge 0,
\]
\[
d_1 \le 250,\qquad d_2 \le 6.
\]
Tuttavia, questo non è un modello particolarmente sensato.

\paragraph{Soluzione della prima formulazione}
La soluzione, in termini di prezzi ottimi, è:
\[
p_1^* = 400,\qquad p_2^* = 0.
\]
Questi prezzi implicano le domande:
\[
d_1^* = 100,\qquad d_2^* = 6.
\]
Si osserva che le sale riunioni sono offerte gratuitamente, cosa che può essere sensata per stimolare le vendite del bene complementare.

\paragraph{Prezzi nulli o negativi per beni complementari}
In alcuni casi estremi, il prezzo di un bene potrebbe anche essere negativo per incrementare le vendite del bene complementare.
Questo può sembrare strano ma, in realtà, alcuni beni possono essere venduti sotto costo per ottenere ricavi dai beni complementari.
Ad esempio, una stampante laser permette di vendere cartucce toner, e un rasoio permette di vendere lame.

\paragraph{Effetti su capacità e ricavi}
Il prezzo per le camere standard è piuttosto elevato e alcune camere restano inutilizzate, mentre la capacità è vincolante per le sale riunioni.
Il ricavo complessivo è 40{,}000.

\subsection{Che cosa non funziona nel primo modello?}

\paragraph{Collegamento rigido tra domanda e vendite}
Il prezzo delle camere standard è elevato ed è tale che la domanda di sale riunioni è esattamente pari alla disponibilità.

\paragraph{Inventory rationing}
Una pratica comune nel revenue management è l’\textbf{inventory rationing}, ossia la restrizione delle vendite.
Questo è lo strumento principale nel revenue management basato sulle quantità, come si vedrà.

\subsection{Formulazione alternativa: disaccoppiare vendite e domanda}

\paragraph{Seconda formulazione (con razionamento)}
Si può disaccoppiare la vendita dalla domanda risolvendo la seguente formulazione alternativa:
\[
\max \; p_1 q_1 + p_2 q_2
\]
soggetto a
\[
q_1 \le 500 - p_1 - 5p_2,
\]
\[
q_2 \le 10 - 0.01p_1 - 0.05p_2,
\]
\[
p_1,\; p_2,\; q_1,\; q_2 \ge 0,
\]
\[
q_1 \le 250,\qquad q_2 \le 6.
\]
Qui, la funzione di domanda lineare fornisce un limite superiore alla quantità offerta.

\paragraph{Soluzione della seconda formulazione}
La soluzione ottima è:
\[
p_1^* = 250,\qquad p_2^* = 0,\qquad q_1^* = 250,\qquad q_2^* = 6,
\]
con ricavo pari a 62{,}500.

\paragraph{Interpretazione della differenza tra le due formulazioni}
Il problema della prima formulazione è che questa soluzione produrrebbe una domanda pari a 7.5 per il secondo bene, che non è realizzabile se la domanda è rigidamente legata alle vendite.
Il disaccoppiamento tramite razionamento fornisce gradi di libertà aggiuntivi.

\subsection{Parte 2 -- Modelli di teoria dei giochi}

\paragraph{Problemi decisionali con più decisori}
Talvolta le decisioni di pricing non possono essere prese ignorando il contesto complessivo.
In un contesto B2C, i consumatori strategici sono un problema. Le interazioni strategiche sono ancora più rilevanti in un contesto B2B, dove i produttori possono essere collegati da una supply chain in cui beni intermedi vengono scambiati e trasformati, oppure dove produttori e retailer che distribuiscono beni finali interagiscono.
In tale contesto, i prezzi possono svolgere un ruolo importante come strumento per coordinare azioni e condividere rischi.
Infine, che dire della competizione tra imprese? Occorre distinguere tra competizione in termini di prezzi o di quantità, e tra beni/servizi identici o differenziati.
Questi problemi possono essere (almeno parzialmente) affrontati con modelli di teoria dei giochi.

\paragraph{Ipotesi semplificative}
Per semplicità, si considerano giochi stilizzati:
\begin{itemize}[label=-]
    \item ci sono solo due decisori (giocatori); ciascun giocatore ha un obiettivo (payoff) che vuole massimizzare e non esiste cooperazione;
    \item ciascun giocatore prende una sola decisione; quindi non si considerano giochi sequenziali con decisioni multiple nel tempo;
    \item si assume informazione completa e common knowledge: non c’è incertezza sui dati del problema né sui meccanismi che mappano decisioni in payoff; i due giocatori condividono la stessa visione del mondo e le regole del gioco, conoscono gli incentivi della controparte e ciascuno sa che l’altro possiede tutte le informazioni rilevanti.
\end{itemize}

\paragraph{Problema centralizzato e interazione tra decisioni}
Per avvicinarsi a una formalizzazione, si consideri il problema:
\[
\max\ \pi_1(x_1;x_2) + \pi_2(x_1;x_2) \tag{4}
\]
soggetto a
\[
x_1 \in S_1,\qquad x_2 \in S_2.
\]
La funzione obiettivo (4) può essere interpretata come un profitto dipendente da due variabili decisionali, $x_1$ e $x_2$, che devono appartenere ai rispettivi insiemi ammissibili $S_1$ e $S_2$.

\paragraph{Perché non si può decomporre}
Anche se i vincoli su $x_1$ e $x_2$ sono separabili, non è possibile decomporre il problema complessivo, poiché le due decisioni interagiscono tramite le funzioni di profitto $\pi_1(x_1;x_2)$ e $\pi_2(x_1;x_2)$.
Risolvendo (4) si ottengono decisioni ottime $x_1^*$ e $x_2^*$ e il profitto totale ottimo:
\[
\pi_{1+2}^*=\pi_1(x_1^*;x_2^*)+\pi_2(x_1^*;x_2^*).
\]
Così facendo si assume un decisore unico che prende entrambe le decisioni, oppure una coppia di decisori cooperativi che scelgono $x_1$ e $x_2$ condividendo l’obiettivo di massimizzare la somma dei profitti.

\paragraph{Caso non cooperativo}
Nel caso realistico di due decisori non cooperativi con profitti $\pi_1(x_1;x_2)$ e $\pi_2(x_1;x_2)$, il decisore 1 vorrebbe risolvere:
\[
\max\ \pi_1(x_1;x_2) \tag{5}
\]
soggetto a
\[
x_1 \in S_1,
\]
mentre il decisore 2 vorrebbe risolvere:
\[
\max\ \pi_2(x_1;x_2) \tag{6}
\]
soggetto a
\[
x_2 \in S_2.
\]
Questi due problemi, scritti così, non hanno senso: quale valore di $x_2$ usare nel problema (5)? Quale valore di $x_1$ usare nel problema (6)? È necessario chiarire come i due decisori muovono.

\paragraph{Sequenziale vs. simultaneo}
\begin{enumerate}[label=\arabic*.]
    \item Una possibilità è che i decisori agiscano in modo sequenziale. Ad esempio, il decisore 1 sceglie $x_1 \in S_1$ prima che il decisore 2 scelga $x_2 \in S_2$. In tal caso, il decisore 1 è il leader e il decisore 2 è il follower. Nel prendere la decisione, il decisore 1 può cercare di anticipare la reazione del decisore 2 a ciascun valore possibile di $x_1$.
    \item Un’altra possibilità è che le due decisioni siano prese simultaneamente. In tal caso, servono strumenti concettuali per capire quali decisioni aspettarsi.
\end{enumerate}

\paragraph{Equilibrio e perdita di performance}
La teoria dei giochi mira a fornire una previsione sensata di una soluzione di equilibrio $(x_1^e;x_2^e)$, che dipende dalle ipotesi sulla struttura del gioco.
Qualunque equilibrio si ottenga, esso non può produrre un profitto totale maggiore di $\pi_{1+2}^*$, poiché necessariamente vale:
\[
\pi_{1+2}^e=\pi_1(x_1^e;x_2^e)+\pi_2(x_1^e;x_2^e)
\le
\pi_1(x_1^*;x_2^*)+\pi_2(x_1^*;x_2^*)=\pi_{1+2}^*.
\]
Se questa disuguaglianza fosse violata, $(x_1^*;x_2^*)$ non sarebbe soluzione ottima di (4).
Questo significa che, decentralizzando le decisioni, il sistema complessivo è verosimilmente incapace di raggiungere la performance ottima globale.

\subsection{Giochi con decisioni continue}

\paragraph{Equilibrio di Nash con azioni continue}
Nel seguito si considera l’equilibrio di Nash nel caso in cui ogni giocatore disponga di un continuo di azioni possibili.
Si analizza prima il comportamento di due imprese che competono in termini di quantità. Entrambe massimizzano il profitto, ma influenzano reciprocamente il prezzo di mercato tramite la quantità totale prodotta. Il prezzo è comune a entrambe, poiché si assume un prodotto perfettamente identico.
Questa competizione si chiama competizione di Cournot; il caso in cui le imprese competono sui prezzi si chiama competizione di Bertrand.
Nel caso di competizione sui prezzi, si distingue tra prodotti omogenei o differenziati.

\subsection{Competizione di Cournot}

\paragraph{Impostazione del modello}
Un gioco a mosse simultanee in cui le azioni sono quantità conduce all’equilibrio di Cournot--Nash.
Ciò può essere rilevante in mercati, come l’energia, dove le imprese scelgono quantità e i prezzi sono determinati tramite meccanismi d’asta.
Si considera un modello molto semplice in cui ciascuna impresa ha solo costo variabile:
\[
TC_i(q_i)=c_i q_i,\qquad i=1,2,
\]
dove $TC_i$ è il costo totale dell’impresa $i$, $c_i$ è il costo variabile, e $q_i$ è la quantità prodotta dall’impresa $i$.
La quantità totale sul mercato è $Q=q_1+q_2$ e influenza il prezzo secondo una funzione di domanda inversa lineare:
\[
P(Q)=a-bQ,\qquad a,b>0,\qquad a\ge c_i.
\]
Questo modello implica che tutta la produzione venga venduta sul mercato.
Il profitto dell’impresa $i$ è:
\[
\pi_i(q_1;q_2)=P(q_1+q_2)q_i-TC_i(q_i)
=
\bigl[a-b(q_1+q_2)\bigr]q_i-c_iq_i,\qquad i=1,2.
\]

\paragraph{Funzioni di reazione (best response)}
Assumendo decisioni simultanee, si può determinare l’equilibrio tramite le funzioni di best response $R_i(q_j)$.
La condizione di stazionarietà per l’impresa 1 è:
\[
\frac{\partial \pi_1(q_1;q_2)}{\partial q_1}
=
a-2bq_1-bq_2-c_1=0
\;\Rightarrow\;
R_1(q_2)=\frac{a-c_1}{2b}-\frac{1}{2}q_2. \tag{7}
\]
Analogamente, per l’impresa 2:
\[
R_2(q_1)=\frac{a-c_2}{2b}-\frac{1}{2}q_1. \tag{8}
\]

\paragraph{Sistema per l’equilibrio di Cournot}
Per trovare l’equilibrio si individua l’intersezione delle due funzioni di risposta, cioè si risolve:
\[
\begin{cases}
q_1^c = R_1(q_2^c)\\
q_2^c = R_2(q_1^c)
\end{cases}
\]
(dove il superscritto $c$ denota l’equilibrio di Cournot; le funzioni di risposta sono rette a pendenza negativa).
Si risolve quindi il sistema lineare:
\[
\begin{cases}
q_1^c = \dfrac{a-c_1}{2b}-\dfrac{1}{2}q_2^c\\[6pt]
q_2^c = \dfrac{a-c_2}{2b}-\dfrac{1}{2}q_1^c
\end{cases}
\]
ottenendo:
\[
q_1^c=\frac{a-2c_1+c_2}{3b},
\qquad
q_2^c=\frac{a-2c_2+c_1}{3b}. \tag{9}
\]

\paragraph{Prezzo di equilibrio e profitti}
Il prezzo di equilibrio risulta:
\[
p^c=\frac{a+c_1+c_2}{3}. \tag{10}
\]
Il profitto di ciascuna impresa è:
\[
\pi_i^c=(p^c-c_i)q_i^c
=
\left(\frac{a+c_i+c_j}{3}-c_i\right)
\left(\frac{a-2c_i+c_j}{3b}\right)
=
\frac{(a-2c_i+c_j)^2}{9b}
=
b\,(q_i^c)^2. \tag{11}
\]
Se un’impresa riduce il proprio costo, aumenta sia la quantità prodotta sia il profitto.
Se le imprese hanno la stessa tecnologia ($c_1=c_2$), si ottiene una soluzione simmetrica $q_1^c=q_2^c$, come atteso.

\subsection{Competizione di Bertrand}

\paragraph{Impostazione del problema}
Si considerino due imprese che competono sui prezzi per un bene omogeneo, sotto l’ipotesi semplice di domanda e costi lineari.
È necessario specificare la reazione dei consumatori ai prezzi.
Con prodotto omogeneo, è sensato assumere che i consumatori acquistino dal venditore più economico; se i prezzi sono uguali, si assume che il mercato sia diviso equamente tra le due imprese.
Non si considerano vincoli di capacità.

\paragraph{Quantità venduta in funzione dei prezzi}
La quantità venduta dall’impresa $i$ (con $i=1,2$ e $j=2,1$ per indicare il concorrente) è:
\[
q_i=
\begin{cases}
0 & \text{se } p_i \ge \alpha/\beta,\\
0 & \text{se } p_i > p_j,\\
\dfrac{\alpha-\beta p}{2} & \text{se } p_i=p_j \equiv p < \alpha/\beta,\\
\alpha-\beta p_i & \text{se } p_i < \min\{p_j,\alpha/\beta\}.
\end{cases} \tag{12}
\]
L’idea è che non ci sia domanda se il prezzo supera il prezzo limite $\alpha/\beta$ oppure se è strettamente maggiore di quello del concorrente.
Se i prezzi coincidono (e sono sotto il limite) la domanda viene divisa a metà; altrimenti l’impresa più economica cattura tutta la domanda.

\paragraph{Definizione di equilibrio di Bertrand--Nash}
Un equilibrio di Bertrand--Nash è una quadrupla di prezzi e quantità,
\[
(p_1^b,\;p_2^b,\;q_1^b,\;q_2^b),
\]
tale che:
\begin{enumerate}[label=\arabic*.]
    \item $p_1^b$ risolve $\max_{p_1}\ \pi_1(p_1;p_2^b)=(p_1-c_1)q_1$, per $p_2=p_2^b$;
    \item $p_2^b$ risolve $\max_{p_2}\ \pi_2(p_1^b;p_2)=(p_2-c_2)q_2$, per $p_1=p_1^b$;
    \item le quantità $q_1$ e $q_2$ sono date dall’equazione (12).
\end{enumerate}
Chiaramente, i prezzi non saranno fissati al di sotto dei costi marginali $c_1$ e $c_2$, rispettivamente.

\paragraph{Undercutting e (assenza di) equilibrio con costi diversi}
Il concetto chiave è l’undercutting.
Se $c_1<c_2$, l’impresa 1 può undercut l’impresa 2 fissando un prezzo strettamente minore di $p_2$.
In senso stretto non esiste equilibrio, poiché non c’è un massimo (ma solo un supremo).
In effetti, c’è una discontinuità (a differenza di Cournot): una diminuzione infinitesima del prezzo può generare un salto della domanda per un’impresa.

\paragraph{Equilibrio con costi marginali uguali}
Si può mostrare che esiste equilibrio quando il costo marginale è lo stesso $c$ per entrambe le imprese:
\[
p_1=p_2=c,
\qquad
q_1=q_2=\frac{\alpha-\beta c}{2}.
\]
Il risultato si dimostra facilmente per assurdo: per qualunque altra configurazione dei prezzi, un’impresa avrebbe incentivo a deviare.

\paragraph{Conseguenze e discreto passo di prezzo}
Il risultato netto è che può non esserci profitto per nessuna impresa.
Se i costi marginali sono diversi, per trovare un massimo (e un equilibrio) è necessario che i prezzi possano variare solo di un ammontare minimo $\varepsilon$ (ad esempio un centesimo di euro).
Se $c_2-c_1>\varepsilon$, allora l’impresa 1 può undercut impostando $p_1=p_2-\varepsilon$, e l’equilibrio risultante è:
\[
p_2=c_2,\qquad p_1=c_2-\varepsilon,\qquad
q_1=\alpha-\beta(c_2-\varepsilon),\qquad q_2=0.
\]

\paragraph{Osservazioni finali}
Lasciando da parte le tecnicalità, questa previsione è in parte in contrasto con i risultati empirici.
Nella pratica, la struttura dei costi non è semplicemente lineare, esistono vincoli di capacità e i beni non sono realmente omogenei.
Possono emergere ulteriori complicazioni, per cui non è detto che esista una sola impresa sul mercato.
Guardando il risultato in modo opposto, esso suggerisce che le imprese dovrebbero differenziare le proprie offerte e sfruttare l’eterogeneità dei consumatori.

\subsection{Parte 2 -- Competizione di prezzo con beni non omogenei: modello di Hotelling}

\paragraph{Beni differenziati e scelta del consumatore}
Per trattare beni non omogenei (differenziati), è necessario un modello di scelta del consumatore.
Si consideri un modello unidimensionale delle preferenze dei consumatori.
I beni prodotti da due imprese, $A$ e $B$, sono caratterizzati da una feature, misurata su una scala da $0$ a $L$.
Per semplicità, si trascurano i costi di produzione.
L’impresa $A$ produce un bene con feature al livello $a$, e l’impresa $B$ produce un bene con feature al livello $L-b$, a distanza $b$ dal livello massimo.
Questa rappresentazione geometrica è nota come \emph{Hotelling's linear street model}.
Ogni consumatore ha un livello preferito per la feature, rappresentato dalla sua posizione sulla ``strada''.
Si assuma che i consumatori siano uniformemente distribuiti sulla strada, dal livello $0$ al livello $L$.
Ogni consumatore ha una marca preferita, quella più vicina ai propri gusti.
Se i due prezzi $p_A$ e $p_B$ sono uguali, ciascun consumatore acquista semplicemente la marca più vicina.
Tuttavia, esiste un trade-off tra prezzo e soddisfazione.

\paragraph{Costo di trasporto e utilità}
Si immagini di esprimere la (dis)utilità di ciascun consumatore misurando un costo di ``trasporto'' $\tau$.
Per un consumatore in posizione $x$ sulla scala, l’utilità è:
\[
u(x)=
\begin{cases}
-p_A-\tau |x-a| & \text{se sceglie la marca } A,\\
-p_B-\tau |x-(L-b)| & \text{se sceglie la marca } B.
\end{cases}
\]
Si cerca un consumatore critico, in posizione $x_i$, tale che $a<x_i<L-b$, indifferente tra le due marche:
\[
-p_A-\tau(x_i-a)=-p_B-\tau(L-b-x_i),
\]
da cui segue:
\[
x_i=\frac{p_B-p_A}{2\tau}+\frac{L-b+a}{2}. \tag{13}
\]
Come controllo di coerenza, quando i prezzi sono uguali, $x_i$ è il punto medio tra le due posizioni delle marche.
L’impresa $A$ cattura tutta la domanda nell’intervallo $[0,x_i)$, quindi la sua domanda è $x_i$, mentre la domanda per la marca $B$ è:
\[
L-x_i=\frac{p_A-p_B}{2\tau}+\frac{L+b-a}{2}.
\]
Ci si può chiedere se esista un equilibrio di Bertrand--Nash nei prezzi, a posizioni fissate.

\paragraph{Problemi di massimizzazione dei profitti}
L’impresa $A$, dato $p_B$, risolve:
\[
\max_{p_A}\ \pi_A
=
\left(\frac{p_Bp_A-p_A^2}{2\tau}+\frac{L-b+a}{2}\right)p_A,
\]
mentre l’impresa $B$, dato $p_A$, risolve:
\[
\max_{p_B}\ \pi_B
=
\left(\frac{p_Ap_B-p_B^2}{2\tau}+\frac{L+b-a}{2}\right)p_B.
\]

\paragraph{Condizioni del primo ordine ed equilibrio nei prezzi}
Le condizioni di stazionarietà del primo ordine sono:
\[
\frac{\partial \pi_A}{\partial p_A}
=
\frac{p_B-2p_A}{2\tau}+\frac{L-b+a}{2}=0,
\qquad
\frac{\partial \pi_B}{\partial p_B}
=
\frac{p_A-2p_B}{2\tau}+\frac{L+b-a}{2}=0,
\]
che forniscono i prezzi di equilibrio:
\[
p_A^e=\frac{\tau(3L-b+a)}{3},
\qquad
p_B^e=\frac{\tau(3L+b-a)}{3}.
\]

\paragraph{Profitto dell’impresa A}
Il profitto per l’impresa $A$ è:
\[
\pi_A^e=x_i^e\,p_A^e=\frac{\tau(3L-b+a)^2}{18}.
\]

\paragraph{Caso simmetrico}
Nel caso simmetrico $a=b$ (che non implica che le posizioni coincidano), i prezzi sono uguali e il profitto si riduce a:
\[
\frac{\tau L^2}{2}
\]
per entrambe le imprese.
Si osserva che, maggiore è il costo di trasporto, maggiori sono i prezzi.
Ciò vale anche in contesti con switching costs.

\paragraph{Esistenza di prezzi di equilibrio positivi}
Non si deve dare per scontato che esista una coppia di prezzi di equilibrio strettamente positivi.
Si può mostrare che:
\begin{enumerate}[label=\arabic*.]
    \item se le marche sono omogenee (localizzate nello stesso punto), allora $p_A^e=p_B^e=0$ a causa dell’undercutting;
    \item se le marche non sono troppo vicine, allora valgono gli equilibri precedenti; altrimenti, non esiste equilibrio (si veda la discussione successiva);
    \item non esiste equilibrio nel gioco in cui sia le posizioni sia i prezzi sono variabili decisionali.
\end{enumerate}

\subsection{Esistenza di equilibrio nel modello di Hotelling}

\paragraph{Caso simmetrico e condizione su $a$}
Si consideri il caso simmetrico $a=b$.
Dalla discussione precedente si dovrebbe avere:
\[
p_A^e=p_B^e=\tau L,
\qquad
\pi_A^e=\pi_B^e=\frac{\tau L^2}{2}.
\]
In realtà, si può dimostrare che ciò vale solo per $a\le L/4$.

\paragraph{Prezzo limite per catturare tutto il mercato}
Si assuma $p_B=\tau L$, il prezzo di equilibrio, e si cambi $p_A$.
L’impresa $A$ cattura tutto il mercato se anche il consumatore in $L-b$ sceglie $A$.
Ciò accade se il prezzo $p_A$ più il costo di trasporto è minore di $p_B=\tau L$.
Osservando che la distanza tra $A$ e $B$ è $L-2a$, $A$ cattura il mercato se:
\[
p_A<\tau L-\tau(L-2a)=2a\tau=p_{A,\lim}.
\]
In questo caso, il profitto per $A$ è una funzione lineare crescente di $p_A$ e il suo valore limite è:
\[
\pi_A^{\triangle}=p_{A,\lim}L=2a\tau L.
\]
Per il prezzo limite c’è un’ambiguità, poiché i consumatori a destra di $B$ sono indifferenti; si può assumere che quel segmento di mercato sia diviso in parti uguali.
In ogni caso, esiste una discontinuità nel profitto di $A$ in funzione del prezzo.

\paragraph{Profitto quando il mercato è diviso}
Per $p_A>p_{A,\lim}$, il mercato si divide e, dalla (13), si ottiene:
\[
\pi_A=
\left[\frac{p_B-p_A}{2\tau}+\frac{L-b+a}{2}\right]p_A
=
\left[\frac{\tau L-p_A}{2\tau}+\frac{L}{2}\right]p_A
=
p_A L-\frac{p_A^2}{2\tau}.
\]
All’equilibrio, quando $p_A=\tau L$, si ottiene:
\[
\pi_A^e=\frac{\tau L^2}{2}.
\]
Poi, quando $p_A$ diventa abbastanza grande, l’impresa $B$ cattura tutto il mercato e $\pi_A$ scende a 0.

\paragraph{Condizione di esistenza dell’equilibrio}
Esiste un equilibrio (cioè esiste un massimo e non solo un supremo) se:
\[
\pi_A^e \ge \pi_A^{\triangle},
\]
ossia:
\[
\frac{\tau L^2}{2}\ge 2a\tau L
\;\Rightarrow\;
a\le \frac{L}{4}.
\]
Ciò significa che le imprese devono essere sufficientemente distanti per garantire l’esistenza di un equilibrio.

\subsection{Giochi simultanei vs. giochi sequenziali}

\paragraph{Motivazione dei giochi sequenziali}
Finora si è assunto che le due imprese competano simultaneamente e debbano prendere decisioni simili.
Talvolta è più naturale assumere che uno dei due giocatori muova per primo.
Ad esempio, un produttore può dover decidere il prezzo all’ingrosso, e un retailer la quantità acquistata (assumendo prezzo di vendita fissato).

\paragraph{Da Cournot a von Stackelberg}
Si consideri quindi che cosa accade nel gioco sulle quantità se si assume che l’impresa 1 (leader) scelga la quantità $q_1$ prima dell’impresa 2 (follower).
Diversamente dal gioco simultaneo, l’impresa 2 conosce la decisione di $1$ prima di decidere; quindi ha informazione perfetta.
L’analisi del gioco sequenziale porta all’equilibrio di von Stackelberg.

\paragraph{Problema del leader}
L’impresa 1 prende la decisione conoscendo la best response dell’impresa 2, data dall’equazione (8).
Il problema del leader è:
\[
\max_{q_1}\ \pi_1^s
=
P\bigl(q_1+R_2(q_1)\bigr)q_1-c_1q_1
=
\left[
a-b\left(
q_1+\frac{a-c_2}{2b}-\frac{q_1}{2}
\right)
\right]q_1-c_1q_1,
\]
dove il superscritto $s$ si riferisce alla competizione di von Stackelberg.

\paragraph{Quantità ottima del leader}
Applicando la condizione di stazionarietà si ottiene:
\[
q_1^s=\frac{a-2c_1+c_2}{2b}
=\frac{3}{2}\,q_1^c.
\]
L’impresa 1 produce di più nel gioco sequenziale rispetto a Cournot.

\paragraph{Quantità del follower}
Sostituendo in $R_2(q_1)$ si ottiene:
\[
q_2^s
=
\frac{a-c_2}{2b}-\frac{a-2c_2+c_1}{4b}
=
\frac{a-3c_2+2c_1}{4b}
=
\frac{3}{4}q_2^c+\frac{c_1-c_2}{4b}. \tag{15}
\]
La produzione dell’impresa 2 è una frazione di quella del gioco di Cournot, più un termine positivo se l’impresa 1 è meno efficiente dell’impresa 2.

\paragraph{Confronto dei profitti}
È interessante confrontare i profitti delle due imprese in questo gioco.
Ciò è semplice quando i costi marginali sono uguali; l’idea è illustrata con un esempio numerico.

\subsection{Esempio}

\paragraph{Dati}
Due imprese hanno lo stesso costo marginale, $c_1=c_2=5$, e il mercato è caratterizzato dalla funzione prezzo/quantità:
\[
P(Q)=120-Q.
\]
Si confrontano tre casi:
\begin{enumerate}[label=\arabic*.]
    \item le due imprese colludono e operano come un cartello (o come due rami di un monopolista);
    \item le imprese non cooperano e muovono simultaneamente (gioco di Cournot);
    \item le imprese non cooperano e muovono sequenzialmente (gioco di von Stackelberg).
\end{enumerate}

\paragraph{Caso 1: monopolio/cartello}
Nel primo caso si lavora con l’output aggregato $Q$. Il monopolista risolve:
\[
\max\ \pi^m=(120-Q)Q-5Q.
\]
Applicando la condizione di stazionarietà:
\[
120-2Q-5=0 \;\Rightarrow\; Q^m=57.50.
\]
Il prezzo e il profitto risultano:
\[
p^m=120-57.5=62.50,
\qquad
\pi_{1+2}^m=(62.50-5)\cdot 57.50=3306.25.
\]

\paragraph{Caso 2: Cournot (simultaneo)}
Nel secondo caso, la soluzione (9) è simmetrica:
\[
q_1^c=q_2^c=\frac{120-10+5}{3}=38.33.
\]
La produzione totale e il prezzo sono:
\[
Q^c=2\cdot 38.33=76.77,
\qquad
p^c=120-76.77=43.33.
\]
Il profitto di ciascuna impresa è:
\[
\pi_1^c=\pi_2^c=(q_1^c)^2=1469.19.
\]
Il profitto complessivo è:
\[
\pi_{1+2}^c=2\cdot 1469.19=2938.89 < 3306.25=\pi_{1+2}^m.
\]
Il monopolista restringe l’output per aumentare il prezzo, ottenendo un profitto totale maggiore rispetto alla competizione di Cournot.
La collusione produce quindi un profitto maggiore della competizione.

\paragraph{Caso 3: von Stackelberg (sequenziale)}
Nel gioco sequenziale, usando (14) e (15), si ottiene:
\[
q_1^s=\frac{120-10+5}{2}=57.5,
\qquad
q_2^s=\frac{120-10+5}{4}=28.75.
\]
Rispetto al gioco simultaneo, l’output dell’impresa 1 aumenta mentre quello dell’impresa 2 diminuisce.
La produzione totale e il prezzo sono:
\[
Q^s=57.5+28.75=86.25,
\qquad
p^s=120-86.25=33.75.
\]
Il prezzo è inferiore rispetto ai due casi precedenti e la distribuzione del profitto è asimmetrica:
\[
\pi_1^s=(33.75-5)\cdot 57.5=1653.13,
\]
\[
\pi_2^s=(33.75-5)\cdot 28.75=826.56,
\]
\[
\pi_{1+2}^s=1653.13+826.56=2479.69.
\]
Il profitto totale del gioco sequenziale è inferiore a quello del gioco simultaneo; tuttavia, il leader ha un vantaggio netto e il suo profitto è maggiore nel gioco sequenziale.

\subsection{Parte 2 -- Conviene sempre muovere per primi?}

\paragraph{Vantaggio del leader e limiti}
L’esempio giocattolo precedente mostra che il privilegio di muovere per primi può dare un vantaggio al leader.
Data la struttura del gioco, è facile vedere che il leader nel gioco sequenziale non può fare peggio che nel gioco simultaneo; infatti, potrebbe comunque produrre la stessa quantità del gioco di Cournot.
Tuttavia, ciò non vale in generale.
In particolare, quando esistono asimmetrie informative o elementi casuali, la scelta del leader, o il suo esito in presenza di incertezza, può fornire al follower informazione utile.
L’esempio seguente mostra che essere i primi a muovere non è sempre desiderabile.

\paragraph{Esempio: battle of the sexes (Juliet muove per prima)}
Si consideri il battle of the sexes, assumendo che Juliet abbia il privilegio di muovere per prima.
\[
\begin{array}{c|cc}
 & \text{Romeo: Horror} & \text{Romeo: Shopping} \\
\hline
\text{Juliet: Horror} & (1,3) & (0,0) \\
\text{Juliet: Shopping} & (0,0) & (3,1)
\end{array}
\]
Qualunque sia la scelta di Juliet, Romeo sceglierà la mossa che gli consente di godere della sua compagnia.
Di conseguenza, Juliet sceglierà certamente \emph{shopping} ed è certamente felice di muovere per prima.

\paragraph{Un caso diverso: Morticia}
La situazione è molto diversa per i payoff seguenti:
\[
\begin{array}{c|cc}
 & \text{Romeo: Cinema} & \text{Romeo: Restaurant} \\
\hline
\text{Morticia: Cinema} & (5,-100) & (0,1) \\
\text{Morticia: Restaurant} & (0,1) & (5,-100)
\end{array}
\]
In questo caso, Romeo è indifferente tra andare al cinema o al ristorante.
Ciò che teme davvero è una serata con Morticia.
È facile vedere che questo gioco non ha un equilibrio di Nash, poiché uno dei due giocatori ha sempre incentivo a deviare.
Un equilibrio può essere trovato ammettendo strategie miste, in cui i giocatori selezionano un’azione secondo una distribuzione di probabilità, legata all’incertezza sulla mossa del concorrente.
Qui non si considerano strategie miste, ma il punto importante è che, in questo caso, nessun giocatore vorrebbe muovere per primo.

\paragraph{Interpretazione economica dei due giochi}
Si è notato che la prima versione del battle of the sexes è un gioco di coordinamento stilizzato per due imprese che devono adottare uno standard comune; nella seconda versione, un’impresa vuole adottare lo stesso standard del concorrente, mentre l’altra impresa vorrebbe selezionarne uno differente.

\subsection{Pricing e double marginalization}

\paragraph{Impostazione B2B: produttore e retailer}
Come applicazione interessante al pricing, si consideri un contesto B2B, in cui un produttore e un retailer interagiscono tramite prezzi (si veda Tirole, 2003, Capitolo 4).
È interessante confrontare decisioni di prezzo e profitti complessivi della supply chain in due contesti:
\begin{enumerate}[label=\arabic*.]
    \item impresa verticalmente integrata, che gestisce produzione e distribuzione (un solo decisore prende tutte le decisioni);
    \item schema decentralizzato, in cui il produttore decide il prezzo all’ingrosso e il retailer decide il prezzo di mercato.
\end{enumerate}
In entrambi i casi si considera un mercato con funzione di domanda lineare:
\[
d(p)=1-p,
\]
dove $p$ è il prezzo di mercato, e si assume una struttura di costo lineare con costo marginale $c<1$.

\paragraph{Caso 1: impresa verticalmente integrata}
Il problema complessivo dell’impresa integrata è:
\[
\max_{p}\ (p-c)\,(1-p).
\]
Risolvendo, si ottengono prezzo ottimo, domanda e profitto:
\[
p_{vi}^*=\frac{1+c}{2},
\qquad
d_{vi}^*=\frac{1-c}{2},
\qquad
\pi_{vi}^*=\frac{(1-c)^2}{4},
\]
rispettivamente.

\paragraph{Caso 2: supply chain decentralizzata}
Se la catena non è integrata, ciascun attore imposta un prezzo.
Si assuma che il produttore sia il leader in un gioco sequenziale e fissi un prezzo all’ingrosso $p_w$.
Serve la funzione di risposta del retailer, che imposta il prezzo di mercato $p_m$ risolvendo, dato $p_w$:
\[
\max_{p_m}\ (p_m-p_w)\,(1-p_m).
\]
Questo produce la best response:
\[
R_m(p_w)=\frac{1+p_w}{2}.
\]

\paragraph{Problema del produttore e prezzo all’ingrosso di equilibrio}
Il problema del produttore diventa:
\[
\max_{p_w}\ (p_w-c)\left[1-\frac{1+p_w}{2}\right],
\]
che fornisce il prezzo di equilibrio:
\[
p_{w;dec}^*=\frac{1+c}{2}.
\]

\paragraph{Prezzo retail e domanda nel caso decentralizzato}
Il prezzo retail è maggiore che nel caso integrato (si ricordi $c<1$):
\[
p_{m;dec}^*=\frac{3+c}{4},
\]
e la domanda di mercato è più piccola:
\[
d_{dec}^*=\frac{1-c}{4}.
\]

\paragraph{Profitto complessivo nel caso decentralizzato}
Il profitto complessivo del controllo decentralizzato è la somma dei due profitti:
\[
\pi_{dec}^*=(p_{w;dec}^*-c)\,d_{dec}^*+(p_{m;dec}^*-p_{w;dec}^*)\,d_{dec}^*
=
\frac{(1-c)^2}{8}+\frac{(1-c)^2}{16}
=
\frac{3(1-c)^2}{16}
<
\pi_{vi}^*.
\]

\paragraph{Double marginalization e coordinamento della supply chain}
Questo tipo di problema è noto come \textbf{double marginalization} ed è dovuto al fatto che entrambi i giocatori applicano un markup al costo marginale che osservano, e tale costo percepito cresce lungo la catena.
In un contesto più realistico, si dovrebbe considerare anche l’incertezza della domanda, che introduce temi di risk sharing.
In tal caso, politiche di prezzo (two-part tariffs e buyback contracts) possono essere utilizzate per superare il problema e coordinare la supply chain.

\subsection{Parte 3 -- Modelli di scelta discreta}

\subsection{Conjoint analysis}

\paragraph{Idea di base e utilizzi}
La \emph{conjoint analysis} è un metodo ampiamente utilizzato nel marketing (e in altri campi) per progettare prodotti e servizi, valutare l’impatto del prezzo e costruire simulatori di mercato.
L’idea di base è che i consumatori reagiscono ad alternative rappresentate da combinazioni di valori degli attributi.
Esistono alcune varianti della conjoint analysis, ma le principali sono:
\begin{itemize}[label=-]
    \item \emph{ratings-based conjoint analysis} (quella tradizionale);
    \item \emph{choice-based conjoint analysis} (la più comune).
\end{itemize}

\paragraph{Modello a rating}
Il rating di preferenza $Y_i$ per l’alternativa $i$ può essere modellato come:
\[
Y_i=\sum_{k=1}^{m} U_k(x_{ik})+\varepsilon_i,
\]
dove ciascuna funzione $U_k$ cattura la \emph{partworth} per l’attributo $k$, ed $\varepsilon_i$ è un termine di errore.
Si noti che chiedere ai consumatori di dare priorità agli attributi è tipicamente inutile (``tutto è importante!''). Chiedere un rating $Y_i$ può essere una scelta migliore.

\paragraph{Limiti e interazioni}
La conjoint analysis classica inferisce le partworth sulla base dei rating forniti dai consumatori per un insieme di alternative (stimoli, profili, ecc.).
Il carattere additivo del modello è un chiaro limite, ma esistono modi per introdurre interazioni tra attributi, che possono essere rilevanti per valori specifici (ad esempio ``rosso'' e ``Ferrari'').

\paragraph{Dati di scelta: revealed vs. stated preferences}
Tuttavia, assegnare rating non è il modo in cui i consumatori prendono decisioni.
Per costruire un modello di preferenza migliore, si devono raccogliere dati da scelte effettive tra un insieme di alternative (inclusa l’opzione di non acquistare).
Si possono avere:
\begin{itemize}[label=-]
    \item \emph{revealed preference data}, dove si osservano scelte reali di mercato;
    \item \emph{stated preference data}, dove si analizza la reazione di un panel di consumatori a un insieme di stimoli accuratamente progettati (alternative ipotetiche).
\end{itemize}
La \emph{choice-based conjoint analysis} si basa su stated preferences, poiché offre grande flessibilità nella generazione degli stimoli e nella raccolta dati (al costo di una possibile perdita di realismo).

\paragraph{Design degli esperimenti e stima dei modelli}
Un ruolo chiave è svolto da un’attenta progettazione degli esperimenti (design ortogonali, ecc.), per massimizzare l’informazione raccolta senza imporre un carico cognitivo eccessivo all’utente (troppe alternative per scelta e troppe scelte).
Oltre alle questioni classiche di design of experiments, è importante:
\begin{itemize}[label=-]
    \item scegliere (ed eventualmente combinare) attributi e livelli;
    \item considerare proibizioni (combinazioni prive di senso);
    \item evitare profili incoerenti (prestazioni migliori a prezzo più basso);
    \item gestire con attenzione variabili ordinali.
\end{itemize}
Un modello tipico stimabile tramite CBC è un modello \emph{multinomial logit} (MNL).
Esistono versioni sofisticate di CBC, basate su modelli Bayesiani gerarchici, per tenere conto dell’eterogeneità dei consumatori.
Valutando le partworth utilities, si può prevedere la market share di beni non attualmente offerti, così come l’impatto delle decisioni di prezzo.

\subsection{Microfondazioni dei modelli MNL: utilità casuali}

\paragraph{Utilità rappresentativa e componente casuale}
Si assuma che il consumatore scelga uno tra $n$ beni, in base all’utilità di ciascuna alternativa.
L’utilità del bene $i\in[n]$ è $V_i+\varepsilon_i$, ossia somma di una componente deterministica e una componente casuale.
La componente deterministica è spesso detta \emph{representative utility}.
L’alternativa $i$ è scelta se:
\[
V_i+\varepsilon_i > V_j+\varepsilon_j,\qquad \forall j\neq i.
\]
Quindi, la probabilità di scelta dell’alternativa $i$ è:
\[
\pi_i=\mathbb{P}\{V_i+\varepsilon_i>V_j+\varepsilon_j,\ \forall j\neq i\}
=
\mathbb{P}\{\varepsilon_j<\varepsilon_i+V_i-V_j,\ \forall j\neq i\}.
\]

\paragraph{Probabilità condizionata e integrazione}
Assumendo indipendenza tra le componenti casuali, condizionatamente a $\varepsilon_i$, si ha:
\[
\pi_i\mid \varepsilon_i=\prod_{j\neq i} F_j(\varepsilon_i+V_i-V_j),
\]
dove $F_j$ è la DF di $\varepsilon_j$.
Quindi:
\[
\pi_i=\int_{-\infty}^{+\infty}\prod_{j\neq i} F_j(\varepsilon_i+V_i-V_j)\, f_i(s)\,ds,
\]
dove $f_i$ è la PDF di $\varepsilon_i$.

\paragraph{Caso i.i.d. Gumbel}
Si assuma ora che tutte le componenti casuali siano i.i.d. e seguano una distribuzione di Gumbel, con DF e PDF:
\[
F(x)=\exp\!\left(-e^{-x}\right),\qquad
f(x)=e^{-x}\exp\!\left(-e^{-x}\right).
\]
Allora:
\[
\pi_i=
\int_{-\infty}^{+\infty}
\prod_{j\neq i}
\exp\!\left(-e^{-(s+V_i-V_j)}\right)\;
e^{-s}\exp\!\left(-e^{-s}\right)\,ds.
\]
Per calcolare l’integrale, si osserva che $s+V_i-V_j=s$ per $j=i$, quindi si può includere il fattore $\exp(-e^{-s})$ nel prodotto e raccogliere i termini che non dipendono da $j$:
\[
\pi_i=
\int_{-\infty}^{+\infty}
\prod_{j=1}^{n}
\exp\!\left(-e^{-(s+V_i-V_j)}\right)\; e^{-s}\,ds
=
\int_{-\infty}^{+\infty}
\exp\!\left(-\sum_{j=1}^{n} e^{-(s+V_i-V_j)}\right)\; e^{-s}\,ds
\]
\[
=
\int_{-\infty}^{+\infty}
\exp\!\left(-e^{-s}\sum_{j=1}^{n} e^{-(V_i-V_j)}\right)\; e^{-s}\,ds.
\]
Si applica ora un cambio di variabile $t=e^{-s}$, così che $-e^{-s}ds=dt$ e i limiti inferiore e superiore diventano rispettivamente $+\infty$ e $0$.

\paragraph{Derivazione del modello MNL}
Correggendo per il cambio di segno:
\[
\pi_i=
\int_{0}^{1}
\exp\!\left(-t\sum_{j=1}^{n} e^{-(V_i-V_j)}\right)\,dt
=
\left.
\frac{-\exp\!\left(-t\sum_{j} e^{-(V_i-V_j)}\right)}
{\sum_{j} e^{-(V_i-V_j)}}
\right|_{0}^{1}
=
\frac{1}{\sum_{j} e^{-(V_i-V_j)}}
=
\frac{e^{V_i}}{\sum_{j} e^{V_j}},
\]
che corrisponde al modello MNL.

\paragraph{Assunzione IIA}
Il modello MNL è comune nel machine learning e nella classificazione ed è relativamente semplice da gestire in modelli di ottimizzazione (ad esempio decisioni di assortimento ottimo).
Tuttavia, ha limitazioni definite, in particolare l’assunzione IIA (\emph{Independence from Irrelevant Alternatives}).
Si considerino gli odds relativi di scegliere gli item $i$ e $k$:
\[
\frac{\pi_i}{\pi_k}
=
\frac{e^{V_i}/\sum_{j}e^{V_j}}{e^{V_k}/\sum_{j}e^{V_j}}
=
e^{V_i-V_k},
\]
che non dipendono da nessun’altra alternativa.
Quindi gli odds relativi non cambiano quando nuove alternative vengono aggiunte al choice set.

\paragraph{Esempio (red--blue bus problem)}
Si consideri una decisione sul modo di viaggio tra auto e bus (un bus blu), assumendo utilità rappresentative uguali, quindi $\pi_c=\pi_{bb}=1/2$.
Si introduca ora un bus rosso, per cui è ragionevole assumere $\pi_{rb}=\pi_{bb}$.
Con un modello MNL si ottiene $\pi_c=\pi_{rb}=\pi_{bb}=1/3$, mentre sarebbe più ragionevole assumere una suddivisione della probabilità di scegliere l’autobus, quindi $\pi_c=1/2$ e $\pi_{rb}=\pi_{bb}=1/4$.
I modelli \emph{nested logit} sono un possibile approccio per aggirare l’IIA, quando necessario.
Se si assume che la componente casuale dell’utilità sia normalmente distribuita, si ottiene un modello \emph{probit}.

\subsection{Supplemento tecnico -- EVT e massimi a blocchi}

\paragraph{Distribuzione dei massimi a blocchi}
Nella \emph{Extreme Value Theory} (EVT), tra le altre cose, si studia la distribuzione dei massimi a blocchi.
Si consideri un campione i.i.d. di taglia $n$ da una distribuzione con funzione di distribuzione $F_X(x)$.
Qual è la distribuzione di $M_n=\max\{X_1,\dots,X_n\}$? Che cosa accade quando $n\to\infty$?
Per indipendenza:
\[
F_{\max}(x)=\mathbb{P}\{M_n\le x\}
=
\mathbb{P}\{X_1\le x,\dots,X_n\le x\}
=
\mathbb{P}\{X_1\le x\}\cdots \mathbb{P}\{X_n\le x\}
=
F_X(x)^n.
\]

\paragraph{Esempio: uniforme}
Per $U\sim U(0,1)$:
\[
F_U(x)=x \ \Rightarrow\ F_{\max}(x)=x^n,\qquad x\in[0,1].
\]
Quando $n\to\infty$, la distribuzione collassa in una distribuzione degenere con massa di probabilità concentrata in $x=1$.
Inoltre, in questo caso:
\[
\mathbb{E}[M_n]=\frac{n}{n+1}.
\]
Un caso degenere come questo, dovuto al supporto limitato, non è particolarmente interessante.
Se si considera una variabile casuale con supporto illimitato verso l’alto, la distribuzione si sposta verso $+\infty$. Quindi?

\paragraph{Normalizzazione: confronto con il CLT}
Nel caso della somma $S_n=\sum_{k\in[n]}X_k$, rilevante per il teorema del limite centrale, si normalizza definendo sequenze $a_n$ e $b_n$:
\[
\lim_{n\to\infty}\mathbb{P}\left\{\frac{S_n-a_n}{b_n}\le x\right\}=\Phi(x),
\qquad
a_n=n\mathbb{E}(X_1),
\qquad
b_n=\sqrt{n\operatorname{Var}(X_1)}.
\]

\paragraph{Normalizzazione per i massimi}
Per ottenere una distribuzione limite per i massimi a blocchi, si introducono sequenze normalizzanti $d_n$ e $c_n$ e si verifica se:
\[
\lim_{n\to\infty}\mathbb{P}\left\{\frac{M_n-d_n}{c_n}\le x\right\}
=
\lim_{n\to\infty}F_X^n(c_nx+d_n)
=
H(x), \tag{16}
\]
per una funzione di distribuzione non degenere $H(x)$.

\paragraph{Esempio: esponenziale}
Si consideri una distribuzione esponenziale con rate $\beta>0$, per cui:
\[
F(x)=1-e^{-\beta x},\qquad x\ge 0.
\]
Scegliendo $c_n=1/\beta$ e $d_n=(\log n)/\beta$ si ottiene:
\[
F_X^n(c_nx+d_n)=\left(1-\frac{1}{n}e^{-x}\right)^n,\qquad x\ge -\log n,
\]
e:
\[
\lim_{n\to\infty}F_X^n(c_nx+d_n)=\exp\!\left(-e^{-x}\right).
\]

\paragraph{Definizione: distribuzione GEV}
La funzione di distribuzione della \emph{generalized extreme value} (GEV) standard è:
\[
H_\xi(x)=
\begin{cases}
\exp\!\left(-(1+\xi x)^{-1/\xi}\right), & \xi\neq 0,\\[4pt]
\exp\!\left(-e^{-x}\right), & \xi=0,
\end{cases}
\]
dove $1+\xi x>0$.
Introducendo un parametro di location $\mu\in\mathbb{R}$ e un parametro di scala $\sigma>0$, si ottiene la famiglia:
\[
H_{\xi;\mu;\sigma}(x):=
H_\xi\!\left(\frac{x-\mu}{\sigma}\right).
\]
Il parametro $\xi$ definisce la forma di una famiglia di distribuzioni simili (dello stesso tipo).
Si ricorda che quando due variabili casuali $V$ e $W$ differiscono in distribuzione per un parametro di scala $a>0$ e un parametro di location $b\in\mathbb{R}$, cioè $V \overset{d}{=} aW+b$, allora hanno lo stesso tipo (sono simili).

\paragraph{Interpretazione del parametro di forma}
La figura mostra densità per alcuni valori del parametro di forma (assumendo $\mu=0$, $\sigma=1$):
\begin{itemize}[label=-]
    \item se $\xi>0$, si ha una distribuzione di Fr\'echet;
    \item se $\xi=0$, si ha una distribuzione di Gumbel;
    \item se $\xi<0$, si ha una distribuzione di Weibull.
\end{itemize}
La distribuzione di Weibull ha un estremo destro finito.
La distribuzione di Fr\'echet presenta una decrescita più lenta della Gumbel.
Nota: la distribuzione di Gumbel, tra le altre cose, svolge un ruolo chiave nei modelli di scelta discreta.

\paragraph{Dominio di attrazione e teorema FTG}
Se la condizione (16) vale per una $H$ non degenere, si dice che la funzione di distribuzione $F$ appartiene al massimo dominio di attrazione di $H$: $F\in MDA(H)$.
Nel caso della distribuzione esponenziale, si ha $F\in MDA(H_0)$.

\paragraph{Teorema (Fisher--Tippett--Gnedenko)}
Se $F\in MDA(H)$ per una $H$ non degenere, allora $H$ deve essere una distribuzione GEV di tipo $H_\xi$ per qualche valore di $\xi$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PDF 7 %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CAP 7 -- Il principio della programmazione dinamica}

\subsection{Che cos'è la programmazione dinamica}

\paragraph{Definizione di programmazione dinamica}
La \textbf{programmazione dinamica (Dynamic Programming, DP)} non è un algoritmo come l’algoritmo del simplesso per la programmazione lineare. È piuttosto un \textbf{principio notevolmente generale e flessibile} che può essere utilizzato per progettare una vasta gamma di algoritmi di ottimizzazione.

\paragraph{Idea centrale}
L’idea centrale si basa sulla \textbf{decomposizione di un problema decisionale dinamico multistadio} in una \textbf{sequenza di problemi più semplici a singolo stadio}.

\paragraph{Problemi multistadio e multiperiodo}
Un problema decisionale multistadio non deve essere confuso con un problema multiperiodo. Si consideri un problema a orizzonte finito in cui si devono selezionare $T$ decisioni $x_t$, applicate in una sequenza di istanti temporali $t = 0,1,2,\ldots,T-1$. La domanda fondamentale è: \emph{quando vengono prese queste decisioni}?

\paragraph{Approccio open-loop e closed-loop}
In un problema statico multiperiodo, tutte le decisioni vengono prese al tempo $t=0$, come è naturale in un problema deterministico (\textbf{approccio open-loop}). In un problema stocastico, invece, si osserva nel tempo una traiettoria campionaria dei fattori di rischio rilevanti e si adattano le decisioni lungo il percorso.

\paragraph{Strategia decisionale}
Ciò che serve è una \textbf{strategia}, ovvero una \textbf{regola per prendere decisioni dopo aver osservato le realizzazioni casuali} (\textbf{approccio closed-loop}).

\subsection{Ambito di applicazione della programmazione dinamica}

\paragraph{Limiti della programmazione dinamica}
La programmazione dinamica non è un approccio universale, poiché la sua applicazione richiede una struttura specifica nel modello del sistema, che deve essere \textbf{markoviana}.

\paragraph{Classi di problemi trattabili}
Nonostante ciò, la programmazione dinamica può essere applicata a una vasta gamma di problemi:
\begin{itemize}[label=-]
\item problemi continui e discreti, dove la natura discreta o continua può riguardare:
\begin{itemize}[label=-]
\item variabili di stato;
\item variabili decisionali;
\item rappresentazione del tempo;
\end{itemize}
\item problemi deterministici e stocastici;
\item problemi a orizzonte finito e infinito.
\end{itemize}

\paragraph{Dimensione del problema}
All’interno di questa ampia collezione di problemi, può essere necessario risolvere problemi a dimensione finita o infinita. Per alcune strutture, la programmazione dinamica rappresenta \textbf{l’unico approccio risolutivo praticabile}.

\subsection{Problemi decisionali dinamici}

\paragraph{Modelli a tempo discreto}
Si considerano modelli a tempo discreto ed è necessario chiarire la distinzione tra \textbf{istanti temporali} e \textbf{intervalli di tempo}.

\paragraph{Decisioni e intervalli}
Le decisioni vengono prese agli istanti temporali, ma possono essere implementate lungo un intervallo di tempo, non necessariamente in modo istantaneo. Inoltre, le decisioni devono essere prese prima di osservare la realizzazione dei fattori di rischio, che si manifestano nell’intervallo temporale successivo.

\paragraph{Convenzione temporale}
Si adotta la seguente convenzione:
\begin{itemize}[label=-]
\item gli istanti temporali sono indicizzati da $t = 0,1,2,\ldots$; in tali istanti si osserva lo stato del sistema e si prende una decisione;
\item l’intervallo di tempo $t$ è l’intervallo tra gli istanti $t-1$ e $t$; dopo aver applicato la decisione presa all’istante $t-1$, il sistema evolve durante l’intervallo successivo e raggiunge un nuovo stato all’istante $t$;
\item durante tale intervallo può realizzarsi un input esterno, eventualmente casuale, che influenza la transizione di stato.
\end{itemize}

\subsection{Orizzonte temporale e dinamica del sistema}

\paragraph{Orizzonte finito e infinito}
Un problema può avere un orizzonte finito, definito sugli istanti $t=0,1,\ldots,T$, oppure un orizzonte infinito, definito sugli istanti $t=0,1,\ldots$. Il primo istante temporale è $t=0$, mentre il primo intervallo temporale è indicizzato da $t=1$. In un problema a orizzonte finito, vi sono $T$ intervalli di tempo e $T+1$ istanti temporali.

\paragraph{Equazione di transizione di stato}
La dinamica del sistema è rappresentata da un’equazione di transizione di stato del tipo:
\[
s_{t+1} = g_{t+1}(s_t, x_t, \xi_{t+1}),
\]
dove:
\begin{itemize}[label=-]
\item $g_{t+1}$ è la funzione di transizione sul periodo $t+1$;
\item $s_t$ è il vettore delle variabili di stato all’istante $t$, che riassumono tutta l’informazione rilevante del passato;
\item $s_0$ è lo stato iniziale, tipicamente noto, mentre $s_T$ è lo stato terminale;
\item $x_t$ è il vettore delle variabili decisionali all’istante $t$, dette anche variabili di controllo;
\item $\xi_{t+1}$ è un fattore esogeno che si realizza durante l’intervallo $t+1$.
\end{itemize}

\subsection{Variabili di stato e processi}

\paragraph{Tipologie di variabili di stato}
Si distinguono diverse tipologie di variabili di stato:
\begin{itemize}[label=-]
\item \textbf{variabili di stato fisiche}, che descrivono risorse fisiche o finanziarie e sono direttamente influenzate dalle decisioni;
\item \textbf{variabili di stato informative}, come il prezzo di un’azione finanziaria, non influenzate dall’azione di un singolo decisore;
\item \textbf{variabili di stato di credenza}, rilevanti in problemi con incertezza sui parametri del modello.
\end{itemize}

\paragraph{Processi decisionali e stocastici}
Si definiscono:
\begin{itemize}[label=-]
\item il processo dei dati $(\xi_t)_{t \ge 1}$;
\item il processo decisionale $(x_t)_{t \ge 0}$;
\item il processo di stato $(s_t)_{t \ge 0}$;
\end{itemize}
che possono essere deterministici o stocastici.

\subsection{Informazione e non anticipatività}

\paragraph{Disponibilità dell’informazione}
Nei problemi decisionali dinamici sotto incertezza, due aspetti sono cruciali:
\begin{enumerate}
\item come modellare la disponibilità dell’informazione;
\item come sono correlati i processi dei dati, dello stato e delle decisioni.
\end{enumerate}

\paragraph{Storia osservata}
Si introduce la notazione:
\[
x[t] := (x_0, x_1, \ldots, x_t), \qquad \xi[t] := (\xi_1, \xi_2, \ldots, \xi_t).
\]

\paragraph{Politiche non anticipative}
La decisione $x_t$ può dipendere solo dalle realizzazioni dei fattori di rischio fino all’istante $t$ incluso. Una politica decisionale implementabile è detta \textbf{non anticipativa}.

\subsection{Dipendenza temporale dei fattori di rischio}

\paragraph{Struttura del processo dei dati}
I vettori casuali del processo dei dati possono presentare diversi livelli di dipendenza:
\begin{itemize}[label=-]
\item indipendenza interstadio;
\item dipendenza markoviana;
\item dipendenza dall’intera storia osservata.
\end{itemize}

\paragraph{Endogeneità ed esogeneità}
È inoltre necessario specificare se la distribuzione dei fattori di rischio dipende dal tempo o dallo stato e dalle decisioni.

\paragraph{Obiettivo additivo}
L’obiettivo è ottimizzare una funzione obiettivo additiva nel tempo, pari alla somma dei costi o dei ricavi generati in ciascun istante.

\subsection{Problemi a orizzonte finito scontati}

\paragraph{Formulazione generale}
Un problema stocastico a orizzonte finito $T$ può essere formulato come:
\[
\operatorname{opt} \ \mathbb{E}_0 \left[
\sum_{t=0}^{T-1} \gamma^t f_t(s_t,x_t) + \gamma^T F_T(s_T)
\right],
\]
dove $\operatorname{opt}$ indica una minimizzazione o massimizzazione.

\paragraph{Interpretazione dei termini}
La funzione obiettivo include:
\begin{itemize}[label=-]
\item il contributo immediato $f_t(s_t,x_t)$;
\item il contributo terminale $F_T(s_T)$.
\end{itemize}

\paragraph{Fattore di sconto}
Il fattore di sconto $\gamma \in (0,1)$ è tipico nelle applicazioni finanziarie, anche se non è strettamente necessario in un problema a orizzonte finito.

\subsection{Contributi stocastici e apprendimento}

\paragraph{Contributo dipendente dal rischio}
Il contributo immediato può dipendere dalla realizzazione del fattore di rischio:
\[
h_t(s_t,x_t,\xi_{t+1}).
\]

\paragraph{Aspettazione condizionata}
In tal caso, si può definire:
\[
f_t(s_t,x_t) = \mathbb{E}_t \left[ h_t(s_t,x_t,\xi_{t+1}) \right].
\]

\paragraph{Difficoltà pratiche}
In pratica, il calcolo dell’aspettazione può essere difficile o impossibile. In tali casi, si dispone solo di campioni osservabili tramite simulazione Monte Carlo o sperimentazione online, come avviene nel \textbf{reinforcement learning}.

\subsection{Problemi a orizzonte infinito}

\paragraph{Problemi scontati a orizzonte infinito}
Un problema di programmazione dinamica scontato a orizzonte infinito ha la forma:
\[
\operatorname{opt}\ \mathbb{E}_0 \left[
\sum_{t=0}^{\infty} \gamma^t f(s_t,x_t)
\right],
\tag{6}
\]
dove la sommatoria è ben definita se tutti i contributi sono positivi e limitati e si utilizza un fattore di sconto appropriato $\gamma < 1$.

\paragraph{Ruolo del fattore di sconto}
La programmazione dinamica scontata è naturale nelle applicazioni di business ed economia. Talvolta il fattore di sconto non ha una reale giustificazione economica ed è utilizzato solo come espediente per rendere trattabile un problema a orizzonte infinito.

\paragraph{Criterio del costo medio}
In alcune applicazioni ingegneristiche si preferisce considerare una media nel tempo:
\[
\operatorname{opt}\ \lim_{T \to \infty} \mathbb{E}_0 \left[
\frac{1}{T} \sum_{t=0}^{T-1} f(s_t,x_t)
\right].
\tag{7}
\]
Questo conduce a formulazioni di programmazione dinamica basate sul contributo medio per stadio.

\paragraph{Orizzonte non definito}
Esistono anche problemi con orizzonte non definito, nei quali il processo decisionale termina quando viene raggiunto uno stato obiettivo.

\subsection{Politiche decisionali}

\paragraph{Vincolo di fattibilità delle decisioni}
Ad ogni istante temporale occorre selezionare una decisione $x_t$. Si può introdurre un vincolo astratto del tipo:
\[
x_t \in X(s_t),
\tag{8}
\]
che impone che la decisione al tempo $t$ appartenga a un insieme ammissibile $X$, eventualmente dipendente dallo stato corrente $s_t$.

\paragraph{Caso deterministico e stocastico}
In un contesto deterministico, questo vincolo sarebbe sufficiente, poiché al tempo $t=0$ si dovrebbe individuare una sequenza di decisioni $(x_t)_{t\ge 0}$ da implementare nel tempo. Nel caso stocastico, tuttavia, entra in gioco un vincolo fondamentale.

\paragraph{Non anticipatività}
Le decisioni devono essere \textbf{non anticipative}, cioè non possono dipendere da informazioni future non ancora osservate.

\paragraph{Politiche in feedback}
Una politica decisionale chiusa e non anticipativa è naturalmente espressa nella forma:
\[
x_t = \mu_t(s_t) \in X(s_t),
\tag{9}
\]
dove $\mu_t(\cdot)$ è una funzione che associa allo stato al tempo $t$ una decisione ammissibile.

\paragraph{Politica come sequenza di funzioni}
Una politica può essere vista come una sequenza di funzioni:
\[
\mu \equiv \bigl( \mu_0(\cdot), \mu_1(\cdot), \ldots, \mu_{T-1}(\cdot) \bigr),
\tag{10}
\]
una per ciascun istante temporale in cui è richiesta una decisione.

\paragraph{Politiche stazionarie}
Nel caso di problemi a orizzonte infinito, si cercano politiche \textbf{stazionarie}, caratterizzate dalla stessa funzione $\mu(\cdot)$ in ogni stadio.

\paragraph{Formulazione del problema in termini di politiche}
Una formulazione più precisa del problema a orizzonte finito è:
\[
\operatorname{opt}_{\mu \in \mathcal{M}}
\mathbb{E}_0 \left[
\sum_{t=0}^{T-1} \gamma^t f_t(s_t,\mu_t(s_t))
+ \gamma^T F_T(s_T)
\right],
\tag{11}
\]
dove $\mathcal{M}$ è l’insieme delle politiche ammissibili, tali che $x_t=\mu_t(s_t)\in X(s_t)$ per ogni istante temporale.

\paragraph{Politiche deterministiche e randomizzate}
Sono state introdotte politiche deterministiche. Esistono tuttavia casi in cui politiche randomizzate possono essere necessarie o utili.

\paragraph{Vincoli probabilistici sugli stati}
Le politiche randomizzate possono essere richieste per gestire vincoli probabilistici sugli stati:
\[
\mathbb{P}\{ s_t \in G \} \ge 1 - \alpha,
\]
dove $\alpha$ è una probabilità accettabile di violazione del vincolo.

\paragraph{Esplorazione e sfruttamento}
Le politiche randomizzate risultano inoltre necessarie nel contesto del trade-off tra esplorazione e sfruttamento nel reinforcement learning.

\subsection{Un esempio: il lot-sizing dinamico a singolo prodotto}

\paragraph{Formulazione del problema}
Si consideri il problema di lot-sizing non capacitated a singolo prodotto:
\[
\min \sum_{t=0}^{T-1} \bigl[ c x_t + \varphi \cdot \delta(x_t) \bigr]
+ \sum_{t=1}^{T} h I_t
\tag{12}
\]
soggetto a:
\[
I_{t+1} = I_t + x_t - d_{t+1}, \qquad t=0,1,\ldots,T-1,
\tag{13}
\]
\[
x_t, I_t \ge 0.
\]

\paragraph{Significato delle variabili}
Dove:
\begin{itemize}[label=-]
\item $x_t$ è la quantità ordinata e consegnata immediatamente agli istanti $t=0,1,\ldots,T-1$;
\item $I_t$ è il livello di inventario disponibile agli istanti $t=0,1,\ldots,T$;
\item $d_t$ è la domanda durante gli intervalli temporali $t=1,\ldots,T$.
\end{itemize}

\paragraph{Struttura dei costi}
L’obiettivo è soddisfare la domanda al costo minimo, che comprende:
\begin{itemize}[label=-]
\item un costo di mantenimento a magazzino, con tasso $h$ per unità e per unità di tempo;
\item un costo di ordinazione, composto da una parte variabile con coefficiente $c$ e da un costo fisso $\varphi$, sostenuto solo quando si ordina una quantità positiva.
\end{itemize}

\paragraph{Funzione indicatrice del costo fisso}
Per rappresentare il costo fisso si introduce la funzione:
\[
\delta(x) =
\begin{cases}
1 & \text{se } x>0,\\
0 & \text{se } x=0.
\end{cases}
\]

\paragraph{Variabili di stato e controllo}
Il livello di inventario $I_t$ è una variabile di stato, la quantità ordinata $x_t$ è una variabile di controllo, e la domanda $d_t$ può essere interpretata come input esogeno nel caso deterministico o come fattore di rischio nel caso stocastico.

\paragraph{Equazione di transizione}
L’equazione di stato è data da (13); $I_0$ è lo stato iniziale e $I_T$ è lo stato terminale alla fine dell’orizzonte di pianificazione.

\subsection{Domanda incerta e modelli alternativi}

\paragraph{Vendite perse}
Nel caso di vendite perse, l’equazione di transizione diventa:
\[
I_{t+1} = \max \{0, I_t + x_t - d_{t+1}\}.
\]
Si introduce inoltre una penalità $q$ per ogni unità di domanda insoddisfatta, aggiungendo il termine:
\[
\sum_{t=1}^{T} q \max \{0, d_t - (I_{t-1}+x_{t-1}) \}.
\]

\paragraph{Clienti pazienti e backlog}
Se i clienti sono pazienti, la domanda può essere accumulata come backlog, con una penalità $b$, con $b>h$.

\paragraph{Backlog con una variabile di stato}
Una prima opzione è mantenere una singola variabile di stato $I_t$, rilassandone la non negatività e interpretando valori negativi come backlog.

\paragraph{Backlog con due variabili di stato}
Una seconda opzione è introdurre due variabili di stato:
\begin{itemize}[label=-]
\item inventario disponibile $O_t \ge 0$;
\item backlog $B_t \ge 0$.
\end{itemize}
Il costo diventa:
\[
\sum_{t=1}^{T} (h O_t + b B_t),
\]
e le equazioni di transizione sono:
\[
O_{t+1} = \max \{0, O_t - B_t + x_t - d_{t+1}\},
\]
\[
B_{t+1} = \max \{0, -O_t + B_t - x_t + d_{t+1}\}.
\]

\paragraph{Ulteriori problematiche}
In applicazioni reali possono sorgere ulteriori difficoltà:
\begin{itemize}[label=-]
\item valutazione dello stato terminale;
\item stati parzialmente osservabili;
\item domanda censurata;
\item ritardi dovuti a lead time di consegna;
\item incertezza dipendente dallo stato e dalle decisioni.
\end{itemize}

\subsection{Uno sguardo al principio di programmazione dinamica: il problema del cammino minimo}

\paragraph{Rete diretta aciclica}
Si consideri un problema deterministico: il cammino minimo su una rete diretta aciclica. Una rete diretta è costituita da un insieme di nodi $N=\{0,1,2,\ldots,N\}$ e da un insieme di archi $A$.

\paragraph{Archi e costi}
Un arco è una coppia ordinata $(i,j)\in A$, con $i,j\in N$, ed è associato a un costo $c_{ij}>0$.

\paragraph{Successori e predecessori}
Per un nodo $i\in N$ si definiscono:
\[
S_i = \{ j\in N \mid (i,j)\in A \},
\qquad
B_i = \{ j\in N \mid (j,i)\in A \}.
\]

\paragraph{Cammini e lunghezza}
Un cammino diretto da $i$ a $j$ è una sequenza di nodi $(k_1,k_2,\ldots,k_m)$ con $k_1=i$, $k_m=j$, e archi consecutivi in $A$. La lunghezza del cammino è la somma dei costi sugli archi.

\paragraph{Interpretazione dinamica}
Ogni nodo è uno stato e ogni arco una transizione possibile. In ogni stato occorre scegliere una transizione in modo da raggiungere lo stato terminale al costo minimo.

\paragraph{Approccio greedy}
Una regola greedy consisterebbe nel muoversi verso il nodo più vicino:
\[
\min_{j\in S_i} c_{ij}.
\tag{14}
\]
Tale approccio non è in generale ottimale.

\paragraph{Valore degli stati}
Si introduce il valore $V_j$ associato a ciascun nodo successore, risolvendo in $i$ il problema:
\[
\min_{j\in S_i} (c_{ij}+V_j).
\]

\paragraph{Definizione del valore ottimo}
Sia $V_i$ la lunghezza del cammino minimo da $i$ al nodo terminale $N$:
\[
V_i := L(i^* \to N).
\]

\paragraph{Proprietà di annidamento}
Se $j$ appartiene a un cammino ottimo da $i$ a $N$, allora il sottocammino da $j$ a $N$ è ottimo.

\paragraph{Ricorsione di Bellman}
Si ottiene quindi l’equazione funzionale:
\[
V_i = \min_{j\in S_i} \{ c_{ij} + V_j \}, \qquad \forall i\in N,
\tag{17}
\]
con condizione terminale $V_N=0$.

\paragraph{Programmazione dinamica all’indietro}
L’equazione viene risolta etichettando i nodi a partire da quello terminale, seguendo un ordinamento topologico.

\paragraph{Funzione valore}
L’equazione (17) definisce una funzione valore $V(\cdot):N\to\mathbb{R}$ che associa a ciascuno stato il suo valore ottimo.

\subsection{Esempio numerico}

\paragraph{Condizione terminale e primi nodi etichettabili}
Si ha la condizione terminale $V_7 = 0$ per il nodo terminale, e si considerano i suoi predecessori immediati 4 e 6 (non si può ancora etichettare il nodo 5; si noti che i nodi non sono stati numerati usando un ordinamento topologico).
Si trova:
\[
V_4 = c_{47}+V_7 = 10+0 = 10;
\qquad
V_6 = c_{67}+V_7 = 8+0 = 8.
\]
Se si denota con $a_i^\star$ il nodo successore selezionato quando si è nel nodo $i\in N$ (l’azione ottima nello stato $i$), si ha banalmente:
\[
a_4^\star = 7;\qquad a_6^\star = 7.
\]

\paragraph{Etichettatura del nodo 5}
Ora si può etichettare il nodo 5:
\[
V_5 =
\min\left\{
\begin{array}{l}
c_{56}+V_6\\
c_{57}+V_7
\end{array}
\right\}
=
\min\left\{
\begin{array}{l}
1+8\\
5+0
\end{array}
\right\}
=5
\ \Rightarrow\ 
a_5^\star = 7.
\]

\paragraph{Etichettatura del nodo 3}
Si considera poi il nodo 3, i cui successori immediati 4, 5 e 6 sono già stati etichettati:
\[
V_3 =
\min\left\{
\begin{array}{l}
c_{34}+V_4\\
c_{35}+V_5\\
c_{36}+V_6
\end{array}
\right\}
=
\min\left\{
\begin{array}{l}
3+10\\
2+5\\
1+8
\end{array}
\right\}
=7
\ \Rightarrow\ 
a_3^\star = 5.
\]

\paragraph{Etichettatura dei nodi 1, 2 e 0}
Allo stesso modo, si ha:
\[
V_1 =
\min\left\{
\begin{array}{l}
c_{13}+V_3\\
c_{14}+V_4
\end{array}
\right\}
=
\min\left\{
\begin{array}{l}
2+7\\
1+10
\end{array}
\right\}
=9
\ \Rightarrow\ 
a_1^\star = 3;
\]
\[
V_2 =
\min\left\{
\begin{array}{l}
c_{23}+V_3\\
c_{25}+V_5
\end{array}
\right\}
=
\min\left\{
\begin{array}{l}
4+7\\
7+5
\end{array}
\right\}
=11
\ \Rightarrow\ 
a_2^\star = 3;
\]
\[
V_0 =
\min\left\{
\begin{array}{l}
c_{01}+V_1\\
c_{02}+V_2
\end{array}
\right\}
=
\min\left\{
\begin{array}{l}
7+9\\
6+11
\end{array}
\right\}
=16
\ \Rightarrow\ 
a_0^\star = 1.
\]

\paragraph{Ricostruzione del cammino minimo}
Le azioni ottime in ciascuno stato possono essere memorizzate in una tabella, e si può trovare facilmente il cammino minimo applicando la politica decisionale: $(0,1,3,5,7)$, con lunghezza totale pari a 16.

\subsection{Cammini minimi su reti strutturate}

\paragraph{Caso strutturato e interpretazione dinamica}
Il problema del cammino minimo considerato nella sezione precedente riguarda una rete non strutturata. Un caso più strutturato si verifica quando i nodi della rete corrispondono a stati di un sistema dinamico che evolve nel tempo.

\paragraph{Contributo terminale e nodo fittizio}
Se lo stato terminale $s_T$ è fissato, non ha senso associare un valore terminale ad esso [Fig. (a)]. Se lo stato terminale è libero, può invece avere senso associare un contributo terminale $F_T(s_T)$. In Fig. (b) si aggiunge uno strato temporale al grafo e si introduce un nodo terminale fittizio $\Omega$.

\subsection{Il principio di decomposizione della programmazione dinamica}

\paragraph{Decomposizione in problemi a singolo stadio}
Il problema del cammino minimo suggerisce che un problema decisionale multistadio può essere decomposto in una sequenza di problemi più semplici a singolo stadio.

\paragraph{Problema deterministico}
In un problema deterministico, si cerca una sequenza di vettori $(x_0,x_1,\ldots,x_{T-1})$ per ottimizzare una misura di prestazione:
\[
\operatorname{opt}\ H(x_0,x_1,\ldots,x_{T-1}; s_0,s_1,\ldots,s_T),
\]
soggetta a un insieme di vincoli su stati e decisioni.

\paragraph{Problema stocastico}
Nel caso stocastico, il problema è:
\[
\operatorname{opt}\ \mathbb{E}\!\left[
H(x_0,x_1,\ldots,x_{T-1}; s_0,s_1,\ldots,s_T)
\right],
\]
dove l’aspettazione è presa rispetto a una sequenza di variabili aleatorie $(\xi_1,\ldots,\xi_T)$.
Questa notazione nasconde la vera natura multistadio del problema, poiché ora si dovrebbero trovare una sequenza di funzioni.

\paragraph{Decisioni come funzioni della storia}
Infatti, a parte la decisione iniziale $x_0$ da prendere qui e ora, i vettori decisionali possono essere funzioni delle decisioni passate e delle realizzazioni osservate dei fattori di rischio:
\[
x_t=\mu_t\!\bigl(x[t-1],\xi[t]\bigr),\qquad t=1,\ldots,T-1.
\tag{18}
\]

\paragraph{Riduzione tramite stato}
Se si individua un insieme adatto di variabili di stato, si può semplificare (18) e cercare funzioni più semplici che mappano lo stato corrente nella decisione ottima, $x_t=\mu_t(s_t)$.

\paragraph{Additività della funzione obiettivo}
Oltre alla struttura markoviana della dinamica, una decomposizione “pulita” è possibile se si assume che la funzione obiettivo abbia forma additiva:
\[
\mathbb{E}_0\left[
\sum_{t=0}^{T-1}\gamma^t f_t(s_t,x_t)+\gamma^T F_T(s_T)
\right].
\]

\paragraph{Regola myopica}
Una regola decisionale rapida e approssimativa è: quando si è nello stato $s_t$, risolvere il problema miopico
\[
\operatorname{opt}_{x_t\in X(s_t)} f_t(s_t,x_t),
\]
dove $X(s_t)$ è l’insieme delle decisioni ammissibili nello stato $s_t$.
È già noto che tale approccio greedy non è atteso funzionare bene in generale: occorre bilanciare obiettivi di breve e lungo periodo introducendo una funzione valore $V_t(\cdot)$.

\paragraph{Significato della funzione valore}
L’idea fondamentale della programmazione dinamica è che si può trovare una funzione valore tale da ottenere la prestazione ottima.
Il valore $V_t(s)$ dovrebbe essere il costo/ricavo atteso ottenuto applicando una politica ottima dal tempo $t$ in avanti, partendo dallo stato $s$.

\subsection{Equazione di Bellman}

\paragraph{Problema a singolo stadio parametrizzato dallo stato}
Formalmente, nello stato $s_t$ al tempo $t$ si deve risolvere:
\[
\boxed{
V_t(s_t)=\operatorname{opt}_{x_t\in X(s_t)}
\left\{
f_t(s_t,x_t)+\gamma\,\mathbb{E}\!\left[
V_{t+1}\!\bigl(g_{t+1}(s_t,x_t,\xi_{t+1})\bigr)\,\middle|\, s_t,x_t
\right]
\right\}.
}
\tag{19}
\]
Questa equazione funzionale ricorsiva è detta \textbf{equazione di Bellman}.

\paragraph{Politica ottima in feedback}
La decisione ottima $x_t^\star$ si ottiene risolvendo un problema di ottimizzazione parametrizzato dallo stato corrente $s_t$, usando la conoscenza della funzione valore $V_{t+1}(\cdot)$.
Nel caso di uno spazio degli stati finito e piccolo, la politica ottima può essere rappresentata in forma tabellare. In generale, la politica è implicita nella sequenza delle funzioni valore.
In ogni caso, concettualmente si trova una politica ottima in forma di feedback:
\[
x_t^\star=\mu_t^\star(s_t)\in X(s_t).
\tag{20}
\]
Raccogliendo tutte le funzioni $\mu_t^\star(\cdot)$ si ottiene la politica complessiva:
\[
\mu^\star\equiv\bigl(\mu_0^\star(\cdot),\mu_1^\star(\cdot),\ldots,\mu_{T-1}^\star(\cdot)\bigr).
\tag{21}
\]
Quando si considera una singola funzione $\mu^\star(\cdot)$ indipendente dal tempo si parla di \textbf{politica stazionaria}, adatta ai problemi a orizzonte infinito.
Talvolta ci si accontenta di una politica subottima, ad esempio ottenuta da un’approssimazione della funzione valore ottima.

\subsection{Teorema: principio di ottimalità della programmazione dinamica}

\paragraph{Enunciato}
Si consideri una politica ottima $\bigl(\mu_0^\star(\cdot),\mu_1^\star(\cdot),\ldots,\mu_{T-1}^\star(\cdot)\bigr)$ per il problema multistadio
\[
\operatorname{opt}\ \mathbb{E}_0\left[
\sum_{t=0}^{T-1}\gamma^t f_t(s_t,x_t)+\gamma^T F_T(s_T)
\right].
\]
Si assuma che al tempo $\tau$ ci si trovi nello stato $s_\tau$ e si consideri il problema di coda:
\[
\operatorname{opt}\ \mathbb{E}_\tau\left[
\sum_{t=\tau}^{T-1}\gamma^{t-\tau} f_t(s_t,x_t)+\gamma^{T-\tau}F_T(s_T)
\right].
\]
Allora la politica troncata $\bigl(\mu_\tau^\star(\cdot),\mu_{\tau+1}^\star(\cdot),\ldots,\mu_{T-1}^\star(\cdot)\bigr)$ è ottima per il problema di coda.

\paragraph{Osservazione sulla dimostrazione}
Le dimostrazioni si basano sull’induzione matematica e possono essere piuttosto complicate quando si considerano questioni matematiche sottili.

\subsection{DP stocastica per orizzonti finiti}

\paragraph{Risoluzione all’indietro}
L’equazione (19) è un’equazione di ottimalità e richiede di determinare la funzione valore per ogni istante temporale.
Il processo naturale procede all’indietro nel tempo, partendo dalla condizione terminale:
\[
V_T(s_T)=F_T(s_T)\qquad \forall s_T,
\]
cioè il valore dello stato terminale è dato dalla funzione $F_T(\cdot)$.

\paragraph{Ultimo istante decisionale}
All’ultimo istante decisionale, $t=T-1$, si risolve per ogni possibile stato $s_{T-1}$:
\[
\boxed{
V_{T-1}(s_{T-1})=\operatorname{opt}_{x_{T-1}\in X(s_{T-1})}
\left\{
f_{T-1}(s_{T-1},x_{T-1})+
\gamma\,\mathbb{E}\!\left[
V_T\!\bigl(g_T(s_{T-1},x_{T-1},\xi_T)\bigr)\,\middle|\, s_{T-1},x_{T-1}
\right]
\right\}.
}
\tag{22}
\]
Questo è un problema statico ma non miopico, poiché $V_T(\cdot)$ incorpora l’effetto della decisione $x_{T-1}$ sullo stato terminale.

\paragraph{Ricorsione fino a $V_1$}
Risolvendo (22) per ogni stato $s_{T-1}$ si costruisce la funzione valore $V_{T-1}(\cdot)$.
Poi, svolgendo la ricorsione all’indietro nel tempo, si ottengono $V_{T-2}(s_{T-2})$ e così via, fino a $V_1(s_1)$.

\paragraph{Prima decisione}
Infine, dato lo stato iniziale $s_0$, la prima decisione ottima si trova risolvendo:
\[
\boxed{
V_0(s_0)=\operatorname{opt}_{x_0\in X(s_0)}
\left\{
f_0(s_0,x_0)+
\gamma\,\mathbb{E}\!\left[
V_1\!\bigl(g_1(s_0,x_0,\xi_1)\bigr)\,\middle|\, s_0,x_0
\right]
\right\}.
}
\tag{23}
\]

\paragraph{Uso delle funzioni valore: caso deterministico}
Come sfruttare la conoscenza delle funzioni valore $V_t(\cdot)$?
In un contesto deterministico, si può trovare la sequenza di decisioni ottime $x_t^\star$ risolvendo una sequenza di problemi a singolo stadio e aggiornando lo stato secondo le decisioni applicate.

\paragraph{Uso delle funzioni valore: simulazione Monte Carlo}
In un contesto stocastico, si può eseguire una simulazione Monte Carlo come segue:
\begin{itemize}[label=-]
\item dato lo stato iniziale $s_0$ e la funzione valore $V_1(\cdot)$, si risolve il problema del primo stadio e si trova $x_0^\star$;
\item si campiona $\xi_1$ e si genera lo stato successivo $s_1=g_1(s_0,x_0^\star,\xi_1)$;
\item dato lo stato $s_1$ e la funzione valore $V_2(\cdot)$, si risolve il problema del secondo stadio e si trova $x_2^\star$;
\item si ripete il processo fino a generare l’ultima decisione $x_{T-1}^\star$ e lo stato terminale $s_T$.
\end{itemize}

\subsection{DP stocastica per orizzonti infiniti}

\paragraph{Problema a orizzonte infinito}
La forma ricorsiva (19) deve essere adattata per un problema a orizzonte infinito del tipo:
\[
\operatorname{opt}\ \mathbb{E}\left[
\sum_{t=0}^{\infty}\gamma^t f(s_t,x_t)
\right],
\tag{24}
\]
dove si assume che i costi immediati siano limitati e $\gamma<1$, così che la serie converga a un valore finito.
Si elimina l’indice $t$ dal contributo immediato e dalla funzione di transizione:
\[
s_{t+1}=g(s_t,x_t,\xi_{t+1}).
\]

\paragraph{Equazione al punto fisso}
L’equazione funzionale diventa:
\[
\boxed{
V(s)=\operatorname{opt}_{x\in X(s)}
\left\{
f(s,x)+\gamma\,\mathbb{E}\!\left[
V\bigl(g(s,x,\xi)\bigr)
\right]
\right\},
}
\tag{25}
\]
dove $X(s)$ è l’insieme delle decisioni ammissibili nello stato $s$.
Ora la funzione valore è definita come \textbf{punto fisso} di un operatore potenzialmente complicato, poiché $V(s)$ compare su entrambi i lati dell’equazione.
È necessario un metodo iterativo per risolvere (25).




\section{CAP 8 -- Implementazione della programmazione dinamica}

\subsection{Allocazione discreta di risorse: il problema dello zaino}

\paragraph{Definizione del problema}
Il problema dello \textbf{zaino} richiede di selezionare un sottoinsieme di oggetti di \textbf{valore totale massimo}, rispettando un vincolo di \textbf{capacità} (budget).

Per modellare la selezione di ciascun oggetto si introducono variabili decisionali \textbf{binarie}:
\[
x_k =
\begin{cases}
1 & \text{se l’oggetto } k \text{ è selezionato},\\
0 & \text{altrimenti}.
\end{cases}
\]

Il problema può essere formulato come un problema di \textbf{programmazione lineare binaria pura}:
\[
\max \sum_{k=1}^{n} v_k x_k
\]
\[
\text{s.t. } \sum_{k=1}^{n} w_k x_k \le B
\]
\[
x_k \in \{0,1\} \quad \forall k.
\]

Si assume un’allocazione discreta del budget, poiché la selezione di un’attività è una decisione \textbf{tutto-o-niente}.

\paragraph{Motivazione dell’approccio DP}
Il problema può essere risolto in modo efficiente con algoritmi \textbf{branch-and-cut} per la programmazione lineare intera, ma qui si adotta un approccio basato sul principio della \textbf{programmazione dinamica}.

\subsection{Riformulazione sequenziale del problema}

\paragraph{Indice temporale fittizio e stadi decisionali}
Il problema non è intrinsecamente dinamico, ma può essere riformulato come un problema di \textbf{allocazione sequenziale di risorse} introducendo un indice discreto fittizio \(k\), associato agli oggetti.

Per ogni stadio \(k = 1, \dots, n\), si decide se includere o meno l’oggetto \(k\) nel sottoinsieme.

\paragraph{Stato e dipendenza dalle decisioni passate}
Allo stadio \(k=1\) si dispone del budget \(B\) e si deve scegliere se includere il primo oggetto, affrontando un compromesso tra \textbf{ricompensa immediata} e \textbf{ricompense future}.  
Allo stadio \(k=n\) il problema è banale, poiché resta da valutare solo l’ultimo oggetto dato il budget residuo.

La selezione degli oggetti successivi dipende dalle decisioni passate soltanto attraverso il \textbf{budget residuo}. La variabile di stato naturale allo stadio \(k\) è il budget disponibile \(s_k\) prima di selezionare l’oggetto \(k\), mentre la variabile decisionale è la variabile binaria \(x_k\).

\paragraph{Equazione di transizione dello stato}
Poiché non esiste l’oggetto \(k=0\), si usa la transizione:
\[
s_{k+1} = s_k - w_k x_k, \quad k = 1, \dots, n,
\]
con condizione iniziale
\[
s_1 = B.
\]
Assumendo che i pesi \(w_k\) siano \textbf{interi}, anche la variabile di stato assume valori \textbf{interi}.

\subsection{Funzione valore e ricorsione DP}

\paragraph{Definizione della funzione valore}
Si definisce la funzione valore:
\[
V_k(s) := \text{profitto della selezione ottimale degli oggetti } \{k, k+1, \dots, n\}\text{ con budget residuo } s.
\]
Nel caso discreto, è possibile \textbf{tabulare} tutte le funzioni \(V_k(s)\) per \(k=1,\dots,n\) e \(s\in\{0,\dots,B\}\).

\paragraph{Equazione di Bellman}
La ricorsione DP è:
\[
V_k(s) =
\begin{cases}
V_{k+1}(s) & \text{se } 0 \le s < w_k,\\[6pt]
\max\{ V_{k+1}(s),\; V_{k+1}(s - w_k) + v_k \} & \text{se } w_k \le s \le B.
\end{cases}
\]

\paragraph{Interpretazione della ricorsione}
Allo stadio \(k\), si considera l’oggetto \(k\).  
Se \(s < w_k\), l’oggetto non entra nel budget residuo e quindi la funzione valore resta \textbf{invariata}.  
Se \(s \ge w_k\), si confrontano due alternative: includere l’oggetto (ottenendo \(v_k\) e aggiornando lo stato a \(s-w_k\)) oppure escluderlo (lasciando lo stato a \(s\)).  
Poiché la decisione è \textbf{binaria}, l’ottimizzazione a singolo stadio è immediata.

\paragraph{Condizione terminale}
Per l’ultimo oggetto \(k=n\):
\[
V_n(s) =
\begin{cases}
0 & \text{se } 0 \le s < w_n,\\
v_n & \text{se } w_n \le s \le B.
\end{cases}
\]

\subsection{Implementazione MATLAB del problema dello zaino}

\paragraph{Obiettivo della funzione}
La funzione \texttt{DPKnapsack} ha lo scopo di calcolare la soluzione ottima del problema dello \textbf{zaino} dato un vettore di \textbf{valori}, un vettore di \textbf{pesi} e una \textbf{capacità}.  
L’output è la selezione ottima come vettore \textbf{binario} e il \textbf{valore totale} ottenuto.

\paragraph{Idea di memorizzazione: tabelle di valore e decisione}
La funzione valore viene memorizzata in una tabella \texttt{valueTable}, indicizzata per \textbf{stadio} (oggetto) e per \textbf{stato} (budget residuo).  
Poiché MATLAB indicizza a partire da \(1\), lo stato \(s\) corrisponde alla riga \(s+1\).

In parallelo, la tabella \texttt{decisionTable} memorizza, per ogni coppia \textbf{stato--stadio}, la \textbf{decisione ottima} (includere o non includere).

\paragraph{Ricorsione backward e ricostruzione forward}
La tabulazione implementa una ricorsione \textbf{backward} sugli stadi, calcolando le funzioni valore a ritroso.  
Successivamente, la soluzione ottima viene ricostruita procedendo \textbf{forward} sugli oggetti, usando la tabella delle decisioni e aggiornando lo stato (budget residuo) coerentemente.

\paragraph{Complessità computazionale}
Assumendo dati interi, la complessità è \(O(nB)\), quindi \textbf{pseudo-polinomiale}. Quando \(B\) è grande, occorre costruire una tabella molto grande e l’algoritmo può diventare poco efficiente.

\subsection{Allocazione continua del budget}

\paragraph{Formulazione del problema}
Si considera una versione \textbf{continua} del problema di allocazione delle risorse: un budget \(B\) deve essere allocato tra \(n\) attività, con decisioni continue \(x_k \ge 0\), \(k=1,\dots,n\).  
Il contributo di profitto dell’attività \(k\) dipende dall’allocazione tramite una funzione \(f_k(\cdot)\) \textbf{crescente e concava}:
\[
\max \sum_{k=1}^{n} f_k(x_k)
\]
\[
\text{s.t. } \sum_{k=1}^{n} x_k \le B,
\qquad
x_k \ge 0 \quad \forall k.
\]

\paragraph{Uso completo del budget}
Poiché le funzioni di profitto sono \textbf{strettamente crescenti}, si può assumere che il budget sia interamente utilizzato, quindi il vincolo di budget è soddisfatto come \textbf{uguaglianza} nella soluzione ottima.

\paragraph{Soluzione interna e Lagrangiana}
Assumendo una soluzione interna \(x_k^\ast>0\), il problema diventa un problema non lineare con un solo vincolo di uguaglianza.  
Introducendo il moltiplicatore di Lagrange \(\lambda\), la lagrangiana è:
\[
L(x,\lambda)=\sum_{k=1}^{n}\sqrt{x_k}+\lambda\left(\sum_{k=1}^{n}x_k-B\right).
\]
Le condizioni del primo ordine sono:
\[
\frac{1}{2\sqrt{x_k}}+\lambda=0,\quad k=1,\dots,n,
\]
\[
\sum_{k=1}^{n}x_k-B=0,
\]
che implicano un’allocazione \textbf{uniforme}:
\[
x_k^\ast=\frac{B}{n},\quad k=1,\dots,n.
\]
In questo caso, la funzione obiettivo è \textbf{concava} e le condizioni di ottimalità sono \textbf{necessarie e sufficienti}.

\subsection{Allocazione continua del budget: formulazione DP}

\paragraph{Riformulazione DP}
Il problema può essere riformulato in un quadro di \textbf{programmazione dinamica} introducendo un indice discreto fittizio \(k=1,\dots,n\) associato alle attività.  
Sia \(V_k(s)\) il profitto ottimale ottenibile allocando un budget residuo \(s\) alle attività \(\{k,k+1,\dots,n\}\).

\paragraph{Transizione dello stato}
La transizione dello stato è:
\[
s_{k+1}=s_k-x_k,
\]
con condizione iniziale \(s_1=B\).

\paragraph{Equazioni di ottimalità}
Le funzioni valore soddisfano:
\[
V_k(s_k)=\max_{0\le x_k\le s_k}\left\{f_k(x_k)+V_{k+1}(s_k-x_k)\right\},
\]
con condizione terminale:
\[
V_n(s_n)=\max_{0\le x_n\le s_n} f_n(x_n)=f_n(s_n).
\]
Il vincolo \(s_k\ge 0\) è garantito imponendo il vincolo sulla decisione \(0\le x_k\le s_k\), poiché la transizione è deterministica.

\paragraph{Funzioni valore in spazio infinito-dimensionale}
Nel caso continuo, la funzione valore \(V_k(s)\) è un oggetto in uno spazio funzionale \textbf{infinito-dimensionale}.  
Poiché la valutazione è possibile solo su un insieme finito di stati, occorre usare \textbf{approssimazione} o \textbf{interpolazione} per stimare i valori fuori dalla griglia.

\subsection{Intermezzo: interpolazione con spline cubiche in MATLAB}

\paragraph{Limiti dell’interpolazione polinomiale}
È noto che l’interpolazione polinomiale può soffrire di \textbf{oscillazioni inaccettabili}.  
Una funzione \textbf{concava} e \textbf{monotona crescente} può essere approssimata da una funzione \textbf{non monotona}, con effetti negativi sulle procedure di ottimizzazione.

\paragraph{Spline cubiche come alternativa}
Un’alternativa standard consiste nell’uso di funzioni polinomiali a tratti di ordine più basso, in cui ogni tratto è associato a un sottointervallo della griglia.  
Una scelta comune sono le \textbf{spline cubiche}, ottenute in MATLAB tramite \texttt{spline} (costruzione) e \texttt{ppval} (valutazione in punti arbitrari, anche fuori griglia).

\paragraph{Accuratezza, griglia e questioni aperte}
L’errore può essere significativo con griglie grossolane e migliora con griglie più fitte. Restano aperti temi come: la garanzia che una funzione monotona venga approssimata da una spline monotona, la scelta dei nodi per un buon compromesso tra costo computazionale ed errore, e la generalizzazione a dimensioni superiori.

\subsection{Risoluzione numerica del problema continuo tramite DP}

\paragraph{Idea della discretizzazione e interpolazione}
Si usa una \textbf{spline cubica} per approssimare la funzione valore del problema continuo.  
Si imposta una griglia uniforme su \([0,B]\), replicata per ciascuno stadio. La griglia contiene \(m+1\) valori di stato, con passo \(\delta s=B/m\), cioè stati del tipo \(j\,\delta s\), \(j=0,1,\dots,m\).

\paragraph{Risoluzione dei sottoproblemi e policy implicita}
Per ogni punto della griglia si risolve un sottoproblema del tipo:
\[
V_k(s_k)=\max_{0\le x_k\le s_k}\left\{f_k(x_k)+V_{k+1}(s_k-x_k)\right\},
\]
tramite ottimizzazione numerica.  
I valori fuori dalla griglia sono stimati via spline cubiche. La condizione al bordo su \(V_n(\cdot)\) è banale e si arresta il calcolo a \(V_2(\cdot)\) quando si vuole risolvere il problema per un budget specifico \(B\).  
La politica ottima \(x_t^\ast=\mu_t^\ast(s_t)\) non è memorizzata in una tabella esplicita, ma è \textbf{implicita} nella sequenza delle funzioni valore.

\subsection{Implementazione MATLAB: idea generale}

\paragraph{Scopo di \texttt{findPolicy}}
La funzione \texttt{findPolicy} serve a costruire, stadio per stadio, un’\textbf{approssimazione} delle funzioni valore tramite \textbf{spline cubiche}, usando una griglia uniforme del budget.  
Riceve il budget totale, una lista di funzioni di profitto (una per attività) e il numero di punti della griglia.  
Restituisce una lista di spline che rappresentano le funzioni valore approssimate; la prima funzione non viene usata direttamente quando l’interesse è risolvere il problema per un budget iniziale specifico.

\paragraph{Scopo di \texttt{applyPolicy}}
La funzione \texttt{applyPolicy} applica la politica ottima \textbf{in avanti nel tempo} utilizzando le funzioni valore approssimate: a ogni stadio sceglie l’allocazione che massimizza la somma tra contributo immediato e valore futuro, aggiorna il budget residuo e costruisce il vettore delle allocazioni ottime insieme al valore complessivo ottenuto.

\subsection{Controllo stocastico delle scorte}

\paragraph{Impostazione del problema}
Si considera una variazione \textbf{stocastica} del problema di lot-sizing, assumendo una \textbf{domanda aleatoria discreta}. In questo caso è naturale adottare una rappresentazione \textbf{tabellare} della funzione valore.

\paragraph{Dinamica dello stato con stockout (lost sales)}
Occorre specificare la dinamica dello stato in caso di stockout. Qui si assumono \textbf{vendite perse}:
\[
\boxed{
I_{t+1}=\max\{0,\; I_t + x_t - d_{t+1}\}
}
\]
dove \(\big(d_t\big)_{t=1,\dots,T}\) è una sequenza di variabili i.i.d., e \(x_t\) è la quantità ordinata al tempo \(t\) e consegnata immediatamente.

\paragraph{Sequenza degli eventi}
Al tempo \(t\) si osserva l’inventario a mano \(I_t\).  
Poi si decide l’ordine \(x_t\), che viene ricevuto immediatamente e porta la disponibilità a \(I_t+x_t\).  
Durante l’intervallo \(t+1\) si osserva la domanda aleatoria \(d_{t+1}\) e si aggiorna l’inventario secondo la dinamica sopra.

\paragraph{Spazio degli stati e azioni ammissibili}
Per tabulare la funzione valore occorre fissare un limite superiore allo stato. Si assume un vincolo \(I_t\le I_{\max}\), che implica:
\[
X(I_t)=\{0,1,\dots, I_{\max}-I_t\}.
\]

\paragraph{Costo immediato e stato terminale}
Il costo immediato include un costo lineare d’ordine \(c\,x_t\) e una penalità quadratica sull’inventario contabile dopo la domanda del periodo successivo:
\[
\boxed{
\phi\,(I_t+x_t-d_{t+1})^2.
}
\]
L’inventario fisico non può essere negativo ed è determinato dalla dinamica con \(\max\{0,\cdot\}\); un inventario contabile negativo rappresenta domanda non soddisfatta.  
Per semplicità, il costo terminale è nullo:
\[
F_T(I_T)=0.
\]

\paragraph{Ricorsione DP con costo immediato stocastico}
Il costo immediato dipende dalla realizzazione del fattore aleatorio nel periodo successivo alla decisione; quindi nella ricorsione compare un termine \textbf{stocastico}. La ricorsione è:
\[
\boxed{
V_t(I_t)=
\min_{x_t\in X(I_t)}
\mathbb{E}_{d_{t+1}}
\Big[
c\,x_t
+\phi\,(I_t+x_t-d_{t+1})^2
+V_{t+1}\big(\max\{0,\; I_t+x_t-d_{t+1}\}\big)
\Big]
}
\]
per \(t=0,1,\dots,T-1\) e \(I_t\in\{0,1,2,\dots,I_{\max}\}\).

\paragraph{Modellare l’incertezza}
Poiché la domanda è discreta e i.i.d., è sufficiente una \textbf{funzione di massa di probabilità}, cioè un vettore \(\pi_k\) per i possibili valori \(k=0,1,\dots,d_{\max}\).  
Occorre inoltre specificare lo stato iniziale \(I_0\) e l’orizzonte \(T\).

\subsection{Lot-sizing stocastico: idea delle procedure MATLAB}

\paragraph{Scopo di \texttt{MakePolicy}}
La funzione \texttt{MakePolicy} serve ad apprendere le funzioni valore \textbf{a ritroso} (\textbf{backward}) e la \textbf{politica ottima} in forma tabellare, dato un limite massimo di inventario, una distribuzione discreta della domanda e i parametri economici.  
L’output è una tabella dei valori (funzione valore per ogni stato e istante) e una tabella delle azioni (decisione ottima d’ordine per ogni stato e istante).

\paragraph{Scopo di \texttt{SimulatePolicy}}
La funzione \texttt{SimulatePolicy} serve a \textbf{simulare} l’applicazione della politica ottima su molti scenari di domanda generati casualmente, per ottenere una stima statistica del costo totale e confrontarla con il valore previsto dalla funzione valore.

\subsection{Sfruttare la struttura: cammini minimi per il lot-sizing deterministico}

\paragraph{Motivazione}
Nel problema toy di lot-sizing è semplice memorizzare funzioni valore e politica in una tabella, ma ciò è \textbf{inefficiente} quando i valori possibili di domanda sono molti, e diventa impossibile con spazio degli stati \textbf{continuo}.  
Talvolta è possibile semplificare drasticamente il problema sfruttando la \textbf{struttura}.

\paragraph{Lot-sizing deterministico con costi fissi e holding}
Si consideri una versione deterministica con soli \textbf{costi fissi d’ordine} \(\phi\) e \textbf{costi di giacenza} \(h\), con inventario iniziale e finale pari a zero, e domanda non nulla nel primo periodo.  
In linea di principio, una ricorsione DP è:
\[
\boxed{
V_t(I_t)=\min_{x_t \ge d_{t+1}-I_t}
\left\{
\phi\,\delta(x_t)
+h\,(I_t+x_t-d_{t+1})
+V_{t+1}(I_{t+1})
\right\},
\quad t=0,\dots,T-1
}
\]
con condizione:
\[
\boxed{
V_T(I_T)\equiv 0.
}
\]
Il vincolo su \(x_t\) garantisce che la domanda sia sempre soddisfatta e che lo stato non diventi negativo.

\paragraph{Teorema: proprietà di Wagner--Whitin}
Per il lot-sizing deterministico non capacitated con \textbf{costi fissi} e \textbf{costi lineari di inventario}, esiste una soluzione ottima tale che:
\[
\boxed{
I_t\,x_t=0,\quad t=0,1,\dots,T-1.
}
\]
Il messaggio è che non è mai ottimale ordinare quando l’inventario è positivo: si ordina solo se l’inventario è \textbf{nullo}.

\paragraph{Rete di flusso e interpretazione}
Considerando una rappresentazione come rete, deve valere il bilancio globale:
\[
\boxed{
\sum_{t=0}^{T-1} x_t = \sum_{t=1}^{T} d_t.
}
\]
Si introduce un nodo fittizio \(0\) il cui inflow rappresenta l’ammontare totale ordinato.  
I nodi \(t=1,\dots,T\) rappresentano gli istanti associati alla fine di ciascun intervallo, cioè la domanda si soddisfa alla fine dell’intervallo.

Il bilancio al nodo \(t\) corrisponde alla transizione:
\[
\boxed{
I_t = I_{t-1} + x_{t-1} - d_t.
}
\]
Se, contrariamente al teorema, \(I_{t-1}>0\) e \(x_{t-1}>0\), allora reindirizzando l’inflow orizzontale \(I_{t-1}\) lungo l’arco d’ordine associato a \(x_{t-1}\) si può migliorare il costo totale.

\paragraph{Conseguenza pratica: scelte d’ordine candidate}
All’istante \(t\) basta considerare solo:
\[
\boxed{
x_t \in \left\{
0,\;
d_{t+1},\;
(d_{t+1}+d_{t+2}),\;
(d_{t+1}+d_{t+2}+d_{t+3}),\;
\dots,\;
\sum_{\tau=t+1}^{T} d_{\tau}
\right\}.
}
\]

\paragraph{Riformulazione come cammino minimo}
Con questa proprietà, il problema a singolo item si riformula come un \textbf{cammino minimo} su una rete piccola: dal nodo iniziale \(0\) al nodo terminale, scegliendo archi che rappresentano quanti periodi vengono coperti dal prossimo ordine.  
Il costo di ciascun arco incorpora il \textbf{costo fisso d’ordine} e il \textbf{costo di giacenza} risultante.  
Poiché la rete ha un numero limitato di nodi, questa formulazione porta a un algoritmo molto efficiente di complessità \textbf{polinomiale}.

\subsection{Lot-sizing stocastico: politiche \(S\) e \((s,S)\)}

\paragraph{Backlog e penalità convessa}
La proprietà di Wagner--Whitin non vale nel caso stocastico, ma in alcuni casi esistono risultati strutturali.  
Si assume che non vi siano \textbf{vendite perse} e che sia consentito backlog, con costo totale che include:
\[
\boxed{
q(s)=h\max\{0,s\}+b\max\{0,-s\},
}
\]
dove \(s\) può essere positivo (inventario) o negativo (backlog), \(h\) è il costo di giacenza e \(b>h\) è il costo di backlog.  
La funzione \(q(\cdot)\) è \textbf{convessa} e tende a \(+\infty\) quando \(s\to \pm\infty\).

\paragraph{Costo variabile e obiettivo atteso}
Trascurando i costi fissi e includendo un costo variabile lineare unitario \(c\), l’obiettivo è minimizzare:
\[
\boxed{
\mathbb{E}_0\left[
\sum_{t=0}^{T-1}
\left(
c\,x_t + q(I_t+x_t-d_{t+1})
\right)
\right].
}
\]

\paragraph{Ricorsione DP e funzione \(H(\cdot)\)}
La ricorsione DP è:
\[
\boxed{
V_t(I_t)=\min_{x_t\ge 0}
\left\{
c\,x_t + H(I_t+x_t) + \mathbb{E}\!\left[V_{t+1}(I_t+x_t-d_{t+1})\right]
\right\},
}
\]
dove:
\[
\boxed{
H(y_t):=\mathbb{E}\!\left[q(y_t-d_{t+1})\right]
= h\,\mathbb{E}\!\left[\max\{0,y_t-d_{t+1}\}\right]
+b\,\mathbb{E}\!\left[\max\{0,d_{t+1}-y_t\}\right].
}
\]
Si introduce \(y_t\) come inventario disponibile dopo l’ordine (lead time nullo):
\[
y_t := I_t + x_t,
\]
con condizione terminale:
\[
\boxed{
V_T(I_T)=0.
}
\]
Si assume distribuzione della domanda costante nel tempo.

\paragraph{Riscrittura tramite \(G_t(\cdot)\)}
La ricorsione si riscrive come:
\[
\boxed{
V_t(I_t)=\min_{y_t \ge I_t} G_t(y_t) - c\,I_t,
}
\]
dove:
\[
\boxed{
G_t(y_t)=c\,y_t + H(y_t) + \mathbb{E}\!\left[V_{t+1}(y_t-d_{t+1})\right].
}
\]

\paragraph{Convessità e livello obiettivo \(S_t\)}
Si assume (senza prova) che \(V_t(\cdot)\) e \(G_t(\cdot)\) siano \textbf{convesse} per ogni \(t\), e che \(G_t(\cdot)\to +\infty\) quando \(y\to \pm\infty\).  
Ne segue che \(G_t(\cdot)\) ha un minimizzatore non vincolato finito:
\[
\boxed{
S_t = \arg\min_{y_t\in\mathbb{R}} G_t(y_t).
}
\]

\paragraph{Politica base-stock (order-up-to)}
La politica ottima è una politica \textbf{base-stock}:
\[
\boxed{
x_t^\ast=\mu_t^\ast(I_t)=
\begin{cases}
S_t-I_t, & \text{se } I_t<S_t,\\
0, & \text{se } I_t\ge S_t.
\end{cases}
}
\]
I valori \(S_t\) sono livelli obiettivo: si ordina quanto serve per raggiungere il target ottimo. In orizzonte finito, il problema è determinare la sequenza ottima di livelli \(S_t\).

\paragraph{Politiche \((s,S)\) con costi fissi}
Se si introducono costi fissi, si perde convessità. Tuttavia una proprietà correlata (K-convessità) porta a una politica ottima:
\[
\boxed{
\mu_t^\ast(I_t)=
\begin{cases}
S_t-I_t, & \text{se } I_t<s_t,\\
0, & \text{se } I_t\ge s_t,
\end{cases}
}
\]
dipendente da due sequenze \(s_t\) e \(S_t\), con \(s_t\le S_t\). In ambiente stazionario è ottima una politica stazionaria \((s,S)\).  
Si ordina solo quando l’inventario è sotto \textbf{small \(s\)}, riportandolo a \textbf{big \(S\)}; \(S-s\) è una quantità minima d’ordine che aiuta a controllare i costi fissi.

\subsection{Le maledizioni della programmazione dinamica}

\paragraph{Limiti della DP}
La \textbf{DP} è potente e flessibile, ma presenta limitazioni importanti.

\paragraph{Maledizione della dimensionalità dello stato}
Serve la funzione valore per ogni elemento dello spazio degli stati. Se lo spazio è finito e piccolo, si possono usare tabelle; per spazi enormi ciò non è fattibile.

\paragraph{Maledizione dell’ottimizzazione}
La DP decompone un problema multistadio in sottoproblemi a singolo stadio, ma anche questi sottoproblemi possono essere difficili da risolvere.

\paragraph{Maledizione dell’aspettativa}
Se i fattori di rischio \(\xi_t\) sono continui, l’aspettativa richiede integrali multidimensionali difficili; serve quindi una discretizzazione.

\paragraph{Maledizione della modellazione}
Il sistema può essere troppo complesso per avere un modello esplicito delle transizioni di stato. In DP il problema è più complicato perché le transizioni sono almeno in parte influenzate dalle decisioni di controllo.


\section{CAP 9 -- Modelli della programmazione dinamica}
\section{CAP 9 -- Modeling for Dynamic Programming}

\subsection{Frontespizio e riferimenti}

\paragraph{Titolo e autore}
\textbf{Business Analytics} (2020/21). \textbf{Modeling for dynamic programming}. \textbf{Prof.\ Paolo Brandimarte}, Dipartimento di Scienze Matematiche, Politecnico di Torino.

\paragraph{Riferimenti}
Le slide sono tratte dal libro: \textbf{P.\ Brandimarte}, \emph{From Shortest Paths to Reinforcement Learning: A MATLAB-Based Introduction to Dynamic Programming}, Springer, 2021.
Ulteriori riferimenti: P.\ Brandimarte, \emph{An Introduction to Financial Markets: A Quantitative Approach}, Wiley, 2018; J.Y.\ Campbell, L.M.\ Viceira, \emph{Strategic Asset Allocation}, Oxford University Press, 2002; W.B.\ Powell, \emph{Approximate Dynamic Programming: Solving the Curses of Dimensionality} (2nd ed.), Wiley, 2011; G.J.\ van Ryzin, K.T.\ Talluri, \emph{An Introduction to Revenue Management}, INFORMS Tutorials in OR, 2005.

\subsection{Principio di modellazione nella programmazione dinamica}

\paragraph{Idea generale della programmazione dinamica}
Il principio della \textbf{programmazione dinamica (DP)} è un concetto \textbf{flessibile} per la decomposizione di un \textbf{problema decisionale multistadio} in una sequenza di \textbf{problemi a singolo stadio}, bilanciando il \textbf{contributo immediato} e i \textbf{contributi attesi} delle decisioni future.

\paragraph{Funzione valore e ricorsione DP}
Il principio si fonda sulla \textbf{funzione valore}, definita dalla ricorsione di DP:
\[
\boxed{
V_t(s_t) = \operatorname{opt}_{x_t \in X(s_t)}
\left\{
f_t(s_t, x_t) + \gamma\, \mathbb{E}\!\left[\,V_{t+1}(s_{t+1}) \mid s_t, x_t \right]
\right\}.
}
\tag{1}
\]

\paragraph{Forme alternative della ricorsione DP}
L’Eq.\ (1) è solo \textbf{una possibile forma} della ricorsione di DP. Si possono adottare forme più specifiche, ad esempio quando i \textbf{fattori di rischio} sono \textbf{variabili aleatorie discrete} e l’aspettazione si riduce a una \textbf{somma}.

\paragraph{Scambio tra aspettazione e ottimizzazione}
In alcuni casi si adotta una riformulazione più radicale, in cui si \textbf{scambiano} gli operatori di \textbf{aspettazione} e \textbf{ottimizzazione}. Ciò può dipendere dalla \textbf{struttura informativa del problema} oppure essere il risultato di una manipolazione volta a evitare la soluzione di un \textbf{sottoproblema stocastico difficile}.

\paragraph{Q-learning e stati post-decisione}
Un caso ben noto è il \textbf{Q-learning}, una forma di \textbf{reinforcement learning} in cui \(V(s)\) è sostituita da \textbf{Q-factors} \(Q(s,x)\), che rappresentano il valore delle \textbf{coppie stato--azione}. Più in generale, lo scambio tra ottimizzazione e aspettazione può essere ottenuto introducendo il concetto di \textbf{stato post-decisione}.

\paragraph{Scelta della rappresentazione dello stato}
Trovare una descrizione adeguata dello \textbf{stato del sistema} può richiedere un’attenta modellazione: può essere necessaria una \textbf{ridefinizione dello spazio degli stati} per eliminare \textbf{dipendenze dal percorso} e ottenere un modello \textbf{markoviano aumentato ed equivalente}.

\paragraph{Natura degli stati e delle decisioni}
Sebbene sia naturale pensare a stati e decisioni come \textbf{scalari} o \textbf{vettori}, essi possono consistere in oggetti diversi, ad esempio \textbf{insiemi}.

\paragraph{Ruolo delle capacità di modellazione}
Le \textbf{capacità di modellazione} procedono di pari passo con la \textbf{conoscenza algoritmica} nell’affrontare un problema decisionale tramite DP; tali capacità si affinano solo con l’esperienza su una \textbf{vasta gamma di problemi}.

\subsection{Processi decisionali di Markov finiti}

\paragraph{Definizione di MDP}
Il termine \textbf{processo decisionale di Markov (MDP)} è riservato a problemi con \textbf{spazi di stato} e \textbf{spazi delle azioni discreti}. Il termine \textbf{azione} è spesso adottato per riferirsi alle decisioni di controllo. Stati e azioni discreti possono essere \textbf{enumerati} e associati a numeri interi.

\paragraph{MDP finiti e notazione degli stati}
Si considerano \textbf{MDP finiti}. Anche se gli stati possono corrispondere a vettori in uno spazio multidimensionale, si usa una notazione del tipo
\[
\boxed{
i, j \in S = \{1, \dots, N\},
}
\]
dove \(N\) è la \textbf{dimensione dello spazio degli stati}.

\paragraph{Azioni ammissibili}
Si usa \(a\) o \(a_t\) per indicare le azioni; \(A_t(i)\) o \(A(i)\) denota l’insieme delle \textbf{azioni ammissibili} nello stato \(i\) al tempo \(t\). L’insieme di tutte le azioni possibili è \(A\).

\paragraph{Dinamica markoviana controllata}
Il sistema sottostante è una \textbf{catena di Markov finita a tempo discreto}. Nei MDP le \textbf{probabilità di transizione} dipendono dall’azione selezionata:
\[
\boxed{
\pi_{t+1}(i,a,j)
}
\]
è la probabilità di transizione dallo stato \(i\) allo stato \(j\) durante l’intervallo \(t+1\), dopo aver scelto l’azione \(a\) al tempo \(t\).

\subsection{Esempio: controllo stocastico di inventario}
\paragraph{Sintesi dell’esempio}
Dall’esempio si evince come, con \textbf{spazio di stato finito} e \textbf{azioni ammissibili dipendenti dallo stato}, le \textbf{matrici di transizione} dipendano dall’azione e possano essere rappresentate in forma tabellare.

\subsection{Ricorsioni DP per MDP}

\paragraph{Ricorsione a orizzonte finito}
Nel contesto MDP, la funzione valore al tempo \(t\) è un vettore finito-dimensionale con componenti \(V_t(i)\) e soddisfa:
\[
\boxed{
V_t(i) = \operatorname{opt}_{a \in A(i)}
\left\{
f_t(i,a) + \gamma \sum_{j \in S}
\pi_{t+1}(i,a,j)\, V_{t+1}(j)
\right\}, \quad i \in S.
}
\tag{2}
\]

\paragraph{Caso a orizzonte infinito scontato}
Nel caso a \textbf{orizzonte infinito scontato}:
\[
\boxed{
V(i) = \operatorname{opt}_{a \in A(i)}
\left\{
f(i,a) + \gamma \sum_{j \in S}
\pi(i,a,j)\, V(j)
\right\}, \quad i \in S.
}
\tag{3}
\]

\paragraph{Contributo immediato stocastico}
Se il contributo immediato è stocastico e dipende dallo stato successivo, lo si denota \(h(i,a,j)\) e si riscrive:
\[
\boxed{
V(i) = \operatorname{opt}_{a \in A(i)}
\sum_{j \in S}
\pi(i,a,j)\,
\left\{
h(i,a,j) + \gamma V(j)
\right\}, \quad i \in S.
}
\tag{4}
\]

\paragraph{Relazione tra contributi}
In principio,
\[
\boxed{
f(i,a) = \sum_{j \in S} \pi(i,a,j)\, h(i,a,j),
}
\]
ma ciò può essere \textbf{impraticabile} quando \(N\) è finito ma enorme oppure quando le \textbf{probabilità di transizione non sono note}.

\subsection{Esempio: arresto ottimo ricorrente}

\paragraph{Sintesi dell’esempio}
Dall’esempio si evince il \textbf{trade-off} tra \textbf{ricompensa immediata} e \textbf{attesa} di opportunità migliori, e come la struttura della catena consenta una formulazione DP esplicita sui valori \(V(k)\).

\subsection{Valutazione delle politiche e Q-factors}

\paragraph{Policy evaluation e policy iteration}
Una politica stazionaria ammissibile \(\mu\) associa a ciascuno stato \(i\) un’azione \(a=\mu(i)\in A(i)\). La funzione valore associata a \(\mu\) si ottiene risolvendo un sistema lineare:
\[
\boxed{
V^\mu(i)= f\bigl(i,\mu(i)\bigr)+\gamma \sum_{j\in S}\pi\bigl(i,\mu(i),j\bigr)\,V^\mu(j),
\quad i\in S.
}
\tag{5}
\]
Questa relazione è fondamentale nei metodi basati su \textbf{policy iteration}: si valuta una politica candidata e poi si tenta di migliorarla.

\paragraph{Definizione di Q-factor per una politica}
Il \textbf{Q-factor} associato a \(\mu\) è la mappa \(S\times A \to \mathbb{R}\):
\[
\boxed{
Q^\mu(i,a) := f(i,a)+\gamma \sum_{j\in S}\pi(i,a,j)\,V^\mu(j).
}
\tag{6}
\]
L’idea è applicare \(a\) nello stato \(i\) e poi seguire la politica \(\mu\).

\paragraph{Q-factors ottimi e relazione con \(V\)}
Sostituendo la funzione valore ottima in (6) si ottengono i \textbf{Q-factors ottimi}. Vale:
\[
\boxed{
V(j)\equiv \operatorname{opt}_{a\in A(j)} Q(j,a), \quad j\in S.
}
\]

\paragraph{Ricorsione DP in termini di Q-factors}
La ricorsione di DP può essere riscritta come:
\[
\boxed{
Q(i,a)= f(i,a)+\gamma \sum_{j\in S}\pi(i,a,j)\left[\operatorname{opt}_{\tilde a\in A(j)} Q(j,\tilde a)\right],
\quad i\in S,\; a\in A(i).
}
\tag{8}
\]

\paragraph{Vantaggi e svantaggi}
Lo \textbf{svantaggio} è che si passa da \(V(i)\) a funzioni \textbf{stato--azione} \(Q(i,a)\), aggravando potenzialmente la \textbf{curse of dimensionality}. Il \textbf{vantaggio} è lo \textbf{scambio} tra \textbf{aspettazione} e \textbf{ottimizzazione}, utile anche perché i Q-factors possono essere \textbf{appresi tramite campionamento statistico} (DP \textbf{model-free}) e, se necessario, rappresentati con architetture di approssimazione dalla \textbf{regressione lineare} alle \textbf{reti neurali profonde}.

\subsection{Diverse forme di DP stocastica}

\paragraph{Ricorsione DP di base}
Si consideri nuovamente la ricorsione di DP:
\[
\boxed{
V_t(s_t) = \operatorname{opt}_{x_t \in X(s_t)}
\left\{
f_t(s_t, x_t) + \gamma \,\mathbb{E}\!\left[\,V_{t+1}(s_{t+1}) \mid s_t, x_t \right]
\right\}.
}
\]

\paragraph{Ipotesi sottostanti}
Si osserva lo \textbf{stato} \(s_t\) all’istante \(t\); si prende una \textbf{decisione ammissibile} \(x_t\in X(s_t)\); si osserva un \textbf{contributo immediato} \(f_t(s_t,x_t)\); si passa a un nuovo stato \(s_{t+1}\) che dipende dai \textbf{fattori di rischio realizzati} \(\xi_{t+1}\), secondo una distribuzione che può dipendere da \(s_t\) e \(x_t\).

\paragraph{Difficoltà e motivazione dello scambio}
Per determinare \(V_t(\cdot)\) a partire da \(V_{t+1}(\cdot)\) si dovrebbe risolvere un problema di \textbf{ottimizzazione stocastica} che può includere un’\textbf{aspettazione} impegnativa; nei MDP i \textbf{Q-factors} consentono di scambiare \textbf{ottimizzazione} e \textbf{aspettazione}, e talvolta lo scambio è richiesto dalla \textbf{struttura informativa}.

\subsection{Esempio: lot-sizing con lookahead limitato}

\paragraph{Sintesi dell’esempio}
Dall’esempio si evince che, se la domanda del periodo successivo è \textbf{nota} al tempo \(t\), la ricorsione naturale può assumere una forma in cui l’\textbf{ottimizzazione interna è deterministica} e l’\textbf{aspettazione è esterna}.

\subsection{Variabili di stato post-decisione}

\paragraph{Ricorsione in forma scambiata}
Una forma alternativa è:
\[
\boxed{
V_t(s_t)= \mathbb{E}_t\!\left[\operatorname{opt}_{x_t\in X(s_t)}\left\{f_t(x_t,s_t)+\gamma V_{t+1}(s_{t+1})\right\}\right].
}
\tag{9}
\]

\paragraph{Vantaggi potenziali}
Il problema di ottimizzazione interno è \textbf{deterministico} e l’aspettazione esterna può essere stimata tramite \textbf{campionamento statistico}.

\paragraph{Introduzione dello stato post-decisione}
Si introduce uno stato intermedio osservato dopo la decisione \(x_t\) ma prima della realizzazione di \(\xi_{t+1}\), detto \textbf{stato post-decisione} \(s_t^x\). Si scinde la transizione in due passi:
\[
\boxed{
s_t^x = g_t^{(1)}(s_t,x_t),
\qquad
s_{t+1}= g_{t+1}^{(2)}(s_t^x,\xi_{t+1}).
}
\tag{10--11}
\]

\paragraph{Relazione tra funzioni valore}
Si definisce il valore del post-decision state \(V_t^x(s_t^x)\) e vale:
\[
\boxed{
V_t^x(s_t^x)= \mathbb{E}\!\left[\,V_{t+1}(s_{t+1}) \mid s_t^x\right].
}
\tag{12}
\]
Quindi la ricorsione standard può essere riscritta come ottimizzazione deterministica:
\[
\boxed{
V_t(s_t)= \operatorname{opt}_{x_t\in X(s_t)}\left\{f_t(s_t,x_t)+\gamma V_t^x(s_t^x)\right\}.
}
\tag{13}
\]

\paragraph{Passo all’indietro e nuovo scambio}
Scrivendo la relazione un passo indietro e sostituendo (13), si ottiene una ricorsione che presenta nuovamente lo \textbf{scambio} tra \textbf{aspettazione} e \textbf{ottimizzazione}:
\[
\boxed{
V_{t-1}^x(s_{t-1}^x)
=
\mathbb{E}\!\left[
\operatorname{opt}_{x_t\in X(s_t)}
\left(
f_t(s_t,x_t)+\gamma V_t^x(s_t^x)
\right)
\Bigm| s_{t-1}^x
\right].
}
\tag{14}
\]

\paragraph{Connessione con i Q-factors}
La coppia \textbf{stato--azione} \((i,a)\) può essere interpretata come \textbf{stato post-decisione} prima di osservare la transizione casuale al nuovo stato; la formulazione in termini di \textbf{Q-factors} si inserisce naturalmente in questo quadro.

\subsection{Aumento dello stato nella gestione delle scorte}

\paragraph{Inventario on-hand vs inventario disponibile}
L’\textbf{inventario on-hand} appare naturale come variabile di stato, ma le decisioni d’ordine dovrebbero basarsi sull’\textbf{inventario disponibile}, che include \textbf{on-order} e \textbf{backlog}.

\paragraph{Limite dell’equazione di stato standard}
L’equazione
\[
\boxed{
I_{t+1}= I_t + x_t - d_{t+1}
}
\]
assume disponibilità immediata di quanto ordinato al tempo \(t\).

\paragraph{Lead time e variabili di pipeline}
Se il \textbf{lead time} è un intero \(LT\ge 1\), si introducono variabili \(z_{t,\tau}\) (pipeline), con \(\tau=0,1,\dots,LT-1\). Vale:
\[
\boxed{
I_{t+1}= I_t + z_{t,0} - d_{t+1}
}
\]
(trascurando l’incertezza) e la decisione \(x_t\) entra come:
\[
\boxed{
z_{t+1,LT-1}= x_t.
}
\]
Per \(\tau<LT-1\), le transizioni sono uno shift temporale:
\[
\boxed{
z_{t+1,\tau}= z_{t,\tau+1},
\qquad \tau=0,1,\dots,LT-2.
}
\tag{15}
\]

\paragraph{Items deperibili e stato per età}
Per prodotti \textbf{deperibili} si introduce un array \(I_{t,\tau}\) che descrive l’inventario per età; le transizioni dipendono dalla politica di issuing \textbf{FIFO} o \textbf{LIFO}.

\subsection{Revenue management}

\paragraph{Definizione e obiettivo}
Il \textbf{revenue management} consiste in modelli e tecniche per massimizzare il \textbf{ricavo} ottenuto vendendo \textbf{risorse deperibili}; l’idea nasce (come \textbf{yield management}) nell’industria aerea.

\paragraph{Approcci quantity-based e price-based}
Esistono due approcci: \textbf{quantity-based} (si limita la disponibilità della risorsa per massimizzare il ricavo) e \textbf{price-based} (si aggiustano dinamicamente i prezzi, \textbf{dynamic pricing}). Si considerano modelli DP quantity-based assumendo costo marginale nullo o trascurabile, così che massimizzare il profitto equivale a massimizzare il ricavo.

\paragraph{Classi tariffarie e prezzi}
Si considerino \(C\) unità di risorsa da allocare a \(n\) classi \(j=1,\dots,n\), vendute a prezzi \(p_j\) con:
\[
\boxed{
p_1 > p_2 > \cdots > p_n.
}
\]
I posti sono fisicamente \textbf{identici}, ma differenziati tramite \textbf{ancillaries} (cancellazione, vincoli, pasti, ecc.).

\paragraph{Caratteristiche essenziali}
La domanda \(D_j\) è casuale e può realizzarsi con schemi diversi. Due aspetti sono essenziali: \textbf{comportamento dei clienti} (segmentazione perfetta vs preferenze da modellare con un \textbf{choice model}) e \textbf{timing della domanda} (sequentialità auspicabile; caso peggiore quando arrivano prima i clienti low-budget).

\paragraph{Modelli statici e dinamici; protection levels e bid-prices}
Un modello con intervalli disgiunti, ciascuno dedicato a una sola classe, è \textbf{statico}; se le richieste sono interleaved nel tempo è \textbf{dinamico}. La policy si esprime tramite \textbf{protection levels} oppure \textbf{bid-prices} (prezzo minimo accettabile per vendere un posto).

\subsection{Modello statico con segmentazione perfetta della domanda}

\paragraph{Assunzioni, stadi e stato}
Le domande sono indipendenti e avvengono sequenzialmente (prima \(D_n\)); nessun cliente cambia classe. Gli stadi decisionali sono \(j=n,n-1,\dots,1\). La variabile di stato è la \textbf{capacità residua} \(s_j\), con stato iniziale \(s_n=C\). Condizione al contorno:
\[
\boxed{
V_0(s_0)=0,\qquad s_0=0,1,\dots,C.
}
\]

\paragraph{Ricorsione DP in forma scambiata}
La ricorsione assume la forma scambiata:
\[
\boxed{
V_j(s_j)=
\mathbb{E}\!\left[
\max_{0\le x_j \le \min\{s_j,D_j\}}
\left(
p_j x_j + V_{j-1}(s_j-x_j)
\right)
\right].
}
\]
Con uno shift di indice e notazione compatta:
\[
\boxed{
V_{j+1}(s)=
\mathbb{E}\!\left[
\max_{0\le x \le \min\{s,D_{j+1}\}}
\left(
p_{j+1}x + V_j(s-x)
\right)
\right].
}
\tag{16}
\]

\subsection{Valore marginale atteso e livelli di protezione}

\paragraph{Valore marginale atteso della capacità}
Si definisce:
\[
\boxed{
\Delta V_j(s):= V_j(s)-V_j(s-1),
}
\]
che misura il \textbf{costo opportunità} di un posto dato \(s\).

\paragraph{Somma telescopica e riscrittura}
Si ottiene:
\[
\boxed{
V_{j+1}(s)
=
V_j(s)
+
\mathbb{E}\!\left[
\max_{0\le x \le \min\{s,D_{j+1}\}}
\left(
\sum_{z=1}^{x}\bigl(p_{j+1}-\Delta V_j(s+1-z)\bigr)
\right)
\right].
}
\]
La riscrittura usa una \textbf{somma telescopica}:
\[
\boxed{
V_j(s-x)= V_j(s)-\sum_{z=1}^{x}\Delta V_j(s+1-z).
}
\]

\paragraph{Proprietà dei valori marginali}
Si possono provare:
\[
\boxed{
\Delta V_j(s+1)\le \Delta V_j(s),
\qquad
\Delta V_{j+1}(s)\ge \Delta V_j(s),
\qquad \forall s,j.
}
\]

\paragraph{Regola di accettazione e protezione annidata}
I termini \(p_{j+1}-\Delta V_j(s+1-z)\) sono decrescenti in \(z\): si aumenta \(x\) finché compare il primo termine negativo o si raggiunge \(\min\{s,D_{j+1}\}\). Si ottiene un livello di protezione \textbf{annidato}:
\[
\boxed{
y_j^\ast:= \max\{\,y:\; p_{j+1}<\Delta V_j(y)\,\},
\qquad j=1,\dots,n-1.
}
\tag{18}
\]
La decisione ottima allo stadio \(j+1\) è:
\[
\boxed{
x_{j+1}^\ast(s_{j+1},D_{j+1})
=
\min\{(s_{j+1}-y_j^\ast)^+,\; D_{j+1}\}.
}
\]
Lo scambio tra aspettazione e ottimizzazione è giustificato dal fatto che non serve conoscere \(D_{j+1}\) in anticipo: si accetta finché si raggiunge la protezione o si esaurisce la capacità o termina lo stadio.

\subsection{Modello dinamico con segmentazione perfetta della domanda}

\paragraph{Assunzioni e probabilità di arrivo}
Si rilassa la sequenzialità della domanda, mantenendo una segmentazione rigida. Il tempo è discretizzato in \(t=1,\dots,T\) assumendo \textbf{al più un arrivo per intervallo}. Sia \(\lambda_j(t)\) la probabilità di arrivo della classe \(j\) nell’intervallo \(t\), con vincolo:
\[
\boxed{
\sum_{j=1}^{n}\lambda_j(t)\le 1,
\qquad \forall t.
}
\]

\paragraph{Funzione valore e condizioni al contorno}
Si determinano \(V_t(s)\) (capacità residua \(s\)) con:
\[
\boxed{
V_t(0)=0,\quad t=1,\dots,T,
\qquad
V_{T+1}(s)=0,\quad s=0,1,\dots,C.
}
\]

\paragraph{Ricavo disponibile e decisione binaria}
Sia \(R(t)\) il \textbf{ricavo disponibile} al tempo \(t\): vale \(p_j\) se arriva un cliente di classe \(j\), \(0\) altrimenti. Alla richiesta si decide con \(x\in\{0,1\}\) se accettare o meno.

\paragraph{Ricorsione DP in forma scambiata}
La ricorsione assume la forma scambiata:
\[
\boxed{
V_t(s)=
\mathbb{E}\!\left[
\max_{x\in\{0,1\}}
\left\{
R(t)\,x+V_{t+1}(s-x)
\right\}
\right].
}
\]
Usando i \textbf{valori marginali attesi} si ottiene una politica in termini di \textbf{protection levels}, qui potenzialmente \textbf{time-varying}.

\subsection{Modello dinamico con scelta del cliente}

\paragraph{Modello di scelta}
Se la segmentazione perfetta viene rilassata, occorre un \textbf{choice model}. La decisione di controllo al tempo \(t\) è il sottoinsieme \(S_t\subseteq N=\{1,\dots,n\}\) delle classi offerte.

\paragraph{Ricavi e probabilità di scelta}
Si introduca \(p_0=0\) per il caso di \textbf{non-acquisto}. Con probabilità \(\lambda\) arriva un potenziale passeggero; dato \(S_t\), la probabilità di scelta della classe \(j\) è \(P_j(S_t)\), includendo \(P_0(S_t)\), con vincoli:
\[
\boxed{
P_j(S)\ge 0,\quad S\subseteq N,\; j\in S\cup\{0\},
\qquad
\sum_{j\in S}P_j(S)+P_0(S)=1,\quad S\subseteq N.
}
\]

\paragraph{Probabilità di acquisto dipendenti dalla decisione}
Le probabilità dipendenti dalla decisione sono \(\lambda P_j(S_t)\) per \(j=1,\dots,n\), e \((1-\lambda)+\lambda P_0(S_t)\) per \(j=0\).

\paragraph{Ricorsione DP}
In questo caso la decisione deve essere dichiarata prima della scelta del passeggero, quindi la ricorsione è nella forma standard:
\[
\boxed{
V_t(s)
=
\max_{S_t\subseteq N}
\left\{
\sum_{j\in S_t}\lambda P_j(S_t)\bigl(p_j+V_{t+1}(s-1)\bigr)
+
\bigl(\lambda P_0(S_t)+1-\lambda\bigr)\,V_{t+1}(s)
\right\}.
}
\]
\paragraph{Osservazione conclusiva}
La forma della ricorsione riflette la \textbf{struttura informativa}: qui la massimizzazione è sull’\textbf{aspettazione} perché la decisione precede la scelta del cliente.

\section{CAP 10 -- Programmazione dinamica numerica per stati discreti}
\subsection{Catene di Markov a tempo discreto}

\paragraph{Definizione di catena di Markov a tempo discreto}
Una \textbf{catena di Markov a tempo discreto} è un processo stocastico con variabile di stato $s_t$, $t = 0,1,2,\dots$, che assume valori in un insieme \textbf{discreto}.  
Se lo spazio degli stati è numerabile, è possibile associare ciascuno stato a un numero intero e rappresentare il processo mediante una \textbf{rete}.

\paragraph{Probabilità di transizione}
Le transizioni tra stati sono rappresentate da archi diretti etichettati con le \textbf{probabilità di transizione}.  
Le probabilità di transizione sono \textbf{probabilità condizionate} che dipendono solo dallo stato corrente:
\[
\pi(i,j) := \mathbb{P}\{ s_{t+1} = j \mid s_t = i \}.
\]

\paragraph{Catene omogenee}
Nel caso di problemi a \textbf{orizzonte infinito} si utilizzano catene \textbf{omogenee} (o \textbf{time-invariant}), in cui le probabilità di transizione non dipendono dal tempo.

\paragraph{Matrice di transizione}
Una catena di Markov a tempo discreto può essere descritta raccogliendo le probabilità di transizione in una \textbf{matrice di transizione a un passo} $\Pi$, in cui l’elemento $\pi_{ij}$ rappresenta la probabilità di transizione dallo stato $i$ allo stato $j$.

\paragraph{Proprietà di normalizzazione}
Poiché dopo una transizione il processo deve necessariamente trovarsi in uno stato dello spazio degli stati, ogni riga della matrice di transizione somma a uno:
\[
\sum_{j=1}^N \pi(i,j) = 1, \quad \forall i.
\]

\subsection{Catene di Markov controllate e MDP}

\paragraph{Azioni e transizioni controllate}
Nei \textbf{processi decisionali markoviani} (MDP), le transizioni sono parzialmente controllate tramite la selezione di azioni.  
In ciascuno stato $i$ è disponibile un insieme finito di azioni ammissibili $A(i)$.  
Per ogni azione $a \in A(i)$ sono definite probabilità di transizione:
\[
\pi(i,a,j).
\]

\paragraph{Distribuzione stazionaria}
Per valutare le prestazioni di una politica di controllo stazionaria, si può considerare la \textbf{probabilità di lungo periodo} di trovarsi in ciascuno stato, indicata con $q(i)$ e raccolta nel vettore $q$.  
Tali probabilità potrebbero non esistere se la catena non soddisfa opportune proprietà strutturali.

\paragraph{Proprietà strutturali delle catene}
Una catena può presentare stati \textbf{transienti}, \textbf{ricorrenti}, \textbf{assorbenti} e \textbf{periodicità}.  
Si assume una \textbf{unichain}, tale che ogni stato possa essere visitato infinitamente spesso nel lungo periodo e raggiunto da ogni altro stato in tempo finito con probabilità positiva.  
Nel seguito si assume che il processo di Markov sia \textbf{ben comportato} per ogni politica di controllo.

\subsection{MDP a orizzonte temporale finito}

\paragraph{Ricorsione di programmazione dinamica}
Per un MDP a \textbf{orizzonte finito}, la ricorsione di DP è:
\[
V_t(i) = \operatorname{opt}_{a \in A(i)}
\left\{
f(i,a) + \sum_j \pi(i,a,j)\, V_{t+1}(j)
\right\}, \quad i \in S.
\]

\paragraph{Cosa insegna l’esempio (arresto ottimo su random walk)}
L’esempio mostra che, in un problema di arresto ottimo a orizzonte finito, la politica risultante può essere \textbf{non stazionaria} e dipendere dal tempo residuo (confronto tra \texttt{wait} e \texttt{stop} tramite DP).

\subsection{Processi decisionali markoviani a orizzonte infinito}

\paragraph{Equazioni di Bellman a orizzonte infinito}
Nel caso di un \textbf{MDP a orizzonte infinito}, la funzione valore è definita implicitamente da equazioni del tipo:
\[
V(i) = \operatorname{opt}_{a \in A(i)}
\left\{
f(i,a) + \sum_j \pi(i,a,j)\, V(j)
\right\}, \quad i \in S,
\tag{2}
\]
oppure, quando il contributo immediato dipende anche dallo stato successivo,
\[
V(i) = \operatorname{opt}_{a \in A(i)}
\sum_{j \in S} \pi(i,a,j)
\left\{
h(i,a,j) + V(j)
\right\}, \quad i \in S.
\tag{3}
\]

\paragraph{Rappresentazione vettoriale}
Per MDP finiti, lo spazio degli stati è:
\[
S = \{1,\dots,n\},
\]
e la funzione valore $V : S \to \mathbb{R}$ è rappresentabile come vettore $V \in \mathbb{R}^n$ con componenti $V(i)$.

\paragraph{Strategie di soluzione}
Esistono due strategie fondamentali:
\begin{itemize}[label=-]
\item \textbf{Value iteration}: iterazioni economiche, convergenza al valore ottimo solo nel limite;
\item \textbf{Policy iteration}: iterazioni più costose, ma convergenza in tempo finito per MDP finiti (numero finito di politiche).
\end{itemize}

\subsection{Operatori di Bellman}

\paragraph{Operatore ottimo}
Data una funzione valore generica $\tilde V$, si definisce l’operatore $T$:
\[
[T\tilde V](i) =
\operatorname{opt}_{a \in A(i)}
\sum_{j \in S} \pi(i,a,j)
\left\{
h(i,a,j) + \tilde V(j)
\right\}, \quad i \in S.
\tag{4}
\]

\paragraph{Operatore associato a una politica}
Data una politica stazionaria generica $\mu$, si definisce l’operatore $T_\mu$:
\[
[T_\mu \tilde V](i) =
\sum_{j \in S} \pi(i,\mu(i),j)
\left\{
h(i,\mu(i),j) + \tilde V(j)
\right\}, \quad i \in S.
\tag{5}
\]

\paragraph{Ruolo degli operatori e punti fissi}
L’operatore $T$ è centrale per la \textbf{value iteration}, mentre $T_\mu$ è centrale per la \textbf{policy iteration}.  
Il vettore valore ottimo $V$ è un \textbf{punto fisso} di $T$:
\[
V = TV.
\tag{6}
\]
La funzione valore $V^\mu$ di una politica stazionaria $\mu$ è il punto fisso di $T_\mu$.

\paragraph{Condizioni di esistenza}
Per un MDP finito con \textbf{sconto stretto} ($\gamma < 1$) e contributi per stadio \textbf{limitati} (esiste $M$ con $|h(i,a,j)| \le M$), si assume che $T$ e $T_\mu$ siano \textbf{operatori di contrazione}, con punto fisso unico.

\paragraph{Miglioramento delle politiche}
L’operatore $T$ fornisce una caratterizzazione della politica stazionaria ottima e consente di \textbf{migliorare} una politica non ottima usando il valore $V^\mu$, che viene ottenuto tramite $T_\mu$.

\subsection{Value iteration}

\paragraph{Idea di base}
Con sconto stretto ($\gamma<1$), si cerca un punto fisso di $T$ tramite iterazione:
\[
V^{(k+1)} = T V^{(k)}.
\]
Se $V^{(k)} \to V$, allora $V$ è un punto fisso di $T$ e rappresenta la soluzione.

\subsection{Algoritmo di Value Iteration}

\begin{algorithm}[H]
\caption{Value Iteration per MDP a orizzonte infinito}
\begin{algorithmic}[1]
\State Scegliere una funzione valore iniziale $V^{(0)}$ (ad esempio $V^{(0)}(i)=0$ per ogni $i \in S$)
\State Scegliere una tolleranza $\varepsilon$
\State Inizializzare $k=0$ e \texttt{stop}=\texttt{false}
\While{\texttt{stop} $\neq$ \texttt{true}}
    \For{ogni stato $i \in S$}
        \State
        \[
        V^{(k+1)}(i) =
        \operatorname{opt}_{a \in A(i)}
        \left\{
        f(i,a) + \sum_{j \in S} \pi(i,a,j)\, V^{(k)}(j)
        \right\}
        \]
    \EndFor
    \If{$\|V^{(k+1)} - V^{(k)}\|_1 < \varepsilon$}
        \State \texttt{stop}=\texttt{true}
    \Else
        \State $k = k + 1$
    \EndIf
\EndWhile
\State Porre $\hat V = V^{(k+1)}$
\State Per ogni $i \in S$, determinare una politica stimata ottima:
\[
\hat\mu(i) \in
\arg\operatorname{opt}_{a \in A(i)}
\left\{
f(i,a) + \sum_j \pi(i,a,j)\, \hat V(j)
\right\}
\]
\State Restituire $\hat V$ e $\hat\mu$
\end{algorithmic}
\end{algorithm}

\paragraph{Cosa insegna l’esempio numerico (value iteration)}
L’esempio illustra che la politica ottima ottenuta da value iteration può dipendere in modo sensibile da probabilità di transizione e fattore di sconto, e che la convergenza in iterazioni può variare significativamente con tali parametri.

\subsection{Policy iteration}

\paragraph{Motivazione}
Con value iteration può accadere di individuare presto la politica ottima (da valori approssimati) ma senza valutarne ancora con precisione il valore.  
Policy iteration mira invece alla valutazione (più) diretta del valore di una politica.

\paragraph{Policy evaluation come sistema lineare}
Per una politica stazionaria $\mu$, $T_\mu$ non include ottimizzazione:
\[
[T_\mu \tilde V](i)= f\bigl(i,\mu(i)\bigr)+\sum_{j\in S}\pi\bigl(i,\mu(i),j\bigr)\tilde V(j),\quad i\in S.
\]
Il valore $V^\mu$ soddisfa:
\[
V^\mu(i)= f\bigl(i,\mu(i)\bigr)+\sum_{j\in S}\pi\bigl(i,\mu(i),j\bigr)V^\mu(j),\quad i\in S.
\]
Definendo la matrice di transizione indotta $\Pi^\mu$ e il vettore dei contributi $f^\mu$:
\[
f^\mu :=
\begin{bmatrix}
f\bigl(1,\mu(1)\bigr)\\
f\bigl(2,\mu(2)\bigr)\\
\vdots\\
f\bigl(n,\mu(n)\bigr)
\end{bmatrix},
\tag{9}
\]
si ottiene il sistema:
\[
V^\mu = f^\mu + \gamma \Pi^\mu V^\mu,
\qquad
\bigl(I-\gamma \Pi^\mu\bigr)V^\mu=f^\mu.
\]
Formalmente:
\[
V^\mu = \bigl(I-\gamma \Pi^\mu\bigr)^{-1}f^\mu.
\]
Per matrici grandi e sparse, metodi diretti possono essere proibitivi; si adottano quindi metodi iterativi.

\paragraph{Policy improvement}
Data $V^\mu$, una politica migliorata $\tilde\mu$ è definita da:
\[
\tilde\mu(i)\in
\arg\operatorname{opt}_{a\in A(i)}
\left\{
f(i,a)+\sum_j \pi(i,a,j)\,V^\mu(j)
\right\},\quad i\in S.
\]
Per un problema di massimizzazione:
\[
V^\mu(i)\le V^{\tilde\mu}(i),\quad i\in S,
\]
(con disuguaglianza invertita in minimizzazione). Se $\mu$ non è ottima, la disuguaglianza è stretta per almeno uno stato.  
Poiché il numero di politiche stazionarie deterministiche è finito, una sequenza di miglioramenti porta a una politica ottima.

\subsection{Algoritmo di Policy Iteration}

\begin{algorithm}[H]
\caption{Policy Iteration per MDP finito a orizzonte infinito}
\begin{algorithmic}[1]
\State Definire una politica stazionaria iniziale arbitraria $\mu^{(0)}$
\State Inizializzare $k=0$ e \texttt{stop}=\texttt{false}
\While{\texttt{stop} $\neq$ \texttt{true}}
    \State \textbf{Policy evaluation:} risolvere
    \[
    \bigl(I-\gamma \Pi^{\mu^{(k)}}\bigr)V^{\mu^{(k)}} = f^{\mu^{(k)}}
    \]
    \State \textbf{Policy improvement:} porre, per ogni $i\in S$,
    \[
    \mu^{(k+1)}(i)\in
    \arg\operatorname{opt}_{a\in A(i)}
    \left\{
    f(i,a)+\sum_j \pi(i,a,j)\,V^{\mu^{(k)}}(j)
    \right\}
    \]
    \If{$\mu^{(k+1)}=\mu^{(k)}$}
        \State \texttt{stop}=\texttt{true}
    \Else
        \State $k=k+1$
    \EndIf
\EndWhile
\State Restituire la funzione valore ottima e la politica stazionaria ottima
\end{algorithmic}
\end{algorithm}

\paragraph{Cosa insegna l’esempio numerico (policy iteration)}
L’esempio evidenzia che policy iteration può raggiungere la politica ottima in un numero finito (spesso piccolo) di passi di miglioramento, grazie alla valutazione esplicita della politica corrente.

\subsection{Value iteration vs. policy iteration}

\paragraph{Confronto e collegamento}
Value iteration impiega molte iterazioni economiche e converge nel limite; policy iteration usa poche iterazioni potenzialmente costose e converge in tempo finito per MDP finiti.  
La valutazione di una politica può anche essere eseguita iterativamente tramite:
\[
V^{(k+1)}_\mu(i)= f\bigl(i,\mu(i)\bigr)+\sum_{j\in S}\pi\bigl(i,\mu(i),j\bigr)V^{(k)}_\mu(j),\quad i\in S,
\tag{10}
\]
e interrompendo in modo prematuro tale procedura si ottiene una stima $\widehat V_\mu$ da usare nel miglioramento, dando luogo a \textbf{optimistic policy iteration} e, più in generale, a metodi di \textbf{generalized policy iteration}.  
Value iteration e policy iteration possono quindi essere visti come estremi di un continuum di approcci.

\subsection{Collegamento con Reinforcement Learning}

\paragraph{DP model-free e Q-factors}
Quando le probabilità di transizione (e possibilmente i contributi immediati) non sono note, si passa a una DP \textbf{model-free}. In tale contesto, la funzione valore è tipicamente sostituita dai \textbf{Q-factors} $Q(s,a)$ dipendenti da stati e azioni.

\paragraph{Q-learning e SARSA}
Il \textbf{Q-learning} è il corrispettivo RL della value iteration ed è un metodo \textbf{off-policy}.  
I corrispettivi RL della policy iteration sono \textbf{on-policy}; un approccio noto è \textbf{SARSA}.

\paragraph{Osservazione operativa in RL}
In RL non è in generale possibile valutare esattamente il valore di una politica se ciò richiede simulazioni Monte Carlo costose o esperimenti online; è quindi necessario effettuare un passo di miglioramento prima o poi, generando diverse varianti di strategie.

\section{CAP 11 -- Programmazione dinamica approssimativa e apprendimento per rinforzo per stati discreti}


\subsection{Approximate Dynamic Programming e DP model-free}

\paragraph{Approximate Dynamic Programming}
L’\textbf{Approximate Dynamic Programming (ADP)} è il nome collettivo di un’ampia classe di \textbf{metodi numerici di programmazione dinamica} che, a differenza dei metodi standard, \textbf{non garantiscono} di trovare una \textbf{politica ottima}.  
In cambio della natura approssimata, i metodi ADP mirano ad attenuare le \textbf{maledizioni della DP}.  
Nel seguito si considerano \textbf{metodi approssimati} per \textbf{MDP finiti} nel caso \textbf{a orizzonte infinito}.

\paragraph{Reinforcement Learning}
Le versioni \textbf{model-free} di \textbf{value iteration} e \textbf{policy iteration} sono raccolte sotto il termine \textbf{Reinforcement Learning (RL)}.  
Lo schema di base è il seguente: un \textbf{agente} interagisce con un sistema per apprendere una \textbf{politica di controllo} senza disporre di un \textbf{modello} del sistema; al tempo $t$ osserva lo \textbf{stato} $s_t$, prova un’\textbf{azione} $a_t$, e osserva il \textbf{contributo immediato} $h_{t+1}$ e lo \textbf{stato successivo} $s_{t+1}$.  
Una buona politica deve bilanciare \textbf{obiettivi di breve e lungo periodo}, misurati dal contributo immediato e dal valore dello stato successivo.

\subsection{Valori di stato, Q-factors e necessità di rappresentazioni compatte}

\paragraph{Perché $V(s)$ non basta in RL model-free}
Un punto chiave nel RL model-free è che, anche con una conoscenza perfetta della \textbf{funzione valore} $V(s)$, non è possibile valutare correttamente la bontà di un’azione se mancano informazioni sulle \textbf{transizioni} verso lo stato successivo.

\paragraph{Q-factors}
Un rimedio consiste nell’introdurre i \textbf{valori stato--azione}, cioè i \textbf{Q-factors}:
\[
Q(s,a).
\]
Sostituendo i valori di stato con i valori stato--azione, diventa possibile adottare \textbf{strategie di campionamento} per apprendere una politica decisionale.  
Si assume che lo spazio \textbf{stato--azione} non sia troppo grande e che una rappresentazione \textbf{tabellare} dei Q-factors sia fattibile; spesso, però, ciò non vale, e risulta necessaria una \textbf{rappresentazione compatta}.

\subsection{Campionamento e stima in contesti non stazionari}

\paragraph{Monte Carlo}
In teoria della probabilità si è interessati al valore atteso di una funzione $h(\cdot)$ di una variabile aleatoria $X$:
\[
\theta=\mathbb{E}\!\left[h(X)\right]=\int_{\mathcal{X}} h(x)f_X(x)\,dx.
\]
Poiché l’integrazione ad alta dimensionalità è difficile, si ricorre al \textbf{campionamento Monte Carlo}:
\[
\hat{\theta}=\frac{1}{m}\sum_{k=1}^{m} h\!\left(X^{(k)}\right),
\]
che fornisce una stima \textbf{non distorta} di $\theta$.

\paragraph{Complicazioni negli MDP}
Negli MDP si vogliono stimare \textbf{valori di stato} $V(i)$ o \textbf{Q-factors} $Q(i,a)$. In questo contesto l’apprendimento statistico deve affrontare:
\begin{itemize}[label=-]
\item il bilanciamento tra \textbf{esplorazione} ed \textbf{exploitation};
\item la difficoltà di stimare un \textbf{target non stazionario}.
\end{itemize}

\subsection{Il tradeoff esplorazione--exploitation}

\paragraph{Scelta dell’azione durante l’apprendimento}
Se si sta imparando $Q(i,a)$ e ci si trova nello stato $i$, una strategia di pura \textbf{exploitation} sceglierebbe l’azione che massimizza le stime correnti.  
Questa scelta è sensata solo quando le stime dei valori delle azioni sono sufficientemente accurate, condizione che tipicamente non vale all’inizio. Un’azione può apparire non buona perché:
\begin{itemize}[label=-]
\item il contributo immediato è stato stimato male;
\item conduce a uno stato molto buono che non è ancora stato esplorato.
\end{itemize}
Da qui nasce il problema fondamentale di bilanciare \textbf{exploitation} ed \textbf{esplorazione}.

\paragraph{Problema statico semplificato}
Si consideri un insieme finito $A$ di azioni, ciascuna associata a una ricompensa casuale $R(a)$. Se fossero noti i valori attesi $\nu(a)$, un decisore neutrale al rischio sceglierebbe l’azione con valore atteso massimo. Se invece si hanno solo stime rumorose $\hat{\nu}(a)$, la pura exploitation (massimizzare $\hat{\nu}$) può essere una strategia inadeguata.

\subsection{Multi-armed bandits}

\paragraph{Idea}
Un classico esempio è il \textbf{multi-armed bandit}: una slot machine con bracci diversi, associati a differenti distribuzioni di payoff.  
Con pochi campioni è facile mancare l’azione ottima, soprattutto quando l’azione con valore atteso più alto presenta anche una varianza elevata.

\subsection{Politiche randomizzate per l’esplorazione}

\paragraph{Compromesso tra estremi}
All’estremo opposto della pura exploitation, la pura esplorazione seleziona azioni a caso con probabilità uniformi. Occorre un compromesso, che porta a \textbf{politiche randomizzate}.

\paragraph{Approccio $\varepsilon$-greedy}
\textbf{Versione statica}: fissata una probabilità $\varepsilon$,
con probabilità $1-\varepsilon$ si seleziona l’azione più promettente, e con probabilità $\varepsilon$ si seleziona un’azione casuale.

\textbf{Versione dinamica}: una possibile raffinazione è rendere $\varepsilon$ dipendente dal tempo, ad esempio:
\[
\varepsilon(k)=\frac{c}{d+k},
\qquad \text{oppure} \qquad
\varepsilon(k)=d+\frac{c}{k}.
\]

\paragraph{Esplorazione di Boltzmann}
L’esplorazione può essere realizzata tramite una funzione soft-max che definisce la probabilità di scegliere l’azione $a\in A$:
\[
\pi(a)=\frac{\exp\!\big(\rho\,\hat{\nu}(a)\big)}{\sum_{a'\in A}\exp\!\big(\rho\,\hat{\nu}(a')\big)}.
\]
Quando $\rho=0$ si ha \textbf{pura esplorazione}, mentre per $\rho\to +\infty$ si tende alla \textbf{pura exploitation}.

\subsection{Non stazionarietà e smoothing esponenziale}

\paragraph{Media campionaria e aggiornamento incrementale}
Per stimare $\theta=\mathbb{E}[X]$ si usa la media campionaria, che soddisfa:
\[
\hat{\theta}(m)
=
\hat{\theta}(m-1)+\frac{1}{m}\Big(X(m)-\hat{\theta}(m-1)\Big).
\]
La correzione è smussata da $1/m$, che tende a zero all’aumentare di $m$.

\paragraph{Passo costante in contesti non stazionari}
In un contesto non stazionario si può mantenere costante la correzione:
\[
\boxed{
\hat{\theta}(m)=\hat{\theta}(m-1)+\alpha\Big(X(m)-\hat{\theta}(m-1)\Big)
=\alpha X(m)+(1-\alpha)\hat{\theta}(m-1)
}
\]
dove $\alpha\in(0,1)$ pesa nuova informazione e informazione passata.

\paragraph{Smoothing esponenziale e ruolo di $\alpha$}
Questa procedura è nota come \textbf{smoothing esponenziale}: le osservazioni più vecchie ricevono pesi \textbf{esponenzialmente decrescenti}.  
Il coefficiente $\alpha$ è detto \textbf{learning rate}, \textbf{smoothing coefficient} o \textbf{forgetting factor}.  
Un $\alpha$ grande produce aggiornamenti più reattivi e ``nervosi''; un $\alpha$ piccolo introduce più inerzia e filtraggio del rumore.

\paragraph{Learning rate decrescente}
Se la non stazionarietà è dovuta solo all’apprendimento (e non a variazioni della dinamica del sistema), si può usare una sequenza decrescente $\alpha(k)$ che soddisfa condizioni del tipo:
\[
\boxed{
\sum_{k=1}^{\infty}\alpha(k)=\infty,
\qquad
\sum_{k=1}^{\infty}\alpha(k)^2<\infty.
}
\]

\subsection{Apprendimento tramite differenze temporali e SARSA}

\paragraph{Operatore $T_\mu$ e punto fisso}
Si introduce l’operatore $T_\mu$:
\[
[T_\mu \tilde V](i)
=
\sum_{j\in S}\pi(i;\mu(i);j)\Big(h(i;\mu(i);j)+\gamma \tilde V(j)\Big),
\qquad i\in S,
\]
dove $\mu$ è una \textbf{politica stazionaria} e $\tilde V$ è una funzione valore.  
Il valore $V_\mu$ della politica $\mu$ si ottiene trovando il \textbf{punto fisso} di $T_\mu$:
\[
T_\mu V_\mu = V_\mu.
\]

\paragraph{Punto fisso in termini di Q-factors della politica}
La stessa idea può essere riscritta in termini dei \textbf{Q-factors} associati alla politica stazionaria $\mu$:
\[
Q_\mu\big(i;\mu(i)\big)
=
\mathbb{E}\Big[
h\big(i;\mu(i);j\big)+\gamma Q_\mu\big(j;\mu(j)\big)
\Big],
\]
dove l’aspettativa è rispetto allo \textbf{stato successivo} $j$.

\subsection{Schema di punto fisso, smoothing e differenze temporali}

\paragraph{Riformulazione astratta}
Si consideri un’equazione di punto fisso:
\[
y = Hy,
\]
dove $y$ è un vettore e $H$ è un operatore.  
L’iterazione diretta $y^{(k)}=Hy^{(k-1)}$ non garantisce convergenza in generale e, inoltre, in pratica $H$ non è valutabile esattamente perché richiede aspettative non note.

\paragraph{Schema incrementale con learning rate}
L’equazione può essere riscritta come:
\[
y = y + \alpha(Hy-y),
\]
da cui lo schema iterativo:
\[
y^{(k)}=y^{(k-1)}+\alpha\big(Hy^{(k-1)}-y^{(k-1)}\big).
\]

\paragraph{Sostituzione dell’aspettativa con osservazioni}
Applicando \textbf{smoothing esponenziale} e sostituendo l’aspettativa con osservazioni campionate, si ottiene lo schema tipico dell’apprendimento statistico.

\paragraph{Differenza temporale e aggiornamento (SARSA)}
Quando si osserva una transizione da $i=s(k)$ a $j=s(k+1)$ applicando l’azione prescritta dalla politica $\mu$, la \textbf{differenza temporale} è:
\[
\boxed{
\delta(k)=\Big[h\big(i;\mu(i);j\big)+\gamma \hat Q_\mu^{(k-1)}\big(j;\mu(j)\big)\Big]
-\hat Q_\mu^{(k-1)}\big(i;\mu(i)\big)
}
\]
e l’aggiornamento dei Q-factors è:
\[
\boxed{
\hat Q_\mu^{(k)}\big(i;\mu(i)\big)=\hat Q_\mu^{(k-1)}\big(i;\mu(i)\big)+\alpha\,\delta(k).
}
\]
La differenza temporale svolge il ruolo della correzione $Hy-y$.  
Poiché si usano stime per ottenere nuove stime, il termine \textbf{bootstrapping} descrive questo tipo di procedura.

\paragraph{Significato di SARSA e natura on-policy}
L’algoritmo è noto come \textbf{SARSA} perché utilizza la sequenza
\textbf{(State, Action, Reward, State, Action)}: si osserva lo stato $i$, si seleziona l’azione $\mu(i)$, si osservano il reward e il nuovo stato $j$, e si considera l’azione $\mu(j)$ nel nuovo stato.  
SARSA è un approccio \textbf{on-policy}: si agisce secondo una politica e si apprende il valore di quella stessa politica, come richiesto nella \textbf{policy iteration}.

\paragraph{Esplorazione e policy iteration generalizzata (ottimistica)}
Servono due ingredienti:
\begin{itemize}[label=-]
\item una quantità adeguata di \textbf{esplorazione}, ad esempio tramite \textbf{$\varepsilon$-greedy} (con probabilità $\varepsilon(k)$ si devia dalla politica e si seleziona un’azione casuale);
\item l’interruzione dell’apprendimento prima della valutazione esatta della politica, seguita da un passo di \textbf{miglioramento} della politica.
\end{itemize}
Questo schema è detto \textbf{generalized} o \textbf{optimistic policy iteration}: si assume ottimisticamente che la politica corrente sia stata valutata con accuratezza sufficiente. Ciò può accelerare l’apprendimento, ma può essere dannoso per la convergenza.

\paragraph{Miglioramento greedy della politica}
Quando si dispone di un’approssimazione $\hat Q_\mu(i;a)$, si tenta di migliorare la politica con una scelta greedy:
\[
\boxed{
\tilde\mu(i)\in \arg\opt_{a\in A(i)} \hat Q_\mu(i;a),
\qquad i\in S.
}
\]
A differenza dell’iterazione esatta, non vi è garanzia che $\tilde\mu$ sia effettivamente migliorativa.

\subsection{Q-learning per MDP finiti}

\paragraph{Equazioni per i Q-factors ottimali}
Nel Q-learning si considera direttamente la forma DP basata sui \textbf{Q-factors ottimali}:
\[
Q(i;a)
=
\sum_{j\in S}\pi(i;a;j)\Big[h(i;a;j)+\gamma \opt_{a'\in A(j)}Q(j;a')\Big],
\qquad i\in S,\ a\in A(i),
\]
\[
V(i)=\opt_{a\in A(i)}Q(i;a).
\]
A differenza di SARSA, qui non si valuta una politica fissata $\mu$, ma si mira ai fattori ottimali.

\paragraph{Logica off-policy e scelta dell’azione}
In Q-learning la politica è \textbf{implicita} nei Q-factors: aggiornandoli continuamente, si continua a cambiare politica; in un certo senso, si usa una politica per impararne un’altra.  
Al passo $k$, nello stato $s(k)=i$, la scelta greedy è:
\[
a(k)\in \arg\opt_{a\in A(i)}\hat Q^{(k-1)}(i;a).
\]

\paragraph{Target campionato e aggiornamento con smoothing}
Dopo l’azione si osservano lo stato successivo $j=s(k+1)$ e il contributo immediato.  
Si costruisce un target campionato che bilancia breve e lungo periodo:
\[
\tilde q
=
h\big(i;a(k);j\big)+\gamma \opt_{a'\in A(j)}\hat Q^{(k-1)}(j;a').
\]
L’aggiornamento tramite smoothing esponenziale è:
\[
\boxed{
\hat Q^{(k)}\big(s(k);a(k)\big)
=
\alpha \tilde q + (1-\alpha)\hat Q^{(k-1)}\big(s(k);a(k)\big).
}
\]

\paragraph{Forma a differenze temporali}
La correzione può essere scritta come differenza temporale:
\[
\delta(k)
=
\Big[h\big(s(k);a(k);j\big)+\gamma \opt_{a'\in A(j)}\hat Q^{(k-1)}(j;a')\Big]
-\hat Q^{(k-1)}\big(s(k);a(k)\big),
\]
e l’aggiornamento riguarda solo la coppia stato--azione osservata.

\paragraph{Confronto SARSA vs Q-learning}
La differenza si coglie confrontando le correzioni:
\begin{itemize}[label=-]
\item \textbf{SARSA} è \textbf{on-policy} e riferito a una politica stazionaria incumbente $\mu$;
\item \textbf{Q-learning} è \textbf{off-policy} e la politica cambia mentre si aggiornano i Q-factors.
\end{itemize}



\section{CAP 12 -- Simulazione}
Scrivere un paragrafo per fare recap dei problemi affrontati di simulazione.

\begin{itemize}[label=-]
\item Cap 1 pag 4/48 simulazione M/M/1
\end{itemize}

\end{document}